# ğŸ™ï¸ Live Stream Real-time Translation System (ç›´æ’­å³æ™‚ç¿»è­¯ç³»çµ±) v2.3.3

ä¸€å€‹é«˜æ•ˆèƒ½çš„å¯¦æ™‚ç›´æ’­ç¿»è­¯ç³»çµ±ï¼Œå°ˆç‚ºæ—¥æ–‡ç›´æ’­è¨­è¨ˆã€‚æ¡ç”¨ **Kotoba-Whisper v2.2**ï¼ˆæ—¥æ–‡å„ªåŒ– ASRï¼‰+ **Ollama Qwen3:8b**ï¼ˆæœ¬åœ° LLM ç¿»è­¯ï¼‰æ¶æ§‹ï¼Œé€é WebSocket å°‡ç¿»è­¯çµæœå³æ™‚æ¨é€åˆ° Web å®¢æˆ¶ç«¯ã€‚

![License](https://img.shields.io/badge/license-ISC-blue.svg)
![Docker](https://img.shields.io/badge/docker-%230db7ed.svg?style=flat&logo=docker&logoColor=white)
![Node.js](https://img.shields.io/badge/node.js-6DA55F?style=flat&logo=node.js&logoColor=white)
![Python](https://img.shields.io/badge/python-3670A0?style=flat&logo=python&logoColor=white)
![PyTorch](https://img.shields.io/badge/PyTorch-2.7-EE4C2C?style=flat&logo=pytorch&logoColor=white)
![CUDA](https://img.shields.io/badge/CUDA-12.8-76B900?style=flat&logo=nvidia&logoColor=white)

## âœ¨ v2.3.3 æ ¸å¿ƒç‰¹è‰²

### ğŸ§  AI å¼•æ“å‡ç´š
- **Kotoba-Whisper v2.2**ï¼šæ—¥æ–‡å„ªåŒ– ASRï¼Œæ”¯æ´æ¨™é»ä¸”å¹»è¦ºå°‘
- **Ollama Qwen3:8b**ï¼šæœ¬åœ° LLM ç¿»è­¯ï¼Œéš±ç§å®‰å…¨ï¼›æ”¯æ´ä½ VRAM æ¨¡å¼
- **stable-ts æ•´åˆ**ï¼šæ™‚é–“æˆ³å°é½Šèˆ‡ VAD å¢å¼·ï¼Œæå‡è­˜åˆ¥ç²¾åº¦

### âš¡ æ•ˆèƒ½å„ªåŒ–
- **CUDA 12.8 / RTX 50 ç›¸å®¹**ï¼šåŸºåº•æ˜ åƒæ”¹ç‚º `nvidia/cuda:12.8.0-cudnn-runtime-ubuntu22.04`
- **PyTorch 2.7.0 + cu128**ï¼šèˆ‡ RTX 50 ç³»åˆ— sm_120 ç›¸å®¹ï¼Œtorchaudio åŒæ­¥ç‰ˆæœ¬
- **åˆ†å±¤å¿«å– Dockerfile**ï¼štorch/torchaudio/ASR/Transformers åˆ†å±¤å®‰è£ï¼Œæ­é… BuildKit pip cacheï¼Œé‡å»ºæ›´å¿«
- **ç¨‹å¼å•Ÿå‹•å„ªåŒ–**ï¼šå»¶é²è¼‰å…¥ numpy/LLM/ASR å‡½æ•¸ï¼ŒRedis é€£ç·šé‡è©¦ï¼ŒHTTP é€£ç·šæ±  DNS cache
- **æ™ºèƒ½ç·©è¡**ï¼š5 ç§’æ»‘å‹•è¦–çª— + 1.5 ç§’é‡ç–Šï¼Œå…¼é¡§å»¶é²èˆ‡ä¸Šä¸‹æ–‡

### ğŸ›¡ï¸ ç¿»è­¯å“è³ª
- **å¤šå±¤å¹»è¦ºéæ¿¾**ï¼šASR å¹»è¦ºæª¢æ¸¬ã€é‡è¤‡è©éæ¿¾ã€ç„¡æ„ç¾©éŸ³è­¯éæ¿¾
- **OpenCC ç¹ç°¡è½‰æ›**ï¼šè‡ªå‹•å°‡ç°¡é«”è½‰æ›ç‚ºå°ç£ç¹é«”ç”¨èª
- **ä¸­å°ç”¨èªè½‰æ›**ï¼šã€Œè¦–é »â†’å½±ç‰‡ã€ã€ã€Œè»Ÿä»¶â†’è»Ÿé«”ã€ç­‰åœ¨åœ°åŒ–

### ğŸŒ ç¾ä»£åŒ–ä»‹é¢
- **éŸ¿æ‡‰å¼ Web UI**ï¼šæ·±è‰²æ¨¡å¼ã€è‡ªå‹•æ»¾å‹•ã€è¨Šæ¯ä¸Šé™æ§åˆ¶
- **äº‹ä»¶å§”æ´¾å„ªåŒ–**ï¼šé«˜æ•ˆ DOM æ“ä½œï¼Œæµæš¢è™•ç†å¤§é‡è¨Šæ¯

## ğŸ—ï¸ ç³»çµ±æ¶æ§‹

ç³»çµ±ç”±å››å€‹ Docker å®¹å™¨çµ„æˆï¼Œé€é Redis é€²è¡Œé«˜æ•ˆèƒ½é€šè¨Šï¼š

```mermaid
graph TD
    Live["ç›´æ’­æº (Twitch)"] -->|yt-dlp/ffmpeg| Node["Node.js Server"]
    Node -->|"éŸ³è¨Šæ•¸æ“š (Pub, 0.25s chunks)"| Redis[("Redis Message Broker")]
    Node -->|"ç¿»è­¯çµæœ (Sub)"| Redis
    Redis -->|"éŸ³è¨Šæ•¸æ“š (Sub)"| Python["Python Processor"]
    Python -->|"ç¿»è­¯çµæœ (Pub)"| Redis
    Node -->|WebSocket| Client["Web Client (Browser)"]
    Python -->|"Kotoba-Whisper v2.2 (ASR)"| Python
    Python -->|"Ollama Qwen3:8b (MT)"| Python
```

### Docker æœå‹™çµ„æˆ

| æœå‹™ | èªªæ˜ | åŸ å£ |
|------|------|------|
| `redis` | Redis 8.x è¨Šæ¯ä½‡åˆ— | 6379 (å…§éƒ¨) |
| `ollama` | Ollama LLM æœå‹™ + Qwen3:8b | 11434 (å…§éƒ¨) |
| `processor` | Python ASR + ç¿»è­¯è™•ç†å™¨ | - |
| `server` | Node.js WebSocket ä¼ºæœå™¨ | 8080 |

## ğŸš€ å¿«é€Ÿé–‹å§‹

### ç¡¬é«”éœ€æ±‚

| é …ç›® | æœ€ä½éœ€æ±‚ | å»ºè­°é…ç½® |
|------|----------|----------|
| GPU | NVIDIA GTX 1080 (8GB VRAM) | RTX 3080+ / RTX 50 ç³»åˆ— |
| RAM | 16GB | 32GB |
| ç¡¬ç¢Ÿ | 20GB å¯ç”¨ç©ºé–“ | SSD æ¨è–¦ |

### è»Ÿé«”éœ€æ±‚

- **Docker** & **Docker Compose**
- **NVIDIA Container Toolkit** ([å®‰è£æŒ‡å—](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html))
- **CUDA 12.x** é©…å‹•ç¨‹å¼

### å®‰è£æ­¥é©Ÿ

1. **è¤‡è£½å°ˆæ¡ˆ**
   ```bash
   git clone https://github.com/Chiyan54554/live-stream-translate.git
   cd live-stream-translate
   ```

2. **å»ºç«‹ Ollama æ¨¡å‹æŒä¹…åŒ– Volume**
   ```bash
   docker volume create live-stream-translate_ollama_models
   ```

3. **å•Ÿå‹•æœå‹™**
    - PowerShellï¼š
       ```powershell
       $env:DOCKER_BUILDKIT=1; docker compose up --build
       ```
    - bashï¼š
       ```bash
       DOCKER_BUILDKIT=1 docker compose up --build
       ```
   > â³ é¦–æ¬¡å•Ÿå‹•æœƒä¸‹è¼‰ï¼š
   > - Kotoba-Whisper v2.2 æ¨¡å‹ (~3GB)
   > - Qwen3:8b LLM æ¨¡å‹ (~5GB)
   > - Docker åŸºç¤æ˜ åƒæª”

4. **é–‹å•Ÿå®¢æˆ¶ç«¯**
   
   ç€è¦½å™¨è¨ªå•ï¼š`http://localhost:8080`

## âš™ï¸ é…ç½®èªªæ˜

### ä¿®æ”¹ç›´æ’­æº

ç·¨è¼¯ `server/server.js`ï¼š

```javascript
const LIVE_PAGE_URL = 'https://www.twitch.tv/your_streamer';
```

æ”¯æ´å¹³å°ï¼š
- âœ… Twitch
- âœ… YouTube Live
- âœ… å…¶ä»– yt-dlp æ”¯æ´çš„å¹³å°

### æ›´æ› ASR æ¨¡å‹

ç·¨è¼¯ `docker-compose.yml` ä¸­çš„ `processor` ç’°å¢ƒè®Šæ•¸ï¼š

```yaml
environment:
  # æ—¥æ–‡å„ªåŒ–ï¼ˆæ¨è–¦ï¼‰
  ASR_MODEL_NAME: kotoba-tech/kotoba-whisper-v2.2
  
  # å‚™é¸æ–¹æ¡ˆ
  # ASR_MODEL_NAME: kotoba-tech/kotoba-whisper-v2.1  # å¹»è¦ºæ›´å°‘
  # ASR_MODEL_NAME: large-v3                          # æ¨™æº– Whisper
```

#### æ”¹ç”¨ Google Speech-to-Textï¼ˆé›²ç«¯ï¼‰

1. ä¸‹è¼‰ä¸¦å„²å­˜ GCP æœå‹™å¸³æˆ¶ JSONï¼Œä¸¦åœ¨ `docker-compose.yml` æ›è¼‰ï¼š
    ```yaml
    services:
       processor:
          volumes:
             - ./gcp-sa.json:/app/keys/gcp-sa.json:ro
    ```
    > ä½ ä¹Ÿå¯ä»¥ä½¿ç”¨è‡ªå·±çš„æª”åï¼ˆä¾‹å¦‚ `nodal-alloy-....json`ï¼‰ï¼Œè¨˜å¾—åŒæ­¥æ›´æ–° volume è·¯å¾‘èˆ‡ `GOOGLE_APPLICATION_CREDENTIALS`ã€‚

2. å•Ÿç”¨ Google STT ä¸¦æŒ‡å®šæ†‘è­‰è·¯å¾‘ï¼ˆæ”¯æ´ `latest_short` / `latest_long` æ¨¡å‹ï¼‰ï¼š
    ```yaml
    services:
       processor:
          environment:
             USE_GOOGLE_STT: "1"
             GOOGLE_APPLICATION_CREDENTIALS: /app/keys/gcp-sa.json
             GOOGLE_STT_MODEL: latest_short  # é¸å¡«ï¼Œé è¨­ latest_short
    ```
    > æç¤ºï¼šGoogle STT æœƒè·³éæœ¬åœ° Whisperï¼Œæ¸›å°‘ GPU è² è¼‰ï¼›èªè¨€ä»£ç¢¼æ²¿ç”¨ `SOURCE_LANG_CODE`ï¼ˆé è¨­ jaï¼‰ã€‚

### æ›´æ› LLM æ¨¡å‹

ç·¨è¼¯ `docker-compose.yml`ï¼š

```yaml
environment:
  LLM_MODEL: "qwen3:8b"          # é è¨­ï¼ˆæ¨è–¦ï¼‰
  # LLM_MODEL: "qwen2.5:7b"       # å‚™é¸
  # LLM_MODEL: "llama3.1:8b"      # è‹±æ–‡æ›´å¼·
```

#### æ”¹ç”¨ Google Cloud Translationï¼ˆé›²ç«¯ç¿»è­¯ï¼‰

1. ç¢ºèª `processor` æœå‹™å·²æ›è¼‰ GCP æ†‘è­‰ä¸¦è¨­å®š `GOOGLE_APPLICATION_CREDENTIALS`ã€‚
2. åœ¨ `docker-compose.yml` å•Ÿç”¨ Cloud Translationï¼š
    ```yaml
    services:
       processor:
          environment:
             USE_CLOUD_TRANSLATION: "1"
             CLOUD_TRANSLATE_PROJECT_ID: "your-gcp-project-id"
             # å¯é¸ï¼šCLOUD_TRANSLATE_LOCATION: global
             # å¯é¸ï¼šCLOUD_TRANSLATE_TIMEOUT: 8
    ```
3. å•Ÿç”¨å¾Œæœƒæ”¹ç”¨ Cloud Translationï¼ŒOllama ä¸å†åƒèˆ‡ç¿»è­¯ï¼ˆä»å¯ä¿ç•™ä½œå‚™æ´ï¼Œå°‡ USE_CLOUD_TRANSLATION è¨­å› 0 å³å¯ï¼‰ã€‚

### èª¿æ•´ç·©è¡åƒæ•¸

ç·¨è¼¯ `processor/config.py`ï¼š

```python
BUFFER_DURATION_S = 5.0   # ç·©è¡å€é•·åº¦ï¼ˆç§’ï¼‰- è¶Šé•·æº–ç¢ºåº¦è¶Šé«˜ï¼Œå»¶é²è¶Šå¤§
OVERLAP_DURATION_S = 1.5  # é‡ç–Šé•·åº¦ï¼ˆç§’ï¼‰- é˜²æ­¢å¥å­è¢«åˆ‡æ–·
```

## ğŸ“ å°ˆæ¡ˆçµæ§‹

```
live-stream-translate/
â”œâ”€â”€ client.html              # Web å®¢æˆ¶ç«¯ä»‹é¢
â”œâ”€â”€ docker-compose.yml       # Docker Compose é…ç½®
â”œâ”€â”€ Dockerfile.server        # Node.js Server Dockerfile
â”œâ”€â”€ README.md
â”‚
â”œâ”€â”€ ollama/
â”‚   â”œâ”€â”€ Dockerfile.ollama    # Ollama æœå‹™ Dockerfile
â”‚   â””â”€â”€ entrypoint.sh        # å•Ÿå‹•è…³æœ¬ï¼ˆè‡ªå‹•è¼‰å…¥æ¨¡å‹ï¼‰
â”‚
â”œâ”€â”€ processor/
â”‚   â”œâ”€â”€ Dockerfile.processor # Python è™•ç†å™¨ Dockerfile
â”‚   â”œâ”€â”€ requirements.txt     # Python ä¾è³´
â”‚   â”œâ”€â”€ config.py            # é…ç½®åƒæ•¸
â”‚   â”œâ”€â”€ main.py              # ä¸»ç¨‹å¼å…¥å£
â”‚   â”œâ”€â”€ asr.py               # ASR æ¨¡çµ„ï¼ˆKotoba-Whisperï¼‰
â”‚   â”œâ”€â”€ translator.py        # LLM ç¿»è­¯æ¨¡çµ„
â”‚   â”œâ”€â”€ text_utils.py        # æ–‡å­—è™•ç†å·¥å…·
â”‚   â””â”€â”€ mappings/            # ç”¨èªè½‰æ›è¡¨
â”‚       â”œâ”€â”€ china_to_taiwan.txt
â”‚       â””â”€â”€ simplified_to_traditional.txt
â”‚
â””â”€â”€ server/
    â”œâ”€â”€ package.json
    â””â”€â”€ server.js            # Node.js WebSocket ä¼ºæœå™¨
```

## ğŸ› ï¸ æŠ€è¡“æ£§

| å±¤ç´š | æŠ€è¡“ |
|------|------|
| **Frontend** | HTML5, CSS3 (Dark Mode), Vanilla JS |
| **Backend** | Node.js 25.x, yt-dlp, FFmpeg |
| **AI Core** | PyTorch 2.9, Transformers, stable-ts |
| **AI Core** | PyTorch 2.7.0+cu128, Transformers, stable-ts |
| **ASR** | Kotoba-Whisper v2.2 (Transformers Pipeline) |
| **Translation** | Ollama + Qwen3:8b, OpenCC (s2twp) |
| **Infra** | Docker, Redis 8.x, CUDA 12.8, cuDNN 9 |

## ğŸ“ å¸¸è¦‹å•é¡Œ

### Q: å­—å¹•å»¶é²å¤šå°‘æ˜¯æ­£å¸¸çš„ï¼Ÿ
A: ç³»çµ±è¨­è¨ˆå»¶é²ç´„ **5-8 ç§’**ï¼ŒåŒ…å«ï¼š
- 5 ç§’éŸ³è¨Šç·©è¡
- ASR è™•ç†æ™‚é–“ (~0.3s)
- LLM ç¿»è­¯æ™‚é–“ (~0.2s)
- ç›´æ’­æœ¬èº«å»¶é²

### Q: VRAM ä¸è¶³æ€éº¼è¾¦ï¼Ÿ
A: å¯ä»¥å˜—è©¦ä»¥ä¸‹æ–¹æ¡ˆï¼š
1. ä½¿ç”¨è¼ƒå°çš„ LLM æ¨¡å‹ï¼š`LLM_MODEL: "qwen2.5:3b"`
2. ä½¿ç”¨è¼ƒå°çš„ ASR æ¨¡å‹ï¼š`ASR_MODEL_NAME: "small"`
3. é™ä½ Ollama context lengthï¼ˆåœ¨ docker-compose.yml ä¸­è¨­å®š `OLLAMA_CONTEXT_LENGTH: 2048`ï¼‰

### Q: å¦‚ä½•æ”¯æ´å…¶ä»–èªè¨€ï¼Ÿ
A: ä¿®æ”¹ `processor/config.py`ï¼š
```python
SOURCE_LANG_CODE = "ja"    # æºèªè¨€
TARGET_LANG_CODE = "zh-TW" # ç›®æ¨™èªè¨€
```
ä¸¦èª¿æ•´ `translator.py` ä¸­çš„ LLM Promptã€‚

### Q: Docker å•Ÿå‹•å¤±æ•—ï¼Ÿ
A: å¸¸è¦‹è§£æ±ºæ–¹æ¡ˆï¼š
```bash
# æ¸…ç† Docker å¿«å–
docker builder prune -f

# é‡æ–°å»ºç«‹ Volume
docker volume rm live-stream-translate_ollama_models
docker volume create live-stream-translate_ollama_models

# é‡æ–°å•Ÿå‹•
docker compose up --build
```

## ğŸ“Š æ•ˆèƒ½åƒè€ƒ

åœ¨ RTX 5070 Ti (16GB VRAM) ä¸Šçš„æ¸¬è©¦çµæœï¼š

| æŒ‡æ¨™ | æ•¸å€¼ |
|------|------|
| ASR è™•ç†æ™‚é–“ | ~0.3s / 5s éŸ³è¨Š |
| LLM ç¿»è­¯æ™‚é–“ | ~0.15s / å¥ |
| VRAM ä½¿ç”¨é‡ | ~10GB (ASR + LLM) |
| ç«¯åˆ°ç«¯å»¶é² | ~6-7 ç§’ |

## ğŸ¤ è²¢ç»

æ­¡è¿æäº¤ Issue æˆ– Pull Requestï¼

## ğŸ“„ æˆæ¬Š

ISC License
