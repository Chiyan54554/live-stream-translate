services:
  # 1. Redis Service (æ¶ˆæ¯éšŠåˆ—)
  redis:
    image: redis:8.4.0-alpine
    container_name: redis_pubsub
    expose:
      - "6379"

    command: ["/bin/sh", "-c", "redis-server --loglevel warning --client-output-buffer-limit pubsub 128mb 64mb 60 > /var/log/redis.log 2>&1"]

    # å¥åº·æª¢æŸ¥ç¢ºä¿å…¶ä»–æœå‹™ç­‰å¾… Redis å•Ÿå‹•
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 1s
      timeout: 3s
      retries: 5
    
    restart: always

  # ğŸŒŸ 2. Ollama LLM Service (ç¿»è­¯å¼•æ“)
  ollama:
    build:
      context: .
      dockerfile: ollama/Dockerfile.ollama
    container_name: ollama_llm
    expose:
      - "11434"
    
    # é…ç½® GPU æ”¯æŒ (Ollama ä¹Ÿéœ€è¦ GPU)
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    
    environment:
      OLLAMA_HOST: "0.0.0.0:11434"
      NVIDIA_VISIBLE_DEVICES: all
      # ğŸ¯ æŒ‡å®šè¦ä½¿ç”¨çš„æ¨¡å‹ï¼ˆå•Ÿå‹•æ™‚è‡ªå‹•ä¸‹è¼‰ï¼Œå¦‚æœé‚„æ²’ä¸‹è¼‰çš„è©±ï¼‰
      # é è¨­ä½¿ç”¨å·²çŸ¥å¯ç”¨çš„é‡åŒ–æ¨¡å‹ï¼Œé¿å… 404ï¼ˆæ¨¡å‹ä¸å­˜åœ¨ï¼‰
      # å»ºè­°ä½¿ç”¨å®˜æ–¹ instruct é‡åŒ–æ¨™ç±¤ï¼Œé¿å… 404
      OLLAMA_MODEL: "qwen3:8b-q4_K_M"
      # å¯é¸ï¼šè‹¥æƒ³çœè³‡æºï¼Œå¯æ”¹ç‚º qwen2.5:7b-instruct-q4_K_M
      OLLAMA_KEEP_ALIVE: "2h"                      # ä¿æŒæ¨¡å‹åœ¨è¨˜æ†¶é«”é¿å…é‡è¼‰
      # è‹¥å•Ÿå‹•æœŸé ç†±å¸¸å¤±æ•—ï¼Œå¯é—œé–‰é ç†±åŠ å¿«å•Ÿå‹•ï¼›é¦–æ¬¡è«‹æ±‚æ™‚æœƒè¼‰å…¥
      OLLAMA_WARMUP: "0"
    
    # æŒä¹…åŒ–æ¨¡å‹ï¼Œé¿å…é‡è¤‡ä¸‹è¼‰
    volumes:
      - ollama_models:/root/.ollama
    
    # ğŸš€ å¿«é€Ÿå¥åº·æª¢æŸ¥ï¼šåªç¢ºèªæœå‹™å•Ÿå‹•ï¼Œä¸ç­‰å¾…æ¨¡å‹
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 3s
      timeout: 5s
      retries: 10
      start_period: 5s
    
    restart: always

  # 3. Python Processor Service (Redis Subscriber/Publisher)
  processor:
    build:
      context: .
      dockerfile: processor/Dockerfile.processor
    container_name: python_processor
    
    # é…ç½® GPU æ”¯æŒ
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    
    environment:
      REDIS_HOST: redis 
      REDIS_PORT: 6379
      # ğŸ¯ ASR/MT åˆ‡æ›ï¼šlocal æˆ– apiï¼ˆè‹¥ç•™ç©ºï¼Œæ²¿ç”¨èˆŠçš„ USE_GOOGLE_STT / USE_CLOUD_TRANSLATIONï¼‰
      ASR_MODE: "local"   # api=Google STT, local=æœ¬åœ° Whisper
      MT_MODE: "local"    # api=Cloud Translation, local=Ollama
      USE_GOOGLE_STT: "1"
      GOOGLE_APPLICATION_CREDENTIALS: /app/keys/gcp-sa.json
      # GOOGLE_STT_MODEL: default   # ç•™ç©ºæˆ– default ç”± Google è‡ªè¡Œé¸å‹
      GOOGLE_STT_ENABLE_PUNCTUATION: "1"
      GOOGLE_STT_MAX_ALTERNATIVES: 1
      SOURCE_LANG_CODE: ja
      TARGET_LANG_CODE: zh-TW
      # å¦‚éœ€æ”¹è¾¨è­˜èªè¨€ï¼Œå¯åŠ ï¼šSOURCE_LANG_CODE: zh-TW
      # ğŸŒŸ Kotoba-Whisper v2.2ï¼šæ—¥æ–‡å„ªåŒ– Transformers ç‰ˆ (æœ€æ–°ï¼Œæ”¯æ´æ¨™é»)
      # å‚™é¸: kotoba-tech/kotoba-whisper-v2.1, large-v3 (æ¨™æº– faster-whisper)
      # âš ï¸ v2.2 æ²’æœ‰æä¾› faster (CTranslate2) ç‰ˆæœ¬
      ASR_MODEL_NAME: kotoba-tech/kotoba-whisper-v2.2
      MODEL_CACHE_DIR: /root/.cache/huggingface/hub
      HF_HUB_ENABLE_HF_TRANSFER: "0"
      # ğŸŒŸ NVIDIA ç›¸é—œç’°å¢ƒè®Šæ•¸
      NVIDIA_VISIBLE_DEVICES: all
      NVIDIA_DRIVER_CAPABILITIES: compute,utility
      # ğŸŒŸ LLM ç¿»è­¯é…ç½® (é€£æ¥ Docker å…§çš„ Ollama æœå‹™)
      LLM_HOST: ollama
      LLM_PORT: "11434"
      LLM_MODEL: "qwen3:8b-q4_K_M"
      # å¯é¸ï¼šLLM_MODEL: "qwen2.5:7b-instruct-q4_K_M"ï¼ˆè¼ƒçœ VRAMï¼‰
      USE_CLOUD_TRANSLATION: "1"
      CLOUD_TRANSLATE_PROJECT_ID: "nodal-alloy-480514-e2"  # è«‹æ”¹æˆå¯¦éš› GCP å°ˆæ¡ˆ IDï¼ˆéœ€ä»¥å­—æ¯é–‹é ­ï¼‰
      # å¯é¸ï¼šCLOUD_TRANSLATE_LOCATION: "global"
      # å¯é¸ï¼šCLOUD_TRANSLATE_TIMEOUT: 8
    
    volumes:
      - huggingface_cache:/root/.cache/huggingface
      - ./nodal-alloy-480514-e2-dd3350c6b301.json:/app/keys/gcp-sa.json:ro
    
    # ğŸš€ åªç­‰å¾… Redisï¼ŒOllama æœƒåœ¨èƒŒæ™¯è¼‰å…¥
    depends_on:
      redis:
        condition: service_healthy
      ollama:
        condition: service_started

    restart: always
      
  # 4. Node.js Server Service (FFmpeg, Redis I/O, WebSocket)
  server:
    build:
      context: .
      dockerfile: Dockerfile.server
    container_name: node_websocket_server
    # å°å¤–æš´éœ² WebSocket ç«¯å£
    ports:
      - "8080:8080" 
    depends_on:
      redis:
        condition: service_healthy
    # é…ç½® Redis é€£ç·šä¿¡æ¯ (ä½¿ç”¨æœå‹™åç¨± 'redis')
    environment:
      REDIS_HOST: redis 
      REDIS_PORT: 6379

    restart: always

volumes:
  huggingface_cache:
    driver: local
  ollama_models:
    external: true
    name: live-stream-translate_ollama_models