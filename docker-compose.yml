services:
  # 1. Redis Service (æ¶ˆæ¯éšŠåˆ—)
  redis:
    image: redis:8.4.0-alpine
    container_name: redis_pubsub
    expose:
      - "6379"

    command: redis-server --client-output-buffer-limit pubsub 128mb 64mb 60

    # å¥åº·æª¢æŸ¥ç¢ºä¿å…¶ä»–æœå‹™ç­‰å¾… Redis å•Ÿå‹•
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 1s
      timeout: 3s
      retries: 5
    
    restart: always

  # ğŸŒŸ 2. Ollama LLM Service (ç¿»è­¯å¼•æ“)
  ollama:
    build:
      context: .
      dockerfile: ollama/Dockerfile.ollama
    container_name: ollama_llm
    expose:
      - "11434"
    
    # é…ç½® GPU æ”¯æŒ (Ollama ä¹Ÿéœ€è¦ GPU)
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    
    environment:
      OLLAMA_HOST: "0.0.0.0:11434"
      NVIDIA_VISIBLE_DEVICES: all
    
    # æŒä¹…åŒ–æ¨¡å‹ï¼Œé¿å…é‡è¤‡ä¸‹è¼‰
    volumes:
      - ollama_models:/root/.ollama
    
    # å¥åº·æª¢æŸ¥ (ä½¿ç”¨ wget æˆ– ollama å‘½ä»¤)
    healthcheck:
      test: ["CMD-SHELL", "ollama list || exit 1"]
      interval: 10s
      timeout: 10s
      retries: 10
      start_period: 60s
    
    restart: always

  # 3. Python Processor Service (Redis Subscriber/Publisher)
  processor:
    build:
      context: .
      dockerfile: processor/Dockerfile.processor
    container_name: python_processor
    
    # é…ç½® GPU æ”¯æŒ
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    
    environment:
      REDIS_HOST: redis 
      REDIS_PORT: 6379
      # ğŸŒŸ large-v3 æœ€æº–ç¢ºï¼Œlarge-v3-turbo è¼ƒå¿«ä½†ç•¥ä½æº–ç¢ºåº¦
      ASR_MODEL_NAME: large-v3
      MODEL_CACHE_DIR: /root/.cache/huggingface/hub
      HF_HUB_ENABLE_HF_TRANSFER: "0"
      # ğŸŒŸ NVIDIA ç›¸é—œç’°å¢ƒè®Šæ•¸
      NVIDIA_VISIBLE_DEVICES: all
      NVIDIA_DRIVER_CAPABILITIES: compute,utility
      # ğŸŒŸ LLM ç¿»è­¯é…ç½® (é€£æ¥ Docker å…§çš„ Ollama æœå‹™)
      LLM_HOST: ollama
      LLM_PORT: "11434"
      LLM_MODEL: "qwen2.5:7b-instruct"
    
    volumes:
      - huggingface_cache:/root/.cache/huggingface
    
    # ç­‰å¾… Ollama æœå‹™å°±ç·’
    depends_on:
      redis:
        condition: service_healthy
      ollama:
        condition: service_healthy

    restart: always
      
  # 4. Node.js Server Service (FFmpeg, Redis I/O, WebSocket)
  server:
    build:
      context: .
      dockerfile: Dockerfile.server
    container_name: node_websocket_server
    # å°å¤–æš´éœ² WebSocket ç«¯å£
    ports:
      - "8080:8080" 
    depends_on:
      redis:
        condition: service_healthy
    # é…ç½® Redis é€£ç·šä¿¡æ¯ (ä½¿ç”¨æœå‹™åç¨± 'redis')
    environment:
      REDIS_HOST: redis 
      REDIS_PORT: 6379

    restart: always

volumes:
  huggingface_cache:
    driver: local
  ollama_models:
    driver: local