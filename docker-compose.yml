services:
  # 1. Redis Service (æ¶ˆæ¯éšŠåˆ—)
  redis:
    image: redis:8.4.0-alpine
    container_name: redis_pubsub
    expose:
      - "6379"

    command: redis-server --client-output-buffer-limit pubsub 128mb 64mb 60

    # å¥åº·æª¢æŸ¥ç¢ºä¿å…¶ä»–æœå‹™ç­‰å¾… Redis å•Ÿå‹•
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 1s
      timeout: 3s
      retries: 5
    
    restart: always

  # ğŸŒŸ 2. Ollama LLM Service (ç¿»è­¯å¼•æ“)
  ollama:
    build:
      context: .
      dockerfile: ollama/Dockerfile.ollama
    container_name: ollama_llm
    expose:
      - "11434"
    
    # é…ç½® GPU æ”¯æŒ (Ollama ä¹Ÿéœ€è¦ GPU)
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    
    environment:
      OLLAMA_HOST: "0.0.0.0:11434"
      NVIDIA_VISIBLE_DEVICES: all
      # ğŸ¯ æŒ‡å®šè¦ä½¿ç”¨çš„æ¨¡å‹ï¼ˆå•Ÿå‹•æ™‚è‡ªå‹•ä¸‹è¼‰ï¼Œå¦‚æœé‚„æ²’ä¸‹è¼‰çš„è©±ï¼‰
      OLLAMA_MODEL: "qwen3:8b"
    
    # æŒä¹…åŒ–æ¨¡å‹ï¼Œé¿å…é‡è¤‡ä¸‹è¼‰
    volumes:
      - ollama_models:/root/.ollama
    
    # ğŸš€ å¿«é€Ÿå¥åº·æª¢æŸ¥ï¼šåªç¢ºèªæœå‹™å•Ÿå‹•ï¼Œä¸ç­‰å¾…æ¨¡å‹
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 3s
      timeout: 5s
      retries: 10
      start_period: 5s
    
    restart: always

  # 3. Python Processor Service (Redis Subscriber/Publisher)
  processor:
    build:
      context: .
      dockerfile: processor/Dockerfile.processor
    container_name: python_processor
    
    # é…ç½® GPU æ”¯æŒ
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    
    environment:
      REDIS_HOST: redis 
      REDIS_PORT: 6379
      # ğŸŒŸ Kotoba-Whisper v2.2ï¼šæ—¥æ–‡å„ªåŒ– Transformers ç‰ˆ (æœ€æ–°ï¼Œæ”¯æ´æ¨™é»)
      # å‚™é¸: kotoba-tech/kotoba-whisper-v2.1, large-v3 (æ¨™æº– faster-whisper)
      # âš ï¸ v2.2 æ²’æœ‰æä¾› faster (CTranslate2) ç‰ˆæœ¬
      ASR_MODEL_NAME: kotoba-tech/kotoba-whisper-v2.2
      MODEL_CACHE_DIR: /root/.cache/huggingface/hub
      HF_HUB_ENABLE_HF_TRANSFER: "0"
      # ğŸŒŸ NVIDIA ç›¸é—œç’°å¢ƒè®Šæ•¸
      NVIDIA_VISIBLE_DEVICES: all
      NVIDIA_DRIVER_CAPABILITIES: compute,utility
      # ğŸŒŸ LLM ç¿»è­¯é…ç½® (é€£æ¥ Docker å…§çš„ Ollama æœå‹™)
      LLM_HOST: ollama
      LLM_PORT: "11434"
      LLM_MODEL: "qwen3:8b"
    
    volumes:
      - huggingface_cache:/root/.cache/huggingface
    
    # ğŸš€ åªç­‰å¾… Redisï¼ŒOllama æœƒåœ¨èƒŒæ™¯è¼‰å…¥
    depends_on:
      redis:
        condition: service_healthy
      ollama:
        condition: service_started

    restart: always
      
  # 4. Node.js Server Service (FFmpeg, Redis I/O, WebSocket)
  server:
    build:
      context: .
      dockerfile: Dockerfile.server
    container_name: node_websocket_server
    # å°å¤–æš´éœ² WebSocket ç«¯å£
    ports:
      - "8080:8080" 
    depends_on:
      redis:
        condition: service_healthy
    # é…ç½® Redis é€£ç·šä¿¡æ¯ (ä½¿ç”¨æœå‹™åç¨± 'redis')
    environment:
      REDIS_HOST: redis 
      REDIS_PORT: 6379

    restart: always

volumes:
  huggingface_cache:
    driver: local
  ollama_models:
    external: true
    name: live-stream-translate_ollama_models