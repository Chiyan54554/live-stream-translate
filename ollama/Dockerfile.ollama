# Dockerfile for Ollama LLM Service with pre-downloaded model
FROM ollama/ollama:latest

# è¨­å®šç’°å¢ƒè®Šæ•¸
ENV OLLAMA_HOST=0.0.0.0:11434
ENV OLLAMA_MODELS=/root/.ollama/models

# å»ºç«‹æ¨¡å‹ç›®éŒ„
RUN mkdir -p /root/.ollama/models

# ğŸ¯ åœ¨å»ºæ§‹æ™‚ä¸‹è¼‰æ¨¡å‹
# å•Ÿå‹• Ollama æœå‹™ï¼Œä¸‹è¼‰æ¨¡å‹ï¼Œç„¶å¾Œåœæ­¢
RUN ollama serve & \
    sleep 5 && \
    ollama pull qwen2.5:7b-instruct && \
    pkill ollama

# æš´éœ² Ollama API ç«¯å£
EXPOSE 11434

# å•Ÿå‹• Ollama æœå‹™
CMD ["serve"]
