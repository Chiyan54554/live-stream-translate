# Dockerfile for Python Processor (å»ºè­°ä½¿ç”¨ç¨ç«‹çš„ Dockerfile.processor)

# ä½¿ç”¨ PyTorch 2.9.1 with CUDA 12.8 (æ”¯æ´ RTX 50 ç³»åˆ—)
FROM pytorch/pytorch:2.9.1-cuda12.8-cudnn9-runtime

# è¨­å®šç’°å¢ƒè®Šæ•¸
ENV DEBIAN_FRONTEND=noninteractive
ENV PYTHONUNBUFFERED=1
ENV HF_HOME=/root/.cache/huggingface
ENV MODEL_CACHE_DIR=/root/.cache/huggingface/hub
# ğŸš€ å•Ÿç”¨ pip å¿«å–
ENV PIP_CACHE_DIR=/root/.cache/pip

# å®‰è£å¿…è¦å¥—ä»¶
RUN apt-get update && apt-get install -y \
    libsndfile1 \
    ffmpeg \
    && rm -rf /var/lib/apt/lists/*

# å»ºç«‹å¿«å–ç›®éŒ„
RUN mkdir -p /root/.cache/huggingface/hub /root/.cache/pip

# è¨­å®šå·¥ä½œç›®éŒ„
WORKDIR /app

# è¤‡è£½ requirementsï¼ˆåˆ†é›¢ä»¥åˆ©å¿«å–ï¼‰
COPY processor/requirements.txt .

# ğŸš€ ä½¿ç”¨ BuildKit å¿«å–æ›è¼‰ï¼Œpip å¥—ä»¶åªä¸‹è¼‰ä¸€æ¬¡
RUN --mount=type=cache,target=/root/.cache/pip \
    pip install -r requirements.txt

# ğŸŒŸ å»ºç«‹ç¬¦è™Ÿé€£çµï¼Œè®“ CTranslate2 æ‰¾åˆ° cuDNN
RUN CUDNN_PATH=$(python -c "import nvidia.cudnn; print(nvidia.cudnn.__path__[0])")/lib && \
    echo "cuDNN lib path: $CUDNN_PATH" && \
    ls -la $CUDNN_PATH && \
    mkdir -p /usr/local/lib/ctranslate2 && \
    ln -sf $CUDNN_PATH/libcudnn*.so* /usr/local/lib/ctranslate2/

# è¤‡è£½ç¨‹å¼ç¢¼å’Œè³‡æºæª”æ¡ˆ
COPY processor/__init__.py .
COPY processor/config.py .
COPY processor/text_utils.py .
COPY processor/translator.py .
COPY processor/asr.py .
COPY processor/main.py .
COPY processor/mappings/ ./mappings/

# ğŸŒŸ å•Ÿå‹•æ™‚è¨­å®š LD_LIBRARY_PATHï¼Œè®“ CTranslate2 å„ªå…ˆä½¿ç”¨æˆ‘å€‘å®‰è£çš„ cuDNN
CMD ["sh", "-c", "export LD_LIBRARY_PATH=/usr/local/lib/ctranslate2:$(python -c 'import nvidia.cudnn; print(nvidia.cudnn.__path__[0])')/lib:$LD_LIBRARY_PATH && python -m main"]