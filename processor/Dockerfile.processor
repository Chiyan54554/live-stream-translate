# Dockerfile for Python Processor (å»ºè­°ä½¿ç”¨ç¨ç«‹çš„ Dockerfile.processor)

# ä½¿ç”¨ PyTorch 2.9.1 with CUDA 12.8 (æ”¯æ´ RTX 50 ç³»åˆ—)
FROM pytorch/pytorch:2.9.1-cuda12.8-cudnn9-runtime

# è¨­å®šç’°å¢ƒè®Šæ•¸
ENV DEBIAN_FRONTEND=noninteractive
ENV PYTHONUNBUFFERED=1
ENV HF_HOME=/root/.cache/huggingface
ENV MODEL_CACHE_DIR=/root/.cache/huggingface/hub

# å®‰è£å¿…è¦å¥—ä»¶
RUN apt-get update && apt-get install -y \
    libsndfile1 \
    ffmpeg \
    && rm -rf /var/lib/apt/lists/*

# å»ºç«‹å¿«å–ç›®éŒ„
RUN mkdir -p /root/.cache/huggingface/hub

# è¨­å®šå·¥ä½œç›®éŒ„
WORKDIR /app

# è¤‡è£½ requirements
COPY processor/requirements.txt .

# ğŸŒŸ é—œéµï¼šå®‰è£ CTranslate2 ç›¸å®¹çš„ cuDNN 9.1.x
RUN pip install --no-cache-dir nvidia-cudnn-cu12==9.1.0.70

# ğŸŒŸ å»ºç«‹ç¬¦è™Ÿé€£çµï¼Œè®“ CTranslate2 æ‰¾åˆ° cuDNN
RUN CUDNN_PATH=$(python -c "import nvidia.cudnn; print(nvidia.cudnn.__path__[0])")/lib && \
    echo "cuDNN lib path: $CUDNN_PATH" && \
    ls -la $CUDNN_PATH && \
    mkdir -p /usr/local/lib/ctranslate2 && \
    ln -sf $CUDNN_PATH/libcudnn*.so* /usr/local/lib/ctranslate2/

# å®‰è£å…¶ä»–ä¾è³´ (ä½¿ç”¨ aiohttp ç•°æ­¥é€£æ¥ Ollama LLM)
RUN pip install --no-cache-dir \
    ctranslate2>=4.5.0 \
    faster-whisper>=1.1.0 \
    stable-ts>=2.18.0 \
    requests>=2.31.0 \
    aiohttp>=3.9.0 \
    redis>=5.0.0 \
    numpy>=1.24.0 \
    opencc-python-reimplemented>=0.1.7

# è¤‡è£½ç¨‹å¼ç¢¼å’Œè³‡æºæª”æ¡ˆ
COPY processor/processor.py .
COPY processor/simplified_to_traditional.txt .
COPY processor/china_to_taiwan.txt .

# ğŸŒŸ å•Ÿå‹•æ™‚è¨­å®š LD_LIBRARY_PATHï¼Œè®“ CTranslate2 å„ªå…ˆä½¿ç”¨æˆ‘å€‘å®‰è£çš„ cuDNN
CMD ["sh", "-c", "export LD_LIBRARY_PATH=/usr/local/lib/ctranslate2:$(python -c 'import nvidia.cudnn; print(nvidia.cudnn.__path__[0])')/lib:$LD_LIBRARY_PATH && python processor.py"]