"""
ASR æ¨¡çµ„ - èªéŸ³è¾¨è­˜
"""
import os
import sys
import time
import numpy as np
from collections import Counter

from config import (
    ASR_MODEL_NAME, MODEL_CACHE_DIR, USE_KOTOBA_PIPELINE,
    SAMPLE_RATE, SOURCE_LANG_CODE, MIN_AUDIO_ENERGY,
    USE_VAD, VAD_THRESHOLD, SUPPRESS_SILENCE, ONLY_VOICE_FREQ,
    AVG_PROB_THRESHOLD, MAX_INSTANT_WORDS
)

# === å…¨åŸŸè®Šæ•¸ ===
asr_model = None
DEVICE = "cpu"
COMPUTE_TYPE = "int8"
USING_KOTOBA_PIPELINE = False
TRANSFORMERS_AVAILABLE = False


def setup_environment():
    """è¨­å®šç’°å¢ƒè®Šæ•¸å’Œ CUDA"""
    global DEVICE, COMPUTE_TYPE, TRANSFORMERS_AVAILABLE
    
    # ç¢ºä¿ cuDNN è·¯å¾‘æ­£ç¢º
    try:
        import nvidia.cudnn
        cudnn_lib = os.path.join(nvidia.cudnn.__path__[0], "lib")
        current_ld = os.environ.get("LD_LIBRARY_PATH", "")
        if cudnn_lib not in current_ld:
            os.environ["LD_LIBRARY_PATH"] = f"{cudnn_lib}:{current_ld}"
        print(f"âœ… cuDNN è·¯å¾‘å·²è¨­å®š: {cudnn_lib}", file=sys.stderr, flush=True)
    except ImportError:
        print("âš ï¸ nvidia-cudnn æœªå®‰è£", file=sys.stderr, flush=True)
    
    import torch
    print(f"PyTorch: {torch.__version__}", file=sys.stderr, flush=True)
    print(f"CUDA å¯ç”¨: {torch.cuda.is_available()}", file=sys.stderr, flush=True)
    if torch.cuda.is_available():
        print(f"CUDA ç‰ˆæœ¬: {torch.version.cuda}", file=sys.stderr, flush=True)
        print(f"GPU: {torch.cuda.get_device_name(0)}", file=sys.stderr, flush=True)
        DEVICE = "cuda"
        COMPUTE_TYPE = "float16"
    
    import stable_whisper
    print(f"âœ… stable-ts ç‰ˆæœ¬: {stable_whisper.__version__}", file=sys.stderr, flush=True)
    
    try:
        from transformers import pipeline as hf_pipeline
        TRANSFORMERS_AVAILABLE = True
        print("âœ… Transformers pipeline å¯ç”¨", file=sys.stderr, flush=True)
    except ImportError:
        print("âš ï¸ Transformers æœªå®‰è£ï¼Œå°‡ä½¿ç”¨ faster-whisper", file=sys.stderr, flush=True)


def init_asr_model():
    """åˆå§‹åŒ– ASR æ¨¡å‹"""
    global asr_model, DEVICE, COMPUTE_TYPE, USING_KOTOBA_PIPELINE
    
    import torch
    import stable_whisper
    import requests
    import threading
    from config import LLM_API_URL, LLM_MODEL
    
    # ä¸¦è¡Œæ¸¬è©¦ LLM é€£ç·šä¸¦é ç†±æ¨¡å‹
    def test_llm_async():
        import time as _time
        max_retries = 5
        for attempt in range(max_retries):
            try:
                print(f"ğŸ”„ ç­‰å¾… LLM æ¨¡å‹è¼‰å…¥... ({attempt + 1}/{max_retries})", file=sys.stderr, flush=True)
                test_resp = requests.post(
                    LLM_API_URL,
                    json={"model": LLM_MODEL, "prompt": "æ¸¬è©¦", "stream": False, "think": False},
                    timeout=60  # é¦–æ¬¡è¼‰å…¥éœ€è¦è¼ƒé•·æ™‚é–“
                )
                if test_resp.status_code == 200:
                    print(f"âœ… LLM ç¿»è­¯å¼•æ“å°±ç·’ ({LLM_MODEL})", file=sys.stderr, flush=True)
                    return
                else:
                    print(f"âš ï¸ LLM å›æ‡‰ç•°å¸¸: {test_resp.status_code}", file=sys.stderr, flush=True)
            except requests.exceptions.Timeout:
                print(f"âš ï¸ LLM è¼‰å…¥ä¸­ï¼Œç­‰å¾…...", file=sys.stderr, flush=True)
                _time.sleep(2)
            except requests.exceptions.ConnectionError:
                print(f"âš ï¸ Ollama å°šæœªå°±ç·’ï¼Œç­‰å¾…...", file=sys.stderr, flush=True)
                _time.sleep(2)
            except Exception as e:
                print(f"âš ï¸ LLM æ¸¬è©¦å¤±æ•—: {e}", file=sys.stderr, flush=True)
                _time.sleep(2)
        print(f"âš ï¸ LLM é ç†±å¤±æ•—ï¼Œç¿»è­¯å¯èƒ½å»¶é²", file=sys.stderr, flush=True)
    
    # å•Ÿå‹• LLM æ¸¬è©¦ï¼ˆéé˜»å¡ï¼‰
    llm_thread = threading.Thread(target=test_llm_async, daemon=True)
    llm_thread.start()

    start = time.time()
    
    # æ ¹æ“šæ¨¡å‹é¡å‹é¸æ“‡è¼‰å…¥æ–¹å¼
    if USE_KOTOBA_PIPELINE:
        if not TRANSFORMERS_AVAILABLE:
            print(f"âš ï¸ ä½¿ç”¨ Kotoba éœ€è¦ Transformersï¼Œä½†æœªå®‰è£", file=sys.stderr, flush=True)
            print(f"ğŸ”„ è‡ªå‹•åˆ‡æ›åˆ° large-v3 (faster-whisper)...", file=sys.stderr, flush=True)
        else:
            try:
                from transformers import pipeline as hf_pipeline, AutoModelForSpeechSeq2Seq, AutoProcessor
                
                model_version = "v2.2" if "v2.2" in ASR_MODEL_NAME else "v2.1"
                print(f"ğŸ”„ ä½¿ç”¨ Transformers Pipeline è¼‰å…¥ Kotoba-Whisper {model_version}...", file=sys.stderr, flush=True)
                
                torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32
                device = "cuda:0" if torch.cuda.is_available() else "cpu"
                
                # åªä½¿ç”¨ model_kwargsï¼Œä¸åœ¨ pipeline ä¸­é‡è¤‡è¨­å®š torch_dtype
                model_kwargs = {
                    "attn_implementation": "sdpa",
                    "low_cpu_mem_usage": True,
                } if torch.cuda.is_available() else {
                    "low_cpu_mem_usage": True,
                }
                
                asr_model = hf_pipeline(
                    "automatic-speech-recognition",
                    model=ASR_MODEL_NAME,
                    torch_dtype=torch_dtype,
                    device=device,
                    model_kwargs=model_kwargs,
                    batch_size=1,
                    trust_remote_code=True,
                )
                
                DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
                COMPUTE_TYPE = "float16" if DEVICE == "cuda" else "float32"
                USING_KOTOBA_PIPELINE = True
                
                print(f"âœ… Kotoba-Whisper {model_version} å·²å°±ç·’ (Transformers)", file=sys.stderr, flush=True)
                print(f"âœ… ğŸš€ GPU æ¨¡å¼: {DEVICE}/{COMPUTE_TYPE}, {time.time()-start:.1f}s", file=sys.stderr, flush=True)
                
                # ç­‰å¾… LLM æ¸¬è©¦å®Œæˆ
                llm_thread.join(timeout=5)
                return
                
            except Exception as e:
                print(f"âš ï¸ Kotoba Pipeline è¼‰å…¥å¤±æ•—: {e}", file=sys.stderr, flush=True)
                print(f"ğŸ”„ é€€å›ä½¿ç”¨ large-v3 (faster-whisper)...", file=sys.stderr, flush=True)
                import traceback
                traceback.print_exc()
    
    # æ¨™æº– faster-whisper + stable-ts
    USING_KOTOBA_PIPELINE = False
    fallback_model = "large-v3" if USE_KOTOBA_PIPELINE else ASR_MODEL_NAME
    
    def try_load_model(device, compute_type):
        try:
            print(f"ğŸ”„ ä½¿ç”¨ stable-ts è¼‰å…¥ {fallback_model}: {device}/{compute_type}...", file=sys.stderr, flush=True)
            
            model = stable_whisper.load_faster_whisper(
                fallback_model,
                device=device,
                compute_type=compute_type,
                download_root=MODEL_CACHE_DIR,
                cpu_threads=os.cpu_count() or 4,
                num_workers=2,
            )
            
            # ç§»é™¤é ç†±æ­¥é©Ÿä»¥åŠ é€Ÿè¼‰å…¥ï¼ˆé¦–æ¬¡æ¨ç†æœƒç¨æ…¢ä½†å¯æ¥å—ï¼‰
            return model
        except Exception as e:
            print(f"âš ï¸ {device}/{compute_type} å¤±æ•—: {e}", file=sys.stderr, flush=True)
            import traceback
            traceback.print_exc()
            return None

    for device, ctype in [("cuda", "float16"), ("cuda", "int8_float16"), ("cpu", "int8")]:
        if device == "cuda" and not torch.cuda.is_available():
            continue
        asr_model = try_load_model(device, ctype)
        if asr_model:
            DEVICE, COMPUTE_TYPE = device, ctype
            break
    
    if not asr_model:
        print("âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—", file=sys.stderr, flush=True)
        sys.exit(1)
    
    status = "ğŸš€ GPU" if DEVICE == "cuda" else "âš ï¸ CPU"
    print(f"âœ… {status} æ¨¡å¼ ({fallback_model}): {DEVICE}/{COMPUTE_TYPE}, {time.time()-start:.1f}s", file=sys.stderr, flush=True)
    print(f"âœ… stable-ts æ¨¡å‹å·²å°±ç·’", file=sys.stderr, flush=True)
    
    # ç­‰å¾… LLM æ¸¬è©¦å®Œæˆ
    llm_thread.join(timeout=5)


def check_voice_activity(audio_array: np.ndarray) -> bool:
    """ç°¡å–®çš„èªéŸ³æ´»å‹•åµæ¸¬ (VAD)"""
    rms = np.sqrt(np.mean(audio_array ** 2))
    return rms > MIN_AUDIO_ENERGY


def whisper_asr(audio_array: np.ndarray) -> str:
    """ä½¿ç”¨ ASR é€²è¡ŒèªéŸ³è¾¨è­˜"""
    if asr_model is None or not check_voice_activity(audio_array):
        return ""

    try:
        # Kotoba-Whisper (Transformers Pipeline)
        if USING_KOTOBA_PIPELINE:
            audio_input = {
                "raw": audio_array,
                "sampling_rate": SAMPLE_RATE
            }
            
            result = asr_model(
                audio_input,
                chunk_length_s=30,
                stride_length_s=[4, 2],  # å·¦å³ stride ç¢ºä¿é€£è²«
                batch_size=1,
                return_timestamps=True,  # ä½¿ç”¨ segment-level timestamps (ç©©å®š)
                ignore_warning=True,
                generate_kwargs={
                    "language": "ja",
                    "task": "transcribe",
                    "num_beams": 5,
                    "do_sample": False,
                    "repetition_penalty": 1.3,
                    "no_repeat_ngram_size": 4,
                    "length_penalty": 1.0,
                    "max_new_tokens": 440,
                },
            )
            
            # æå–æ–‡å­—
            text = ""
            if isinstance(result, dict):
                text = result.get("text", "")
            else:
                text = str(result)
            
            return text.strip()
        
        # æ¨™æº– faster-whisper + stable-ts
        result = asr_model.transcribe(
            audio_array,
            language=SOURCE_LANG_CODE,
            beam_size=5,
            best_of=5,
            patience=1.2,
            temperature=[0.0, 0.2],
            compression_ratio_threshold=2.0,
            condition_on_previous_text=False,
            no_speech_threshold=0.5,
            log_prob_threshold=AVG_PROB_THRESHOLD,
            initial_prompt="",
            word_timestamps=True,
            vad=USE_VAD,
            vad_threshold=VAD_THRESHOLD,
            suppress_silence=SUPPRESS_SILENCE,
            suppress_word_ts=True,
            min_word_dur=0.1,
            nonspeech_error=0.3,
            only_voice_freq=ONLY_VOICE_FREQ,
            regroup=True,
        )
        
        if hasattr(result, 'remove_repetition'):
            result.remove_repetition(max_words=1, verbose=False)
        
        # éæ¿¾ä½ç½®ä¿¡åº¦ç‰‡æ®µ
        text_parts = []
        if hasattr(result, 'segments'):
            for seg in result.segments:
                seg_text = seg.text if hasattr(seg, 'text') else str(seg)
                avg_prob = getattr(seg, 'avg_logprob', -0.5)
                no_speech = getattr(seg, 'no_speech_prob', 0.5)
                
                # å¹»è¦ºåµæ¸¬ï¼šç¬æ™‚è©
                if hasattr(seg, 'words') and seg.words:
                    instant_words = sum(1 for w in seg.words if hasattr(w, 'duration') and w.duration < 0.05)
                    instant_ratio = instant_words / len(seg.words) if seg.words else 0
                    if instant_ratio > MAX_INSTANT_WORDS:
                        print(f"âš ï¸ è·³éç¬æ™‚è©éå¤šç‰‡æ®µ: {seg_text[:30]}...", file=sys.stderr, flush=True)
                        continue
                
                # å¹»è¦ºåµæ¸¬ï¼šå–®è©é‡è¤‡
                if hasattr(seg, 'words') and seg.words and len(seg.words) >= 4:
                    word_texts = [w.word.strip() for w in seg.words if hasattr(w, 'word')]
                    if word_texts:
                        word_counts = Counter(word_texts)
                        max_word_count = max(word_counts.values())
                        if max_word_count > len(word_texts) * 0.4:
                            print(f"âš ï¸ è·³éå–®è©é‡è¤‡ç‰‡æ®µ: {seg_text[:30]}...", file=sys.stderr, flush=True)
                            continue
                
                # åˆ†ç´šç½®ä¿¡åº¦éæ¿¾
                if avg_prob > -0.4 and no_speech < 0.3:
                    text_parts.append(seg_text)
                elif avg_prob > -0.7 and no_speech < 0.4 and len(seg_text.strip()) >= 3:
                    text_parts.append(seg_text)
                elif avg_prob > -1.0 and no_speech < 0.15 and len(seg_text.strip()) >= 5:
                    text_parts.append(seg_text)
        else:
            text_parts = [result.text if hasattr(result, 'text') else str(result)]
        
        text = "".join(text_parts).strip()
        return text

    except Exception as e:
        print(f"ASR éŒ¯èª¤: {e}", file=sys.stderr, flush=True)
        import traceback
        traceback.print_exc()
        return ""
