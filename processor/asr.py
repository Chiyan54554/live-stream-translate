"""
ASR æ¨¡çµ„ - èªéŸ³è¾¨è­˜
ğŸš€ å„ªåŒ–ç‰ˆï¼šé å»ºç«‹åƒæ•¸å­—å…¸ã€æ¸›å°‘é‡è¤‡é‹ç®—
"""
import os
import sys
import time
import traceback
import numpy as np
from collections import Counter

# æ§åˆ¶è³‡è¨Šç´šæ—¥èªŒï¼šLOG_VERBOSE=1 æ™‚è¼¸å‡ºï¼›é è¨­éœéŸ³
LOG_VERBOSE = os.getenv("LOG_VERBOSE", "0") == "1"
def info(msg):
    if LOG_VERBOSE:
        print(msg, file=sys.stderr, flush=True)

from config import (
    ASR_MODEL_NAME, MODEL_CACHE_DIR, USE_KOTOBA_PIPELINE,
    SAMPLE_RATE, SOURCE_LANG_CODE, MIN_AUDIO_ENERGY,
    USE_VAD, VAD_THRESHOLD, SUPPRESS_SILENCE, ONLY_VOICE_FREQ,
    AVG_PROB_THRESHOLD, MAX_INSTANT_WORDS
)

# === å…¨åŸŸè®Šæ•¸ ===
asr_model = None
DEVICE = "cpu"
COMPUTE_TYPE = "int8"
USING_KOTOBA_PIPELINE = False
TRANSFORMERS_AVAILABLE = False

# ============================================================
# ğŸš€ é å»ºç«‹ ASR åƒæ•¸å­—å…¸ï¼ˆé¿å…æ¯æ¬¡å‘¼å«é‡æ–°å»ºç«‹ï¼‰
# ============================================================

# Kotoba Pipeline éŸ³è¨Šè¼¸å…¥æ¨¡æ¿ï¼ˆæ¯æ¬¡åªéœ€æ›´æ–° rawï¼‰
_KOTOBA_AUDIO_TEMPLATE = {
    "sampling_rate": SAMPLE_RATE
}

# Kotoba Pipeline ç”Ÿæˆåƒæ•¸ï¼ˆä¸è®Šï¼‰
_KOTOBA_GENERATE_KWARGS = {
    "language": "ja",
    "task": "transcribe",
    "num_beams": 5,
    "do_sample": False,
    "repetition_penalty": 1.3,
    "no_repeat_ngram_size": 4,
    "length_penalty": 1.0,
    "max_new_tokens": 440,
}

# Kotoba Pipeline å‘¼å«åƒæ•¸ï¼ˆä¸å«éŸ³è¨Šï¼‰
_KOTOBA_PIPELINE_KWARGS = {
    "chunk_length_s": 30,
    "stride_length_s": [4, 2],
    "batch_size": 1,
    "return_timestamps": True,
    "ignore_warning": True,
    "generate_kwargs": _KOTOBA_GENERATE_KWARGS,
}

# stable-ts è½‰éŒ„åƒæ•¸ï¼ˆä¸è®Šï¼‰
_STABLE_TS_KWARGS = {
    "language": SOURCE_LANG_CODE,
    "beam_size": 5,
    "best_of": 5,
    "patience": 1.2,
    "temperature": [0.0, 0.2],
    "compression_ratio_threshold": 2.0,
    "condition_on_previous_text": False,
    "no_speech_threshold": 0.5,
    "log_prob_threshold": AVG_PROB_THRESHOLD,
    "initial_prompt": "",
    "word_timestamps": True,
    "vad": USE_VAD,
    "vad_threshold": VAD_THRESHOLD,
    "suppress_silence": SUPPRESS_SILENCE,
    "suppress_word_ts": True,
    "min_word_dur": 0.1,
    "nonspeech_error": 0.3,
    "only_voice_freq": ONLY_VOICE_FREQ,
    "regroup": True,
}

# ç½®ä¿¡åº¦é–¾å€¼ï¼ˆé è¨ˆç®—ï¼‰
_CONFIDENCE_THRESHOLDS = (
    (-0.4, 0.3, 0),   # (avg_prob_min, no_speech_max, min_text_len)
    (-0.7, 0.4, 3),
    (-1.0, 0.15, 5),
)


def setup_environment():
    """è¨­å®šç’°å¢ƒè®Šæ•¸å’Œ CUDA"""
    global DEVICE, COMPUTE_TYPE, TRANSFORMERS_AVAILABLE
    
    # ç¢ºä¿ cuDNN è·¯å¾‘æ­£ç¢º
    try:
        import nvidia.cudnn
        cudnn_lib = os.path.join(nvidia.cudnn.__path__[0], "lib")
        current_ld = os.environ.get("LD_LIBRARY_PATH", "")
        if cudnn_lib not in current_ld:
            os.environ["LD_LIBRARY_PATH"] = f"{cudnn_lib}:{current_ld}"
        info(f"âœ… cuDNN è·¯å¾‘å·²è¨­å®š: {cudnn_lib}")
    except ImportError:
        info("âš ï¸ nvidia-cudnn æœªå®‰è£")
    
    import torch
    info(f"PyTorch: {torch.__version__}")
    info(f"CUDA å¯ç”¨: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        info(f"CUDA ç‰ˆæœ¬: {torch.version.cuda}")
        info(f"GPU: {torch.cuda.get_device_name(0)}")
        DEVICE = "cuda"
        COMPUTE_TYPE = "float16"
    
    import stable_whisper
    info(f"âœ… stable-ts ç‰ˆæœ¬: {stable_whisper.__version__}")
    
    try:
        from transformers import pipeline as hf_pipeline
        TRANSFORMERS_AVAILABLE = True
        info("âœ… Transformers pipeline å¯ç”¨")
    except ImportError:
        info("âš ï¸ Transformers æœªå®‰è£ï¼Œå°‡ä½¿ç”¨ faster-whisper")


def init_asr_model():
    """åˆå§‹åŒ– ASR æ¨¡å‹"""
    global asr_model, DEVICE, COMPUTE_TYPE, USING_KOTOBA_PIPELINE
    
    import torch
    import stable_whisper

    start = time.time()
    
    # æ ¹æ“šæ¨¡å‹é¡å‹é¸æ“‡è¼‰å…¥æ–¹å¼
    if USE_KOTOBA_PIPELINE:
        if not TRANSFORMERS_AVAILABLE:
            info(f"âš ï¸ ä½¿ç”¨ Kotoba éœ€è¦ Transformersï¼Œä½†æœªå®‰è£")
            info(f"ğŸ”„ è‡ªå‹•åˆ‡æ›åˆ° large-v3 (faster-whisper)...")
        else:
            try:
                from transformers import pipeline as hf_pipeline, AutoModelForSpeechSeq2Seq, AutoProcessor
                
                model_version = "v2.2" if "v2.2" in ASR_MODEL_NAME else "v2.1"
                info(f"ğŸ”„ ä½¿ç”¨ Transformers Pipeline è¼‰å…¥ Kotoba-Whisper {model_version}...")
                
                torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32
                device = "cuda:0" if torch.cuda.is_available() else "cpu"
                
                # åªä½¿ç”¨ model_kwargsï¼Œä¸åœ¨ pipeline ä¸­é‡è¤‡è¨­å®š torch_dtype
                model_kwargs = {
                    "attn_implementation": "sdpa",
                    "low_cpu_mem_usage": True,
                } if torch.cuda.is_available() else {
                    "low_cpu_mem_usage": True,
                }
                
                asr_model = hf_pipeline(
                    "automatic-speech-recognition",
                    model=ASR_MODEL_NAME,
                    torch_dtype=torch_dtype,
                    device=device,
                    model_kwargs=model_kwargs,
                    batch_size=1,
                    trust_remote_code=True,
                )
                
                DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
                COMPUTE_TYPE = "float16" if DEVICE == "cuda" else "float32"
                USING_KOTOBA_PIPELINE = True
                
                info(f"âœ… Kotoba-Whisper {model_version} å·²å°±ç·’ (Transformers)")
                info(f"âœ… ğŸš€ GPU æ¨¡å¼: {DEVICE}/{COMPUTE_TYPE}, {time.time()-start:.1f}s")
                return
                
            except Exception as e:
                info(f"âš ï¸ Kotoba Pipeline è¼‰å…¥å¤±æ•—: {e}")
                info(f"ğŸ”„ é€€å›ä½¿ç”¨ large-v3 (faster-whisper)...")
                traceback.print_exc()
    
    # æ¨™æº– faster-whisper + stable-ts
    USING_KOTOBA_PIPELINE = False
    fallback_model = "large-v3" if USE_KOTOBA_PIPELINE else ASR_MODEL_NAME
    
    def try_load_model(device, compute_type):
        try:
            info(f"ğŸ”„ ä½¿ç”¨ stable-ts è¼‰å…¥ {fallback_model}: {device}/{compute_type}...")
            
            model = stable_whisper.load_faster_whisper(
                fallback_model,
                device=device,
                compute_type=compute_type,
                download_root=MODEL_CACHE_DIR,
                cpu_threads=os.cpu_count() or 4,
                num_workers=2,
            )
            
            # ç§»é™¤é ç†±æ­¥é©Ÿä»¥åŠ é€Ÿè¼‰å…¥ï¼ˆé¦–æ¬¡æ¨ç†æœƒç¨æ…¢ä½†å¯æ¥å—ï¼‰
            return model
        except Exception as e:
            info(f"âš ï¸ {device}/{compute_type} å¤±æ•—: {e}")
            traceback.print_exc()
            return None

    for device, ctype in [("cuda", "float16"), ("cuda", "int8_float16"), ("cpu", "int8")]:
        if device == "cuda" and not torch.cuda.is_available():
            continue
        asr_model = try_load_model(device, ctype)
        if asr_model:
            DEVICE, COMPUTE_TYPE = device, ctype
            break
    
    if not asr_model:
        print("âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—", file=sys.stderr, flush=True)
        sys.exit(1)
    
    status = "ğŸš€ GPU" if DEVICE == "cuda" else "âš ï¸ CPU"
    info(f"âœ… {status} æ¨¡å¼ ({fallback_model}): {DEVICE}/{COMPUTE_TYPE}, {time.time()-start:.1f}s")
    info(f"âœ… stable-ts æ¨¡å‹å·²å°±ç·’")


def check_voice_activity(audio_array: np.ndarray) -> bool:
    """ç°¡å–®çš„èªéŸ³æ´»å‹•åµæ¸¬ (VAD)"""
    rms = np.sqrt(np.mean(audio_array ** 2))
    return rms > MIN_AUDIO_ENERGY


def whisper_asr(audio_array: np.ndarray) -> str:
    """ä½¿ç”¨ ASR é€²è¡ŒèªéŸ³è¾¨è­˜"""
    if asr_model is None or not check_voice_activity(audio_array):
        return ""

    try:
        # Kotoba-Whisper (Transformers Pipeline)
        if USING_KOTOBA_PIPELINE:
            # ä½¿ç”¨é å»ºç«‹çš„åƒæ•¸å­—å…¸
            audio_input = {"raw": audio_array, **_KOTOBA_AUDIO_TEMPLATE}
            
            result = asr_model(audio_input, **_KOTOBA_PIPELINE_KWARGS)
            
            # æå–æ–‡å­—ï¼ˆå„ªåŒ–åˆ¤æ–·ï¼‰
            if isinstance(result, dict):
                return result.get("text", "").strip()
            return str(result).strip()
        
        # æ¨™æº– faster-whisper + stable-tsï¼ˆä½¿ç”¨é å»ºç«‹åƒæ•¸ï¼‰
        result = asr_model.transcribe(audio_array, **_STABLE_TS_KWARGS)
        
        if hasattr(result, 'remove_repetition'):
            result.remove_repetition(max_words=1, verbose=False)
        
        # éæ¿¾ä½ç½®ä¿¡åº¦ç‰‡æ®µ
        text_parts = []
        if hasattr(result, 'segments'):
            for seg in result.segments:
                seg_text = seg.text if hasattr(seg, 'text') else str(seg)
                avg_prob = getattr(seg, 'avg_logprob', -0.5)
                no_speech = getattr(seg, 'no_speech_prob', 0.5)
                
                # å¹»è¦ºåµæ¸¬ï¼šç¬æ™‚è©
                if hasattr(seg, 'words') and seg.words:
                    instant_words = sum(1 for w in seg.words if hasattr(w, 'duration') and w.duration < 0.05)
                    instant_ratio = instant_words / len(seg.words) if seg.words else 0
                    if instant_ratio > MAX_INSTANT_WORDS:
                        print(f"âš ï¸ è·³éç¬æ™‚è©éå¤šç‰‡æ®µ: {seg_text[:30]}...", file=sys.stderr, flush=True)
                        continue
                
                # å¹»è¦ºåµæ¸¬ï¼šå–®è©é‡è¤‡
                if hasattr(seg, 'words') and seg.words and len(seg.words) >= 4:
                    word_texts = [w.word.strip() for w in seg.words if hasattr(w, 'word')]
                    if word_texts:
                        word_counts = Counter(word_texts)
                        max_word_count = max(word_counts.values())
                        if max_word_count > len(word_texts) * 0.4:
                            print(f"âš ï¸ è·³éå–®è©é‡è¤‡ç‰‡æ®µ: {seg_text[:30]}...", file=sys.stderr, flush=True)
                            continue
                
                # åˆ†ç´šç½®ä¿¡åº¦éæ¿¾ï¼ˆä½¿ç”¨é å®šç¾©é–¾å€¼ï¼‰
                seg_text_len = len(seg_text.strip())
                for prob_min, speech_max, min_len in _CONFIDENCE_THRESHOLDS:
                    if avg_prob > prob_min and no_speech < speech_max and seg_text_len >= min_len:
                        text_parts.append(seg_text)
                        break
        else:
            text_parts = [result.text if hasattr(result, 'text') else str(result)]
        
        text = "".join(text_parts).strip()
        return text

    except Exception as e:
        print(f"ASR éŒ¯èª¤: {e}", file=sys.stderr, flush=True)
        traceback.print_exc()
        return ""
