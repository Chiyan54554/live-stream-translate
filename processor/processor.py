import sys
import json
import time
from datetime import datetime, timezone, timedelta
import numpy as np
import redis
import os
import base64
import re
from concurrent.futures import ThreadPoolExecutor

# ğŸŒŸ è¨­å®šç’°å¢ƒè®Šæ•¸
os.environ["HF_HUB_DISABLE_SYMLINKS_WARNING"] = "1"

# ğŸŒŸ è¨­å®š cuDNN è·¯å¾‘
def setup_cudnn_path():
    possible_paths = [
        "/opt/conda/lib/python3.11/site-packages/nvidia/cudnn/lib",
        "/opt/conda/lib/python3.12/site-packages/nvidia/cudnn/lib",
        "/usr/local/cuda/lib64",
    ]
    existing = os.environ.get("LD_LIBRARY_PATH", "")
    new_paths = [p for p in possible_paths if os.path.exists(p)]
    if new_paths:
        os.environ["LD_LIBRARY_PATH"] = ":".join(new_paths) + ":" + existing

setup_cudnn_path()

# å¼•å…¥ä¾è³´
try:
    import torch
    print(f"PyTorch ç‰ˆæœ¬: {torch.__version__}", file=sys.stderr, flush=True)
    print(f"CUDA å¯ç”¨: {torch.cuda.is_available()}", file=sys.stderr, flush=True)
    if torch.cuda.is_available():
        print(f"CUDA ç‰ˆæœ¬: {torch.version.cuda}", file=sys.stderr, flush=True)
        print(f"GPU: {torch.cuda.get_device_name(0)}", file=sys.stderr, flush=True)
    
    from faster_whisper import WhisperModel
    from deep_translator import GoogleTranslator
except ImportError as e:
    print(f"éŒ¯èª¤ï¼šç¼ºå°‘ä¾è³´å¥—ä»¶: {e}", file=sys.stderr, flush=True)
    sys.exit(1)

# --- é…ç½®åƒæ•¸ ---
SAMPLE_RATE = 16000           # FFmpeg æ‡‰è©²è¼¸å‡º 16kHz
BYTES_PER_SAMPLE = 2          # 16-bit PCM
SOURCE_LANG_CODE = "ja"       # æºèªè¨€ (æ—¥æ–‡)
TARGET_LANG_CODE = "zh-TW"    # ç›®æ¨™èªè¨€ (ä¸­æ–‡)

# ğŸŒŸ å„ªåŒ–ï¼šèª¿æ•´ç·©è¡é…ç½®ä»¥å¹³è¡¡æº–ç¢ºç‡å’Œé€Ÿåº¦
BUFFER_DURATION_S = 3.0       # ç¸®çŸ­è‡³ 3 ç§’ï¼ŒåŠ å¿«å›æ‡‰é€Ÿåº¦
OVERLAP_DURATION_S = 0.5      # ä¿ç•™ 0.5 ç§’é‡ç–Šï¼Œé¿å…èªå¥åˆ‡æ–·
MIN_AUDIO_ENERGY = 0.005      # é™ä½èƒ½é‡é–¾å€¼ï¼Œæ•æ‰æ›´å¤šèªéŸ³

# Redis é…ç½®
REDIS_HOST = os.getenv('REDIS_HOST', 'redis')
REDIS_PORT = int(os.getenv('REDIS_PORT', 6379))

AUDIO_CHANNEL = "audio_feed"
TRANSLATION_CHANNEL = "translation_feed"

# ğŸŒŸ faster-whisper æ”¯æ´çš„æ¨¡å‹: tiny, base, small, medium, large-v2, large-v3
ASR_MODEL_NAME = os.getenv('ASR_MODEL_NAME', 'medium')

# ä¿®æ­£ï¼šfaster-whisper ä½¿ç”¨çš„å¿«å–ç›®éŒ„
MODEL_CACHE_DIR = os.getenv('MODEL_CACHE_DIR', '/root/.cache/huggingface/hub')

# ğŸŒŸ ä¿®æ”¹ï¼šé è¨­å˜—è©¦ CUDAï¼Œä½†æº–å‚™é™ç´š
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
COMPUTE_TYPE = "float16" if DEVICE == "cuda" else "int8"

# å…¨å±€è³‡æº
asr_model = None
translator = None
audio_buffer = b''
overlap_buffer = b''  # ğŸŒŸ æ–°å¢ï¼šé‡ç–Šç·©è¡å€

# ğŸŒŸ æ–°å¢ï¼šåŸ·è¡Œç·’æ± ç”¨æ–¼ä¸¦è¡Œç¿»è­¯
executor = ThreadPoolExecutor(max_workers=2)

# ğŸŒŸ æ–°å¢ï¼šä¸Šä¸€æ¬¡è½‰éŒ„çµæœï¼Œç”¨æ–¼å»é‡
last_transcription = ""

# ----------------------------------------------------
# è³‡æºåˆå§‹åŒ–
# ----------------------------------------------------

def init_global_resources():
    """è¼‰å…¥ faster-whisper æ¨¡å‹å’Œåˆå§‹åŒ–ç¿»è­¯å™¨ã€‚"""
    global asr_model, translator, DEVICE, COMPUTE_TYPE
    
    print(f"="*50, file=sys.stderr, flush=True)
    print(f"åˆå§‹è¨­å‚™: {DEVICE}, è¨ˆç®—é¡å‹: {COMPUTE_TYPE}", file=sys.stderr, flush=True)
    print(f"æ¨¡å‹åç¨±: {ASR_MODEL_NAME}", file=sys.stderr, flush=True)
    print(f"å¿«å–ç›®éŒ„: {MODEL_CACHE_DIR}", file=sys.stderr, flush=True)
    print(f"="*50, file=sys.stderr, flush=True)

    # 1. åˆå§‹åŒ–ç¿»è­¯å™¨
    try:
        translator = GoogleTranslator(source=SOURCE_LANG_CODE, target=TARGET_LANG_CODE)
        print("âœ… ç¿»è­¯å¼•æ“åˆå§‹åŒ–æˆåŠŸã€‚", file=sys.stderr, flush=True)
    except Exception as e:
        print(f"âŒ ç¿»è­¯å¼•æ“åˆå§‹åŒ–å¤±æ•—: {e}", file=sys.stderr, flush=True)
        sys.exit(1)

    # 2. è¼‰å…¥æ¨¡å‹ (å¸¶æœ‰è‡ªå‹•é™ç´š)
    def try_load_model(device, compute_type):
        try:
            print(f"ğŸ”„ å˜—è©¦ {device}/{compute_type}...", file=sys.stderr, flush=True)
            model = WhisperModel(
                ASR_MODEL_NAME,
                device=device,
                compute_type=compute_type,
                download_root=MODEL_CACHE_DIR,
                local_files_only=False,
                cpu_threads=4,
                num_workers=1,
            )
            # ğŸŒŸ æ¸¬è©¦æ¨¡å‹æ˜¯å¦çœŸçš„èƒ½é‹ä½œ
            test_audio = np.zeros(16000, dtype=np.float32)
            list(model.transcribe(test_audio, language="ja"))
            return model
        except Exception as e:
            print(f"âš ï¸ {device}/{compute_type} å¤±æ•—: {e}", file=sys.stderr, flush=True)
            return None

    print(f"æ­£åœ¨è¼‰å…¥ faster-whisper æ¨¡å‹...", file=sys.stderr, flush=True)
    start_time = time.time()
    
    # ğŸŒŸ å˜—è©¦é †åº
    attempts = [
        ("cuda", "float16"),
        ("cuda", "int8_float16"),
        ("cuda", "int8"),
        ("cpu", "int8"),
        ("cpu", "float32"),
    ]
    
    for device, compute_type in attempts:
        if device == "cuda" and not torch.cuda.is_available():
            continue
        asr_model = try_load_model(device, compute_type)
        if asr_model is not None:
            DEVICE = device
            COMPUTE_TYPE = compute_type
            break
    
    if asr_model is None:
        print("âŒ æ‰€æœ‰è¼‰å…¥å˜—è©¦å‡å¤±æ•—", file=sys.stderr, flush=True)
        sys.exit(1)
    
    elapsed = time.time() - start_time
    print(f"âœ… æ¨¡å‹è¼‰å…¥æˆåŠŸï¼è¨­å‚™: {DEVICE}, é¡å‹: {COMPUTE_TYPE}, è€—æ™‚: {elapsed:.2f}s", file=sys.stderr, flush=True)

def check_voice_activity(audio_array: np.ndarray) -> bool:
    """ç°¡å–®çš„èªéŸ³æ´»å‹•åµæ¸¬ (VAD)ã€‚"""
    rms_energy = np.sqrt(np.mean(audio_array ** 2))
    return rms_energy > MIN_AUDIO_ENERGY

def whisper_asr(audio_array: np.ndarray) -> str:
    """ä½¿ç”¨ faster-whisper é€²è¡ŒèªéŸ³è¾¨è­˜ã€‚"""
    if asr_model is None:
        return ""

    try:
        # ğŸŒŸ æª¢æŸ¥èªéŸ³æ´»å‹•
        if not check_voice_activity(audio_array):
            return ""
        
        # ğŸŒŸ faster-whisper ç›´æ¥æ¥å— numpy array
        segments, info = asr_model.transcribe(
            audio_array,
            language=SOURCE_LANG_CODE,
            beam_size=5,
            best_of=5,
            patience=1.5,
            temperature=0.0,
            compression_ratio_threshold=2.4,
            condition_on_previous_text=False,  # ğŸŒŸ é—œé–‰ä»¥é¿å…éŒ¯èª¤ç´¯ç©
            no_speech_threshold=0.6,
            log_prob_threshold=-1.0,
            initial_prompt="ã“ã‚Œã¯æ—¥æœ¬èªã®ä¼šè©±ã§ã™ã€‚",
            vad_filter=True,  # ğŸŒŸ å•Ÿç”¨å…§å»º VAD éæ¿¾
            vad_parameters=dict(
                min_silence_duration_ms=500,  # æœ€å°éœéŸ³æ™‚é•·
                speech_pad_ms=200,            # èªéŸ³å‰å¾Œå¡«å……
            ),
        )
        
        # æ”¶é›†æ‰€æœ‰ç‰‡æ®µçš„æ–‡å­—
        text_parts = [segment.text for segment in segments]
        return "".join(text_parts).strip()

    except Exception as e:
        print(f"ASR è™•ç†å¤±æ•—: {e}", file=sys.stderr, flush=True)
        return ""

def google_mt(text: str) -> str:
    """ä½¿ç”¨ Deep Translator é€²è¡Œç¿»è­¯ã€‚"""
    if not text or translator is None:
        return ""
    try:
        return translator.translate(text)
    except Exception as e:
        print(f"ç¿»è­¯å¤±æ•—: {e}", file=sys.stderr, flush=True)
        return f"MT_FAILURE: {text}"

def filter_text(text: str) -> str:
    """éæ¿¾ç„¡æ•ˆæ–‡å­—ã€‚"""
    if not text:
        return ""
    
    # æ—¥æ–‡å­—ç¬¦éæ¿¾
    japanese_pattern = re.compile(r'[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FFF\uFF00-\uFFEF\u0020-\u007E]+')
    filtered_segments = japanese_pattern.findall(text)
    cleaned_text = "".join(filtered_segments).strip()
    
    # ä¸æƒ³è¦çš„çŸ­èª
    unwanted_phrases = [
        "[éŸ³å£°ãªã—]", "ã”è¦–è´ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸ", "æœ€å¾Œã¾ã§ã”è¦–è´ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸ",
        "(å¹•ã®é–‹ã‘ã‚‹éŸ³)", "(æ‹æ‰‹)", "(ç¬‘ã„)", "(ãŸã‚æ¯)", "å­—å¹•",
    ]
    
    for phrase in unwanted_phrases:
        if phrase in cleaned_text:
            return ""
    
    # éçŸ­çš„æ–‡å­—
    if len(cleaned_text) < 2:
        return ""
    
    return cleaned_text

def remove_duplicate(current: str, previous: str) -> str:
    """ç§»é™¤èˆ‡ä¸Šä¸€æ¬¡è½‰éŒ„é‡è¤‡çš„éƒ¨åˆ†ã€‚"""
    if not previous or not current:
        return current
    
    # æª¢æŸ¥æ˜¯å¦å®Œå…¨é‡è¤‡
    if current == previous:
        return ""
    
    # æª¢æŸ¥æ˜¯å¦ç‚ºå‰ä¸€æ¬¡çš„å­å­—ä¸²
    if current in previous:
        return ""
    
    # æª¢æŸ¥é‡ç–Šéƒ¨åˆ†ä¸¦ç§»é™¤
    for i in range(min(len(previous), len(current)), 0, -1):
        if previous[-i:] == current[:i]:
            return current[i:]
    
    return current

# ----------------------------------------------------
# æ ¸å¿ƒè™•ç†å‡½æ•¸
# ----------------------------------------------------

def process_audio_chunk(audio_data_b64: str, r):
    """è™•ç†éŸ³è¨Šå¡Šï¼Œä½¿ç”¨æ»‘å‹•è¦–çª—æ©Ÿåˆ¶ã€‚"""
    global audio_buffer, overlap_buffer, last_transcription
    
    # è§£ç¢¼éŸ³è¨Š
    raw_audio_bytes = base64.b64decode(audio_data_b64)
    
    # ğŸŒŸ åˆä½µé‡ç–Šç·©è¡å€å’Œæ–°æ•¸æ“š
    audio_buffer = overlap_buffer + audio_buffer + raw_audio_bytes
    
    # è¨ˆç®—ç›®æ¨™å¤§å°
    target_buffer_size = int(BUFFER_DURATION_S * SAMPLE_RATE * BYTES_PER_SAMPLE)
    overlap_size = int(OVERLAP_DURATION_S * SAMPLE_RATE * BYTES_PER_SAMPLE)
    
    if len(audio_buffer) < target_buffer_size:
        return
    
    # å–å‡ºè™•ç†çš„éŸ³è¨Š
    audio_to_process = audio_buffer[:target_buffer_size]
    
    # ğŸŒŸ ä¿ç•™é‡ç–Šéƒ¨åˆ†ä¾›ä¸‹æ¬¡ä½¿ç”¨
    overlap_buffer = audio_buffer[target_buffer_size - overlap_size:target_buffer_size]
    audio_buffer = audio_buffer[target_buffer_size:]
    
    # è½‰æ›ç‚º numpy array
    audio_array = np.frombuffer(audio_to_process, dtype=np.int16).astype(np.float32) / 32768.0
    
    # ASR è½‰éŒ„
    transcribed_text = whisper_asr(audio_array)
    
    # éæ¿¾æ–‡å­—
    transcribed_text = filter_text(transcribed_text)
    if not transcribed_text:
        return
    
    # ğŸŒŸ å»é™¤é‡è¤‡
    transcribed_text = remove_duplicate(transcribed_text, last_transcription)
    if not transcribed_text:
        return
    
    last_transcription = transcribed_text
    
    # ğŸŒŸ ä¸¦è¡ŒåŸ·è¡Œç¿»è­¯
    future = executor.submit(google_mt, transcribed_text)
    translated_text = future.result(timeout=5)
    
    # æ™‚é–“æˆ³
    tz = timezone(timedelta(hours=8))
    timestamp = datetime.now(tz).strftime("%H:%M:%S")
    
    result = {
        "timestamp": timestamp,
        "source_lang": SOURCE_LANG_CODE,
        "target_lang": TARGET_LANG_CODE,
        "duration_s": f"{BUFFER_DURATION_S:.3f}",
        "transcription": transcribed_text,
        "translation": translated_text
    }
    
    try:
        json_output = json.dumps(result, ensure_ascii=False)
        r.publish(TRANSLATION_CHANNEL, json_output)
    except Exception as e:
        print(f"ç™¼ä½ˆå¤±æ•—: {e}", file=sys.stderr, flush=True)


def main():
    """ä¸»å¾ªç’°ã€‚"""
    init_global_resources()

    try:
        r = redis.Redis(host=REDIS_HOST, port=REDIS_PORT, db=0)
        r.ping()
        print(f"Python æˆåŠŸé€£æ¥åˆ° Redis ({REDIS_HOST}:{REDIS_PORT})ã€‚", file=sys.stderr, flush=True)
    except Exception as e:
        print(f"è‡´å‘½éŒ¯èª¤ï¼šç„¡æ³•é€£æ¥åˆ° Redis: {e}", file=sys.stderr, flush=True)
        sys.exit(1)

    p = r.pubsub()
    p.subscribe(AUDIO_CHANNEL)
    print(f"Python æˆåŠŸè¨‚é–± Redis é »é“: {AUDIO_CHANNEL}ã€‚", file=sys.stderr, flush=True)

    for message in p.listen():
        if message['type'] == 'message':
            audio_chunk_b64 = message['data'].decode('utf-8')
            process_audio_chunk(audio_chunk_b64, r)
        elif message['type'] == 'subscribe':
            print(f"å·²æˆåŠŸè¨‚é–± {message['channel'].decode('utf-8')}", file=sys.stderr, flush=True)

if __name__ == "__main__":
    main()