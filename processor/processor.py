import sys
import json
import time
from datetime import datetime, timezone, timedelta
import numpy as np
import redis
import os
import base64
import re
from concurrent.futures import ThreadPoolExecutor
from collections import deque

os.environ["HF_HUB_DISABLE_SYMLINKS_WARNING"] = "1"

# ğŸŒŸ ç¢ºä¿ cuDNN è·¯å¾‘æ­£ç¢ºï¼ˆåœ¨ import torch ä¹‹å‰ï¼‰
try:
    import nvidia.cudnn
    cudnn_lib = os.path.join(nvidia.cudnn.__path__[0], "lib")
    current_ld = os.environ.get("LD_LIBRARY_PATH", "")
    if cudnn_lib not in current_ld:
        os.environ["LD_LIBRARY_PATH"] = f"{cudnn_lib}:{current_ld}"
    print(f"âœ… cuDNN è·¯å¾‘å·²è¨­å®š: {cudnn_lib}", file=sys.stderr, flush=True)
except ImportError:
    print("âš ï¸ nvidia-cudnn æœªå®‰è£", file=sys.stderr, flush=True)

try:
    import torch
    print(f"PyTorch: {torch.__version__}", file=sys.stderr, flush=True)
    print(f"CUDA å¯ç”¨: {torch.cuda.is_available()}", file=sys.stderr, flush=True)
    if torch.cuda.is_available():
        print(f"CUDA ç‰ˆæœ¬: {torch.version.cuda}", file=sys.stderr, flush=True)
        print(f"GPU: {torch.cuda.get_device_name(0)}", file=sys.stderr, flush=True)
    
    # ğŸ¯ ä½¿ç”¨ stable-ts æ•´åˆ faster-whisper
    import stable_whisper
    from deep_translator import GoogleTranslator
    
    print(f"âœ… stable-ts ç‰ˆæœ¬: {stable_whisper.__version__}", file=sys.stderr, flush=True)
    
except ImportError as e:
    print(f"éŒ¯èª¤ï¼š{e}", file=sys.stderr, flush=True)
    sys.exit(1)

# --- é…ç½®åƒæ•¸ ---
SAMPLE_RATE = 16000
BYTES_PER_SAMPLE = 2
SOURCE_LANG_CODE = "ja"
TARGET_LANG_CODE = "zh-TW"

# ğŸ¯ æº–ç¢ºç‡å„ªåŒ–ï¼šå¹³è¡¡ç·©è¡èˆ‡å»¶é²
BUFFER_DURATION_S = 4.0       # ğŸ¯ 4 ç§’ç·©è¡ï¼Œå¹³è¡¡æº–ç¢ºåº¦èˆ‡å»¶é²
OVERLAP_DURATION_S = 0.8      # ğŸ¯ é©åº¦é‡ç–Šç¢ºä¿é€£è²«æ€§
MIN_AUDIO_ENERGY = 0.006      # ğŸ¯ é©ä¸­çš„èƒ½é‡é–€æª»

REDIS_HOST = os.getenv('REDIS_HOST', 'redis')
REDIS_PORT = int(os.getenv('REDIS_PORT', 6379))
AUDIO_CHANNEL = "audio_feed"
TRANSLATION_CHANNEL = "translation_feed"

# ğŸ¯ ä½¿ç”¨ large-v3-turboï¼šæ›´æ–°çš„æ¨¡å‹ï¼Œå¹»è¦ºæ›´å°‘
ASR_MODEL_NAME = os.getenv('ASR_MODEL_NAME', 'large-v3-turbo')
MODEL_CACHE_DIR = os.getenv('MODEL_CACHE_DIR', '/root/.cache/huggingface/hub')

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
COMPUTE_TYPE = "float16" if DEVICE == "cuda" else "int8"

# ğŸ¯ stable-ts èˆ‡ VAD ç›¸é—œè¨­å®š
USE_STABLE_TS = True                    # å•Ÿç”¨ stable-ts
USE_VAD = True                          # å•Ÿç”¨ Silero VAD
VAD_THRESHOLD = 0.35                    # VAD èªéŸ³åµæ¸¬é–¾å€¼
SUPPRESS_SILENCE = True                 # éœéŸ³æŠ‘åˆ¶
HALLUCINATION_SILENCE_TH = 2.0          # ğŸ¯ å¹»è¦ºéœéŸ³é–¾å€¼ï¼ˆç§’ï¼‰
AVG_PROB_THRESHOLD = -0.8               # å¹³å‡ç½®ä¿¡åº¦é–¾å€¼
MAX_INSTANT_WORDS = 0.35                # ğŸ¯ é™ä½é–¾å€¼ï¼Œæ›´ç©æ¥µéæ¿¾å¹»è¦º
ONLY_VOICE_FREQ = False                 # æ˜¯å¦åªä¿ç•™èªéŸ³é »ç‡ (200-5000 Hz)

asr_model = None
translator = None
audio_buffer = b''
overlap_buffer = b''         # ğŸ¯ æ¢å¾©é‡ç–Šç·©è¡å€
last_transcription = ""
last_full_sentence = ""       # ğŸ¯ æ–°å¢ï¼šè¨˜éŒ„ä¸Šä¸€å€‹å®Œæ•´å¥å­
pending_text = ""             # ğŸ¯ æ–°å¢ï¼šå¾…è™•ç†çš„ä¸å®Œæ•´æ–‡å­—
last_publish_time = 0
recent_texts = deque(maxlen=10)
context_history = deque(maxlen=8)  # ğŸ¯ å¢åŠ ä¸Šä¸‹æ–‡é•·åº¦
executor = ThreadPoolExecutor(max_workers=2)

MIN_PUBLISH_INTERVAL = 0.8    # ğŸ¯ ç¸®çŸ­æœ€å°é–“éš”
SIMILARITY_THRESHOLD = 0.7    # ğŸ¯ æé«˜ç›¸ä¼¼åº¦é–¾å€¼

def init_global_resources():
    global asr_model, translator, DEVICE, COMPUTE_TYPE
    
    print(f"="*50, file=sys.stderr, flush=True)
    print(f"ğŸ¯ è¨­å‚™: {DEVICE}, è¨ˆç®—é¡å‹: {COMPUTE_TYPE}", file=sys.stderr, flush=True)
    print(f"ğŸ¯ æ¨¡å‹: {ASR_MODEL_NAME}", file=sys.stderr, flush=True)
    print(f"ğŸ¯ stable-ts: {USE_STABLE_TS}, VAD: {USE_VAD}", file=sys.stderr, flush=True)
    print(f"="*50, file=sys.stderr, flush=True)

    try:
        translator = GoogleTranslator(source=SOURCE_LANG_CODE, target=TARGET_LANG_CODE)
        print("âœ… ç¿»è­¯å¼•æ“å°±ç·’", file=sys.stderr, flush=True)
    except Exception as e:
        print(f"âŒ ç¿»è­¯å¼•æ“å¤±æ•—: {e}", file=sys.stderr, flush=True)
        sys.exit(1)

    def try_load_model(device, compute_type):
        try:
            print(f"ğŸ”„ ä½¿ç”¨ stable-ts è¼‰å…¥: {device}/{compute_type}...", file=sys.stderr, flush=True)
            
            # ğŸ¯ ä½¿ç”¨ stable-ts çš„ load_faster_whisper
            model = stable_whisper.load_faster_whisper(
                ASR_MODEL_NAME,
                device=device,
                compute_type=compute_type,
                download_root=MODEL_CACHE_DIR,
                cpu_threads=os.cpu_count() or 4,
                num_workers=2,
            )
            
            # é ç†±æ¸¬è©¦
            warmup_audio = np.zeros(16000, dtype=np.float32)
            _ = model.transcribe(
                warmup_audio,
                language="ja",
                vad=False,  # é ç†±æ™‚é—œé–‰ VAD åŠ é€Ÿ
                suppress_silence=False,
            )
            
            return model
        except Exception as e:
            print(f"âš ï¸ {device}/{compute_type} å¤±æ•—: {e}", file=sys.stderr, flush=True)
            import traceback
            traceback.print_exc()
            return None

    start = time.time()
    for device, ctype in [("cuda", "float16"), ("cuda", "int8_float16"), ("cpu", "int8")]:
        if device == "cuda" and not torch.cuda.is_available():
            continue
        asr_model = try_load_model(device, ctype)
        if asr_model:
            DEVICE, COMPUTE_TYPE = device, ctype
            break
    
    if not asr_model:
        print("âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—", file=sys.stderr, flush=True)
        sys.exit(1)
    
    status = "ğŸš€ GPU" if DEVICE == "cuda" else "âš ï¸ CPU"
    print(f"âœ… {status} æ¨¡å¼: {DEVICE}/{COMPUTE_TYPE}, {time.time()-start:.1f}s", file=sys.stderr, flush=True)
    print(f"âœ… stable-ts æ¨¡å‹å·²å°±ç·’", file=sys.stderr, flush=True)

def check_voice_activity(audio_array: np.ndarray) -> bool:
    """ç°¡å–®çš„èªéŸ³æ´»å‹•åµæ¸¬ (VAD)ã€‚"""
    rms = np.sqrt(np.mean(audio_array ** 2))
    return rms > MIN_AUDIO_ENERGY

def get_context_prompt() -> str:
    """ç”Ÿæˆä¸Šä¸‹æ–‡æç¤º - é‡å°ç›´æ’­å„ªåŒ–"""
    # ğŸ¯ ç§»é™¤ initial_prompt - å®ƒå¯èƒ½è¢« Whisper ç•¶æˆè½‰éŒ„è¼¸å‡º
    # è¿”å›ç©ºå­—ä¸²ä»¥é¿å…å¹»è¦º
    return ""

def whisper_asr(audio_array: np.ndarray) -> str:
    """ä½¿ç”¨ stable-ts + faster-whisper é€²è¡ŒèªéŸ³è¾¨è­˜ - å®Œæ•´æ•´åˆç‰ˆ"""
    if asr_model is None or not check_voice_activity(audio_array):
        return ""

    try:
        # ğŸ¯ ä½¿ç”¨ stable-ts çš„ transcribe æ–¹æ³•
        # é€™æœƒè‡ªå‹•æ•´åˆ VADã€éœéŸ³æŠ‘åˆ¶ã€é‡è¤‡ç§»é™¤ç­‰åŠŸèƒ½
        result = asr_model.transcribe(
            audio_array,
            language=SOURCE_LANG_CODE,
            
            # === åŸºæœ¬ Whisper åƒæ•¸ ===
            beam_size=5,
            best_of=5,
            patience=1.2,
            temperature=[0.0, 0.2],
            compression_ratio_threshold=2.0,
            condition_on_previous_text=False,  # ğŸ¯ é—œé–‰é¿å…éŒ¯èª¤ç´¯ç©
            no_speech_threshold=0.5,
            log_prob_threshold=AVG_PROB_THRESHOLD,
            initial_prompt=get_context_prompt(),
            word_timestamps=True,  # ğŸ¯ å•Ÿç”¨è©ç´šæ™‚é–“æˆ³ä»¥æ”¯æ´å»é‡è¤‡
            
            # === stable-ts VAD èˆ‡éœéŸ³æŠ‘åˆ¶ ===
            vad=USE_VAD,                      # ğŸ¯ ä½¿ç”¨ Silero VAD
            vad_threshold=VAD_THRESHOLD,      # ğŸ¯ VAD é–¾å€¼
            suppress_silence=SUPPRESS_SILENCE, # ğŸ¯ éœéŸ³æŠ‘åˆ¶
            suppress_word_ts=True,            # ğŸ¯ æŠ‘åˆ¶éœéŸ³æ™‚çš„æ™‚é–“æˆ³
            
            # === é¡å¤–çš„ stable-ts åƒæ•¸ ===
            min_word_dur=0.1,                 # æœ€çŸ­è©æŒçºŒæ™‚é–“
            nonspeech_error=0.3,              # éèªéŸ³èª¤å·®å®¹å¿åº¦
            only_voice_freq=ONLY_VOICE_FREQ,  # åªä¿ç•™èªéŸ³é »ç‡ç¯„åœ
            
            regroup=True,  # ğŸ¯ è‡ªå‹•é‡æ–°åˆ†çµ„ç‰‡æ®µ
        )
        
        # ğŸ¯ stable-ts çš„æ ¸å¿ƒåŠŸèƒ½ï¼šç§»é™¤é‡è¤‡
        if hasattr(result, 'remove_repetition'):
            result.remove_repetition(max_words=1, verbose=False)
        
        # ğŸ¯ éæ¿¾ä½ç½®ä¿¡åº¦ç‰‡æ®µ
        text_parts = []
        if hasattr(result, 'segments'):
            for seg in result.segments:
                # å–å¾—ç‰‡æ®µå±¬æ€§
                seg_text = seg.text if hasattr(seg, 'text') else str(seg)
                avg_prob = getattr(seg, 'avg_logprob', -0.5)
                no_speech = getattr(seg, 'no_speech_prob', 0.5)
                
                # ğŸ¯ å¹»è¦ºåµæ¸¬ï¼šæª¢æŸ¥æ˜¯å¦æœ‰éå¤šç¬æ™‚è©
                if hasattr(seg, 'words') and seg.words:
                    instant_words = sum(1 for w in seg.words if hasattr(w, 'duration') and w.duration < 0.05)
                    instant_ratio = instant_words / len(seg.words) if seg.words else 0
                    if instant_ratio > MAX_INSTANT_WORDS:
                        print(f"âš ï¸ è·³éç¬æ™‚è©éå¤šç‰‡æ®µ: {seg_text[:30]}...", file=sys.stderr, flush=True)
                        continue
                
                # ğŸ¯ é¡å¤–å¹»è¦ºåµæ¸¬ï¼šæª¢æŸ¥å–®è©é‡è¤‡
                if hasattr(seg, 'words') and seg.words and len(seg.words) >= 4:
                    word_texts = [w.word.strip() for w in seg.words if hasattr(w, 'word')]
                    if word_texts:
                        from collections import Counter
                        word_counts = Counter(word_texts)
                        max_word_count = max(word_counts.values())
                        # å¦‚æœæŸå€‹è©å‡ºç¾è¶…é 40% çš„æ¬¡æ•¸ï¼Œè¦–ç‚ºå¹»è¦º
                        if max_word_count > len(word_texts) * 0.4:
                            print(f"âš ï¸ è·³éå–®è©é‡è¤‡ç‰‡æ®µ: {seg_text[:30]}...", file=sys.stderr, flush=True)
                            continue
                
                # ğŸ¯ åˆ†ç´šç½®ä¿¡åº¦éæ¿¾
                if avg_prob > -0.4 and no_speech < 0.3:
                    text_parts.append(seg_text)
                elif avg_prob > -0.7 and no_speech < 0.4 and len(seg_text.strip()) >= 3:
                    text_parts.append(seg_text)
                elif avg_prob > -1.0 and no_speech < 0.15 and len(seg_text.strip()) >= 5:
                    text_parts.append(seg_text)
        else:
            # fallback: ç›´æ¥å–å¾—æ–‡å­—
            text_parts = [result.text if hasattr(result, 'text') else str(result)]
        
        text = "".join(text_parts).strip()
        return text

    except Exception as e:
        print(f"ASR éŒ¯èª¤: {e}", file=sys.stderr, flush=True)
        import traceback
        traceback.print_exc()
        return ""

def google_mt(text: str) -> str:
    """ä½¿ç”¨ Deep Translator é€²è¡Œç¿»è­¯ï¼Œä¸¦éæ¿¾é‡è¤‡ã€‚"""
    if not text or not translator:
        return ""
    try:
        translated = translator.translate(text)
        
        # ğŸ¯ éæ¿¾ç¿»è­¯å¾Œçš„é‡è¤‡å…§å®¹
        if translated:
            translated = filter_translated_repetition(translated)
        
        return translated
    except Exception as e:
        print(f"ç¿»è­¯éŒ¯èª¤: {e}", file=sys.stderr, flush=True)
        return ""

def filter_translated_repetition(text: str) -> str:
    """éæ¿¾ç¿»è­¯å¾Œçš„é‡è¤‡å…§å®¹ - åŠ å¼·ç‰ˆ"""
    if not text or len(text) < 4:
        return text
    
    original_text = text
    
    # ğŸ¯ æ–¹æ³• 0: åµæ¸¬ç©ºæ ¼åˆ†éš”çš„å®Œå…¨ç›¸åŒç‰‡æ®µ (å¦‚ï¼šä¸åœ¨ä¹çš„åŸºå¾·å…ˆç”Ÿ ä¸åœ¨ä¹çš„åŸºå¾·å…ˆç”Ÿ)
    if ' ' in text:
        space_parts = [p.strip() for p in text.split(' ') if p.strip()]
        if len(space_parts) >= 2:
            # æª¢æŸ¥é€£çºŒé‡è¤‡
            unique_space = []
            for p in space_parts:
                if not unique_space or p != unique_space[-1]:
                    # ä¹Ÿæª¢æŸ¥ç›¸ä¼¼åº¦
                    is_dup = False
                    for u in unique_space:
                        if p == u or calculate_similarity(p, u) > 0.7:
                            is_dup = True
                            break
                    if not is_dup:
                        unique_space.append(p)
            
            if len(unique_space) < len(space_parts):
                result = ' '.join(unique_space)
                print(f"ğŸ”§ å»é™¤ç©ºæ ¼é‡è¤‡: {original_text[:40]} -> {result[:40]}", file=sys.stderr, flush=True)
                text = result
                original_text = result
    
    # ğŸ¯ æ–¹æ³• 1: åµæ¸¬é€£çºŒé‡è¤‡çš„å­å­—ä¸² (å¦‚ï¼šæˆ‘å¯ä»¥èµ°äº†å—ï¼Ÿæˆ‘å¯ä»¥èµ°äº†å—ï¼Ÿ)
    cleaned = remove_repeated_substrings(text)
    if cleaned != text:
        print(f"ğŸ”§ å»é™¤é‡è¤‡å­å­—ä¸²: {original_text[:40]} -> {cleaned[:40]}", file=sys.stderr, flush=True)
        return cleaned
    
    # ğŸ¯ æ–¹æ³• 2: æŒ‰æ¨™é»åˆ†å‰²ä¸¦å»é‡
    separators = ['ï¼Œ', 'ã€‚', 'ï¼', 'ï¼Ÿ']
    for sep in separators:
        if sep in text and text.count(sep) >= 1:
            parts = [p.strip() for p in text.split(sep) if p.strip()]
            if len(parts) >= 2:
                # æª¢æŸ¥æ˜¯å¦æœ‰é‡è¤‡æˆ–é«˜åº¦ç›¸ä¼¼
                unique = []
                for p in parts:
                    is_dup = False
                    for u in unique:
                        # ğŸ¯ é™ä½ç›¸ä¼¼åº¦é–¾å€¼ï¼Œæ›´ç©æ¥µå»é‡
                        if p == u or calculate_similarity(p, u) > 0.6:
                            is_dup = True
                            break
                    if not is_dup:
                        unique.append(p)
                
                if len(unique) < len(parts):
                    result = sep.join(unique)
                    if sep in ['ã€‚', 'ï¼', 'ï¼Ÿ']:
                        result = result + sep if not result.endswith(sep) else result
                    print(f"ğŸ”§ å»é™¤ç¿»è­¯é‡è¤‡: {original_text[:40]} -> {result[:40]}", file=sys.stderr, flush=True)
                    return result
    
    return text

def remove_repeated_substrings(text: str) -> str:
    """ç§»é™¤é€£çºŒé‡è¤‡çš„å­å­—ä¸² - ä¿ç•™ä¸é‡è¤‡çš„å‰ç¶´"""
    if len(text) < 8:
        return text
    
    # ğŸ¯ æ–¹æ³• 1: æŒ‰å¥å°¾æ¨™é»åˆ†å‰²ï¼Œæ‰¾å®Œæ•´çš„é‡è¤‡å¥å­
    sentence_endings = ['ã€‚', 'ï¼', 'ï¼Ÿ', '!', '?']
    for ending in sentence_endings:
        if ending in text:
            # æŒ‰å¥å°¾åˆ†å‰²
            parts = []
            current = ""
            for char in text:
                current += char
                if char == ending:
                    if current.strip():
                        parts.append(current.strip())
                    current = ""
            if current.strip():
                parts.append(current.strip())
            
            if len(parts) >= 2:
                # ğŸ¯ ä¿ç•™æ‰€æœ‰ä¸é‡è¤‡çš„å¥å­
                unique = []
                seen = set()
                for p in parts:
                    if p not in seen:
                        unique.append(p)
                        seen.add(p)
                
                # åªæœ‰ç•¶ç¢ºå¯¦æœ‰é‡è¤‡è¢«ç§»é™¤æ™‚æ‰è¿”å›
                if len(unique) < len(parts):
                    return ''.join(unique)
    
    # ğŸ¯ æ–¹æ³• 2: åµæ¸¬é€£çºŒé‡è¤‡çš„å­å­—ä¸²æ¨¡å¼
    # å„ªå…ˆå˜—è©¦è¼ƒé•·çš„æ¨¡å¼ (å¾é•·åˆ°çŸ­)
    for pattern_len in range(min(30, len(text) // 2), 4, -1):
        for start in range(len(text) - pattern_len * 2 + 1):
            pattern = text[start:start + pattern_len]
            
            # è·³éç´”æ¨™é»æˆ–ç©ºç™½
            if all(c in 'ï¼Œã€‚ï¼ï¼Ÿ ã€,.!? ' for c in pattern):
                continue
            
            # ğŸ¯ ç¢ºä¿ pattern ä»¥æ¨™é»çµå°¾
            has_ending = any(pattern.endswith(e) for e in ['ã€‚', 'ï¼', 'ï¼Ÿ', 'ï¼Œ', '!', '?', ','])
            if not has_ending:
                continue
            
            # è¨ˆç®—é€£çºŒå‡ºç¾æ¬¡æ•¸
            count = 0
            pos = 0
            first_idx = -1
            while True:
                idx = text.find(pattern, pos)
                if idx == -1:
                    break
                if first_idx == -1:
                    first_idx = idx
                count += 1
                pos = idx + len(pattern)
            
            # å¦‚æœæ¨¡å¼é€£çºŒå‡ºç¾ 2 æ¬¡ä»¥ä¸Š
            if count >= 2 and len(pattern) * count > len(text) * 0.5:
                # ğŸ¯ ä¿ç•™é‡è¤‡å‰çš„å…§å®¹ + ä¸€æ¬¡é‡è¤‡æ¨¡å¼
                prefix = text[:first_idx].strip() if first_idx > 0 else ""
                result = pattern.strip()
                if prefix:
                    return prefix + result
                return result
    
    return text

def filter_text(text: str) -> str:
    """éæ¿¾ç„¡æ•ˆæ–‡å­—ï¼Œå»é™¤é‡è¤‡å¾Œä¿ç•™æœ‰æ•ˆå…§å®¹ç¹¼çºŒè™•ç†ã€‚"""
    if not text:
        return ""
    
    # æ—¥æ–‡å­—ç¬¦éæ¿¾
    pattern = re.compile(r'[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FFF\uFF00-\uFFEF\u0020-\u007E]+')
    cleaned = "".join(pattern.findall(text)).strip()
    
    if not cleaned:
        return ""
    
    # ğŸ¯ å¹»è¦ºéæ¿¾åˆ—è¡¨ (é€™äº›æ˜¯å®Œå…¨ç„¡æ„ç¾©çš„ï¼Œç›´æ¥éæ¿¾)
    unwanted = [
        # === Whisper å¸¸è¦‹å¹»è¦º ===
        "[éŸ³å£°ãªã—]", "ã”è¦–è´ã‚ã‚ŠãŒã¨ã†", "æœ€å¾Œã¾ã§ã”è¦–è´",
        "(æ‹æ‰‹)", "(ç¬‘ã„)", "(ãŸã‚æ¯)", "å­—å¹•",
        "ãƒãƒ£ãƒ³ãƒãƒ«ç™»éŒ²", "é«˜è©•ä¾¡", "MBSãƒ‹ãƒ¥ãƒ¼ã‚¹",
        "æä¾›ã¯", "ã”è¦§ã„ãŸã ã", "ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸ",
        "ãŠç–²ã‚Œæ§˜ã§ã—ãŸ", "ã¾ãŸä¼šã„ã¾ã—ã‚‡ã†", "ãƒã‚¤ãƒã‚¤",
        "æ¬¡å›ã‚‚", "ãƒãƒ£ãƒ³ãƒãƒ«", "ç™»éŒ²", "ãŠé¡˜ã„ã—ã¾ã™",
        "â™ª", "BGM", "éŸ³æ¥½", "ã‚¨ãƒ³ãƒ‡ã‚£ãƒ³ã‚°",
        "ãƒ†ãƒ­ãƒƒãƒ—", "ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³", "ã‚¢ãƒŠã‚¦ãƒ³ã‚¹",
        # === Initial Prompt è¢«èª¤è¼¸å‡ºçš„å…§å®¹ ===
        "è©±ã—è¨€è‘‰", "ã‚«ã‚¸ãƒ¥ã‚¢ãƒ«ãªè¡¨ç¾", "ãƒãƒƒãƒˆã‚¹ãƒ©ãƒ³ã‚°",
        "VTuberé…ä¿¡", "é…ä¿¡è€…ã¨ãƒªã‚¹ãƒŠãƒ¼", "æ—¥æœ¬èªã®",
        # === å…¶ä»–å¸¸è¦‹å¹»è¦ºæ¨¡å¼ ===
        "ç¿»è¨³", "å­—å¹•æä¾›", "è‡ªå‹•ç”Ÿæˆ", "æ©Ÿæ¢°ç¿»è¨³",
        "ç¶šãã¯", "è©³ã—ãã¯", "ãƒªãƒ³ã‚¯ã¯",
        "æ¦‚è¦æ¬„", "èª¬æ˜æ¬„", "ã‚³ãƒ¡ãƒ³ãƒˆæ¬„",
    ]
    
    for phrase in unwanted:
        if phrase in cleaned:
            return ""
    
    # ğŸ¯ å»é™¤é‡è¤‡å­—ç¬¦å¾Œä¿ç•™æœ‰æ•ˆå…§å®¹
    if detect_character_repetition(cleaned):
        deduped = remove_source_repetition(cleaned)
        if deduped and len(deduped) >= 2:
            print(f"ğŸ”„ å»é™¤æºæ–‡é‡è¤‡: {cleaned[:30]}... -> {deduped[:30]}", file=sys.stderr, flush=True)
            cleaned = deduped
        else:
            print(f"âš ï¸ éæ¿¾ç´”é‡è¤‡: {cleaned[:30]}...", file=sys.stderr, flush=True)
            return ""
    
    # ğŸ¯ å»é™¤é‡è¤‡è©çµ„å¾Œä¿ç•™æœ‰æ•ˆå…§å®¹
    if detect_phrase_repetition(cleaned):
        deduped = remove_source_repetition(cleaned)
        if deduped and len(deduped) >= 2:
            print(f"ğŸ”„ å»é™¤æºæ–‡é‡è¤‡: {cleaned[:30]}... -> {deduped[:30]}", file=sys.stderr, flush=True)
            cleaned = deduped
        else:
            print(f"âš ï¸ éæ¿¾ç´”é‡è¤‡: {cleaned[:30]}...", file=sys.stderr, flush=True)
            return ""
    
    return cleaned if len(cleaned) >= 2 else ""

def remove_source_repetition(text: str) -> str:
    """å¾æ—¥æ–‡æºæ–‡ä¸­å»é™¤é‡è¤‡ï¼Œä¿ç•™æœ‰æ„ç¾©çš„å…§å®¹"""
    if not text or len(text) < 4:
        return text
    
    original = text
    
    # ğŸ¯ æ–¹æ³• 1: æŒ‰ç©ºæ ¼åˆ†å‰²å»é‡
    if ' ' in text:
        parts = text.split(' ')
        unique = []
        seen = set()
        for p in parts:
            p = p.strip()
            if p and p not in seen:
                unique.append(p)
                seen.add(p)
        if len(unique) < len(parts):
            text = ' '.join(unique)
    
    # ğŸ¯ æ–¹æ³• 2: å°‹æ‰¾é‡è¤‡æ¨¡å¼ä¸¦åªä¿ç•™ä¸€æ¬¡
    for pattern_len in range(2, min(30, len(text) // 2 + 1)):
        for start in range(min(5, len(text) - pattern_len * 2)):
            pattern = text[start:start + pattern_len]
            
            # è·³éç´”æ¨™é»æˆ–ç©ºç™½
            if all(c in 'ã€ï¼Œã€‚ï¼ï¼Ÿã€€ ãƒ»ãƒ¼' for c in pattern):
                continue
            
            # è¨ˆç®—é€£çºŒå‡ºç¾æ¬¡æ•¸
            count = text.count(pattern)
            
            if count >= 3 and len(pattern) * count > len(text) * 0.4:
                # æ‰¾åˆ°é‡è¤‡æ¨¡å¼ï¼Œä¿ç•™ä¸€æ¬¡ + å‰å¾Œå…§å®¹
                first_idx = text.find(pattern)
                last_idx = text.rfind(pattern)
                
                prefix = text[:first_idx].strip() if first_idx > 0 else ""
                suffix = text[last_idx + len(pattern):].strip() if last_idx + len(pattern) < len(text) else ""
                
                result = prefix + pattern + suffix
                result = result.strip()
                
                if result and len(result) >= 2:
                    return result
    
    # ğŸ¯ æ–¹æ³• 3: å¦‚æœæ•´å€‹æ–‡å­—åªæ˜¯å–®ä¸€æ¨¡å¼é‡è¤‡
    for pattern_len in range(2, min(20, len(text) // 3 + 1)):
        pattern = text[:pattern_len]
        if all(c in 'ã€ï¼Œã€‚ï¼ï¼Ÿã€€ ãƒ»ãƒ¼' for c in pattern):
            continue
        
        # æª¢æŸ¥æ˜¯å¦æ•´å€‹æ–‡å­—éƒ½æ˜¯é€™å€‹æ¨¡å¼çš„é‡è¤‡
        repeated = pattern * (len(text) // len(pattern) + 1)
        if text in repeated or repeated.startswith(text):
            return pattern.strip()
    
    return text

def detect_character_repetition(text: str) -> bool:
    """åµæ¸¬ç•°å¸¸çš„å­—ç¬¦é‡è¤‡ (å¹»è¦ºç‰¹å¾µ) - å„ªåŒ–ç‰ˆ"""
    if len(text) < 6:
        return False
    
    # ğŸ¯ æ’é™¤å¸¸è¦‹çš„åˆæ³•é‡è¤‡
    valid_patterns = ['ww', 'ãƒ¼ãƒ¼', '...', 'ï¼ï¼', 'ï¼Ÿï¼Ÿ', 'ã€œã€œ']
    temp_text = text
    for vp in valid_patterns:
        temp_text = temp_text.replace(vp, '')
    
    if len(temp_text) < 4:
        return False
    
    # ğŸ¯ è¨ˆç®—æ¯å€‹å­—ç¬¦å‡ºç¾çš„æ¯”ä¾‹ï¼ˆæ’é™¤ç©ºæ ¼å’Œæ¨™é»ï¼‰
    content_chars = [c for c in temp_text if c not in ' ã€€ã€ã€‚ï¼ï¼Ÿï¼Œ']
    if len(content_chars) < 4:
        return False
    
    from collections import Counter
    char_counts = Counter(content_chars)
    max_count = max(char_counts.values())
    
    # å–®å­—ç¬¦ä½”æ¯”è¶…é 35%
    if max_count > len(content_chars) * 0.35:
        return True
    
    # ğŸ¯ åµæ¸¬é€£çºŒé‡è¤‡æ¨¡å¼ (å¦‚ ABCABCABC)
    for pattern_len in range(2, min(15, len(text) // 3 + 1)):
        for start in range(min(3, len(text) - pattern_len * 3)):
            pattern = text[start:start + pattern_len]
            # è·³éç´”æ¨™é»
            if all(c in 'ã€ï¼Œã€‚ï¼ï¼Ÿã€€ ãƒ»' for c in pattern):
                continue
            if pattern * 3 in text:
                return True
    
    return False

def detect_phrase_repetition(text: str) -> bool:
    """åµæ¸¬é‡è¤‡çš„è©çµ„ - åŠ å¼·ç‰ˆ"""
    # ğŸ¯ æ–¹æ³• 1: åµæ¸¬é€£çºŒé‡è¤‡çš„å­å­—ä¸²
    for pattern_len in range(2, min(20, len(text) // 2 + 1)):
        for start in range(len(text) - pattern_len * 2 + 1):
            pattern = text[start:start + pattern_len]
            
            # è·³éç´”æ¨™é»
            if all(c in 'ã€ï¼Œã€‚ï¼ï¼Ÿã€€ ' for c in pattern):
                continue
            
            # æª¢æŸ¥é€£çºŒé‡è¤‡
            if pattern * 3 in text:
                return True
    
    # ğŸ¯ æ–¹æ³• 2: æŒ‰æ¨™é»åˆ†å‰²æª¢æŸ¥
    separators = ['ã€', 'ï¼Œ', 'ã€‚', ' ']
    for sep in separators:
        if sep in text:
            parts = [p.strip() for p in text.split(sep) if p.strip() and len(p.strip()) >= 2]
            if len(parts) >= 3:
                # æª¢æŸ¥é€£çºŒç›¸åŒ
                consecutive = 1
                for i in range(1, len(parts)):
                    if parts[i] == parts[i-1]:
                        consecutive += 1
                        if consecutive >= 2:  # ğŸ¯ é™ä½åˆ° 2 æ¬¡å°±è¦–ç‚ºé‡è¤‡
                            return True
                    else:
                        consecutive = 1
                
                # æª¢æŸ¥ç¸½é‡è¤‡ç‡
                from collections import Counter
                counts = Counter(parts)
                for part, count in counts.items():
                    if count >= 2 and count >= len(parts) * 0.4:  # ğŸ¯ é™ä½é–€æª»
                        return True
    
    return False

def remove_duplicate(current: str, previous: str) -> str:
    """ç§»é™¤èˆ‡ä¸Šä¸€æ¬¡è½‰éŒ„é‡è¤‡çš„éƒ¨åˆ†ã€‚"""
    if not previous or not current:
        return current
    if current == previous or current in previous:
        return ""
    
    # ğŸ¯ æª¢æŸ¥æ˜¯å¦èˆ‡æœ€è¿‘çš„ä»»ä½•ä¸€æ¬¡è½‰éŒ„é‡è¤‡
    for old in recent_texts:
        if current == old or current in old:
            return ""
    
    # ğŸ¯ æ”¹é€²é‡ç–Šæª¢æ¸¬
    if previous in current:
        idx = current.find(previous)
        if idx == 0:
            return current[len(previous):].strip()
    
    # ğŸ¯ æ›´æ™ºèƒ½çš„å¾Œç¶´-å‰ç¶´é‡ç–Šæª¢æ¸¬
    max_overlap = min(len(previous), len(current), 20)  # é™åˆ¶æª¢æ¸¬é•·åº¦
    for i in range(max_overlap, 2, -1):  # è‡³å°‘ 3 å€‹å­—ç¬¦æ‰ç®—é‡ç–Š
        if previous[-i:] == current[:i]:
            return current[i:].strip()
    
    return current

def calculate_similarity(s1: str, s2: str) -> float:
    """è¨ˆç®—å…©å€‹å­—ä¸²çš„ç›¸ä¼¼åº¦ (0-1) - ä½¿ç”¨å¤šç¨®ç®—æ³•"""
    if not s1 or not s2:
        return 0.0
    if s1 == s2:
        return 1.0
    
    # ğŸ¯ æ–¹æ³• 1: å­å­—ä¸²æª¢æ¸¬
    if s1 in s2 or s2 in s1:
        shorter = min(len(s1), len(s2))
        longer = max(len(s1), len(s2))
        return shorter / longer
    
    # ğŸ¯ æ–¹æ³• 2: N-gram ç›¸ä¼¼åº¦ (æ›´æº–ç¢º)
    def get_ngrams(s, n=2):
        return set(s[i:i+n] for i in range(len(s)-n+1)) if len(s) >= n else {s}
    
    ngrams1 = get_ngrams(s1, 2)
    ngrams2 = get_ngrams(s2, 2)
    
    if not ngrams1 or not ngrams2:
        # fallback to character set
        set1 = set(s1)
        set2 = set(s2)
        intersection = len(set1 & set2)
        union = len(set1 | set2)
        return intersection / union if union > 0 else 0.0
    
    intersection = len(ngrams1 & ngrams2)
    union = len(ngrams1 | ngrams2)
    
    return intersection / union if union > 0 else 0.0

def is_duplicate_or_overlap(text: str) -> bool:
    """æª¢æŸ¥æ–‡å­—æ˜¯å¦èˆ‡æœ€è¿‘ç™¼å¸ƒçš„å…§å®¹é‡è¤‡æˆ–é«˜åº¦é‡ç–Š"""
    global recent_texts, last_transcription
    
    if not text:
        return True
    
    # æª¢æŸ¥æ˜¯å¦å®Œå…¨é‡è¤‡
    if text == last_transcription:
        return True
    
    # æª¢æŸ¥æ˜¯å¦ç‚ºå­å­—ä¸²
    if text in last_transcription or last_transcription in text:
        # å¦‚æœæ–°æ–‡å­—æ˜¯èˆŠæ–‡å­—çš„å­å­—ä¸²ï¼Œè·³é
        if text in last_transcription:
            return True
        # å¦‚æœèˆŠæ–‡å­—æ˜¯æ–°æ–‡å­—çš„å­å­—ä¸²ï¼Œè¨ˆç®—æ–°å¢éƒ¨åˆ†
        # ä¸è¦–ç‚ºé‡è¤‡ï¼Œç¨å¾Œæœƒè™•ç†
    
    # æª¢æŸ¥èˆ‡æœ€è¿‘æ–‡å­—çš„ç›¸ä¼¼åº¦
    for recent in recent_texts:
        similarity = calculate_similarity(text, recent)
        if similarity > SIMILARITY_THRESHOLD:
            return True
    
    return False

def extract_new_content(current: str, previous: str) -> str:
    """æå–æ–°å…§å®¹ï¼Œç§»é™¤èˆ‡å‰ä¸€æ¬¡é‡ç–Šçš„éƒ¨åˆ†"""
    if not previous or not current:
        return current
    
    if current == previous:
        return ""
    
    # å¦‚æœå‰ä¸€æ¬¡æ˜¯ç•¶å‰çš„å­å­—ä¸²ï¼Œæå–æ–°å¢éƒ¨åˆ†
    if previous in current:
        idx = current.find(previous)
        if idx == 0:
            # å‰ç¶´é‡è¤‡ï¼Œå–å¾Œé¢çš„æ–°å…§å®¹
            return current[len(previous):].strip()
        elif idx + len(previous) == len(current):
            # å¾Œç¶´é‡è¤‡ï¼Œå–å‰é¢çš„æ–°å…§å®¹
            return current[:idx].strip()
    
    # æª¢æŸ¥å‰ç¶´é‡ç–Š
    for i in range(min(len(previous), len(current)), 0, -1):
        if previous[-i:] == current[:i]:
            new_part = current[i:].strip()
            # åªæœ‰ç•¶æ–°éƒ¨åˆ†æœ‰æ„ç¾©æ™‚æ‰è¿”å›
            if len(new_part) >= 2:
                return new_part
            return ""
    
    # æª¢æŸ¥å¾Œç¶´é‡ç–Š
    for i in range(min(len(previous), len(current)), 0, -1):
        if previous[:i] == current[-i:]:
            new_part = current[:-i].strip()
            if len(new_part) >= 2:
                return new_part
            return ""
    
    return current

# ğŸ¯ æ–°å¢ï¼šå¥å°¾åµæ¸¬å‡½æ•¸
def is_sentence_complete(text: str) -> bool:
    """æª¢æŸ¥æ–‡å­—æ˜¯å¦ç‚ºå®Œæ•´å¥å­"""
    if not text:
        return False
    
    # æ—¥æ–‡å¥å°¾æ¨™è¨˜
    sentence_endings = [
        'ã€‚', 'ï¼', 'ï¼Ÿ', 'ã€',  # æ—¥æ–‡æ¨™é»
        'ã­', 'ã‚ˆ', 'ã‚ˆã­', 'ã‚', 'ã‹',  # èªæ°£è©
        'ã§ã™', 'ã¾ã™', 'ãŸ', 'ã ',  # å‹•è©çµå°¾
        'ã„', 'ã„ã‚ˆ', 'ã„ã­',  # å½¢å®¹è©çµå°¾
        '...', 'â€¦',  # çœç•¥è™Ÿ
    ]
    
    text = text.strip()
    for ending in sentence_endings:
        if text.endswith(ending):
            return True
    
    # å¦‚æœæ–‡å­—è¶…é 15 å€‹å­—ç¬¦ï¼Œå¯èƒ½æ˜¯å®Œæ•´å¥å­
    if len(text) >= 15:
        return True
    
    return False

# ğŸ¯ æ–°å¢ï¼šåˆä½µä¸å®Œæ•´çš„å¥å­
def merge_incomplete_sentence(pending: str, new_text: str) -> tuple:
    """åˆä½µä¸å®Œæ•´çš„å¥å­ï¼Œè¿”å› (å®Œæ•´å¥å­, å‰©é¤˜å¾…è™•ç†)"""
    if not pending:
        combined = new_text
    else:
        combined = pending + new_text
    
    if is_sentence_complete(combined):
        return combined, ""
    else:
        return "", combined

# ----------------------------------------------------
# æ ¸å¿ƒè™•ç†å‡½æ•¸
# ----------------------------------------------------

def process_audio_chunk(audio_data_b64: str, r):
    """è™•ç†éŸ³è¨Šå¡Šï¼Œä½¿ç”¨æ»‘å‹•è¦–çª—æ©Ÿåˆ¶"""
    global audio_buffer, overlap_buffer, last_transcription, last_publish_time
    global recent_texts, pending_text, last_full_sentence
    
    # è§£ç¢¼éŸ³è¨Š
    raw_bytes = base64.b64decode(audio_data_b64)
    
    # ğŸ¯ æ¢å¾©é‡ç–Šæ©Ÿåˆ¶ï¼šå°‡é‡ç–Šç·©è¡ + æ–°æ•¸æ“šç´¯ç©
    audio_buffer = overlap_buffer + audio_buffer + raw_bytes
    
    # è¨ˆç®—ç›®æ¨™å¤§å°
    target_size = int(BUFFER_DURATION_S * SAMPLE_RATE * BYTES_PER_SAMPLE)
    overlap_size = int(OVERLAP_DURATION_S * SAMPLE_RATE * BYTES_PER_SAMPLE)
    
    if len(audio_buffer) < target_size:
        return
    
    # å–å‡ºè™•ç†çš„éŸ³è¨Š
    audio_to_process = audio_buffer[:target_size]
    
    # ğŸ¯ ä¿ç•™é‡ç–Šéƒ¨åˆ†ä¾›ä¸‹æ¬¡ä½¿ç”¨
    overlap_buffer = audio_buffer[target_size - overlap_size:target_size]
    audio_buffer = audio_buffer[target_size:]
    
    # è½‰æ›ç‚º numpy array
    audio_array = np.frombuffer(audio_to_process, dtype=np.int16).astype(np.float32) / 32768.0
    
    # ASR è½‰éŒ„
    text = whisper_asr(audio_array)
    text = filter_text(text)
    
    if not text:
        return
    
    # ğŸ¯ æª¢æŸ¥æ˜¯å¦èˆ‡æœ€è¿‘å…§å®¹é‡è¤‡
    if is_duplicate_or_overlap(text):
        return
    
    # ğŸ¯ æå–æ–°å…§å®¹
    text = extract_new_content(text, last_transcription)
    if not text or len(text) < 2:
        return
    
    # ğŸ¯ å¥å­å®Œæ•´æ€§è™•ç†
    complete_sentence, pending_text = merge_incomplete_sentence(pending_text, text)
    
    # å¦‚æœæ²’æœ‰å®Œæ•´å¥å­ï¼Œç­‰å¾…æ›´å¤šè³‡æ–™
    if not complete_sentence:
        # ä½†å¦‚æœå¾…è™•ç†æ–‡å­—å¤ªé•·ï¼Œå¼·åˆ¶ç™¼å¸ƒ
        if len(pending_text) >= 30:
            complete_sentence = pending_text
            pending_text = ""
        else:
            return
    
    # æª¢æŸ¥ç™¼å¸ƒé–“éš”
    current_time = time.time()
    if current_time - last_publish_time < MIN_PUBLISH_INTERVAL:
        # é–“éš”å¤ªçŸ­ï¼Œå°‡å…§å®¹åŠ å…¥å¾…è™•ç†
        pending_text = complete_sentence + pending_text
        return
    
    # æ›´æ–°ç‹€æ…‹
    last_transcription = complete_sentence
    last_full_sentence = complete_sentence
    last_publish_time = current_time
    recent_texts.append(complete_sentence)
    context_history.append(complete_sentence)
    
    # ç¿»è­¯
    translation = executor.submit(google_mt, complete_sentence).result(timeout=5)
    
    # ç™¼å¸ƒçµæœ
    tz = timezone(timedelta(hours=8))
    result = {
        "timestamp": datetime.now(tz).strftime("%H:%M:%S"),
        "source_lang": SOURCE_LANG_CODE,
        "target_lang": TARGET_LANG_CODE,
        "duration_s": f"{BUFFER_DURATION_S:.3f}",
        "transcription": complete_sentence,
        "translation": translation
    }
    
    try:
        r.publish(TRANSLATION_CHANNEL, json.dumps(result, ensure_ascii=False))
    except Exception as e:
        print(f"ç™¼ä½ˆéŒ¯èª¤: {e}", file=sys.stderr, flush=True)

def main():
    """ä¸»å¾ªç’°ã€‚"""
    init_global_resources()

    try:
        r = redis.Redis(host=REDIS_HOST, port=REDIS_PORT, db=0)
        r.ping()
        print(f"âœ… Redis é€£ç·šæˆåŠŸ", file=sys.stderr, flush=True)
    except Exception as e:
        print(f"âŒ Redis é€£ç·šå¤±æ•—: {e}", file=sys.stderr, flush=True)
        sys.exit(1)

    p = r.pubsub()
    p.subscribe(AUDIO_CHANNEL)
    print(f"âœ… å·²è¨‚é–±: {AUDIO_CHANNEL}", file=sys.stderr, flush=True)
    print(f"ğŸ¯ stable-ts æ•´åˆæ¨¡å¼å·²å•Ÿç”¨", file=sys.stderr, flush=True)
    print(f"ğŸ¯ VAD: {USE_VAD}, éœéŸ³æŠ‘åˆ¶: {SUPPRESS_SILENCE}", file=sys.stderr, flush=True)

    for msg in p.listen():
        if msg['type'] == 'message':
            process_audio_chunk(msg['data'].decode('utf-8'), r)

if __name__ == "__main__":
    main()