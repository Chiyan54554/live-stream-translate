import sys
import json
import time
from datetime import datetime, timezone, timedelta
import numpy as np
import redis
import os
import base64
import re
from concurrent.futures import ThreadPoolExecutor
from collections import deque

os.environ["HF_HUB_DISABLE_SYMLINKS_WARNING"] = "1"

# ğŸŒŸ ç¢ºä¿ cuDNN è·¯å¾‘æ­£ç¢ºï¼ˆåœ¨ import torch ä¹‹å‰ï¼‰
try:
    import nvidia.cudnn
    cudnn_lib = os.path.join(nvidia.cudnn.__path__[0], "lib")
    current_ld = os.environ.get("LD_LIBRARY_PATH", "")
    if cudnn_lib not in current_ld:
        os.environ["LD_LIBRARY_PATH"] = f"{cudnn_lib}:{current_ld}"
    print(f"âœ… cuDNN è·¯å¾‘å·²è¨­å®š: {cudnn_lib}", file=sys.stderr, flush=True)
except ImportError:
    print("âš ï¸ nvidia-cudnn æœªå®‰è£", file=sys.stderr, flush=True)

try:
    import torch
    print(f"PyTorch: {torch.__version__}", file=sys.stderr, flush=True)
    print(f"CUDA å¯ç”¨: {torch.cuda.is_available()}", file=sys.stderr, flush=True)
    if torch.cuda.is_available():
        print(f"CUDA ç‰ˆæœ¬: {torch.version.cuda}", file=sys.stderr, flush=True)
        print(f"GPU: {torch.cuda.get_device_name(0)}", file=sys.stderr, flush=True)
    
    from faster_whisper import WhisperModel
    from deep_translator import GoogleTranslator
except ImportError as e:
    print(f"éŒ¯èª¤ï¼š{e}", file=sys.stderr, flush=True)
    sys.exit(1)

# --- é…ç½®åƒæ•¸ ---
SAMPLE_RATE = 16000
BYTES_PER_SAMPLE = 2
SOURCE_LANG_CODE = "ja"
TARGET_LANG_CODE = "zh-TW"

# ğŸš€ å»¶é²å„ªåŒ–ï¼šç¸®çŸ­ç·©è¡å€ (5s -> 3s)ï¼Œé‡ç–Šæ™‚é–“ (1.5s -> 1s)
BUFFER_DURATION_S = 3.0
OVERLAP_DURATION_S = 1.0
MIN_AUDIO_ENERGY = 0.005  # ç•¥å¾®é™ä½é–€æª»ï¼Œé¿å…æ¼æ‰è¼•è²

REDIS_HOST = os.getenv('REDIS_HOST', 'redis')
REDIS_PORT = int(os.getenv('REDIS_PORT', 6379))
AUDIO_CHANNEL = "audio_feed"
TRANSLATION_CHANNEL = "translation_feed"

ASR_MODEL_NAME = os.getenv('ASR_MODEL_NAME', 'large-v3')
MODEL_CACHE_DIR = os.getenv('MODEL_CACHE_DIR', '/root/.cache/huggingface/hub')

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
COMPUTE_TYPE = "float16" if DEVICE == "cuda" else "int8"

asr_model = None
translator = None
audio_buffer = b''
overlap_buffer = b''
last_transcription = ""
last_transcriptions = deque(maxlen=3)  # ğŸ¯ è¨˜éŒ„æœ€è¿‘ 3 æ¬¡è½‰éŒ„ç”¨æ–¼å»é‡
context_history = deque(maxlen=8)      # ğŸ¯ å¢åŠ ä¸Šä¸‹æ–‡é•·åº¦ (5 -> 8)
executor = ThreadPoolExecutor(max_workers=2)

def init_global_resources():
    global asr_model, translator, DEVICE, COMPUTE_TYPE
    
    print(f"="*50, file=sys.stderr, flush=True)
    print(f"ğŸ¯ è¨­å‚™: {DEVICE}, è¨ˆç®—é¡å‹: {COMPUTE_TYPE}", file=sys.stderr, flush=True)
    print(f"ğŸ¯ æ¨¡å‹: {ASR_MODEL_NAME}", file=sys.stderr, flush=True)
    print(f"="*50, file=sys.stderr, flush=True)

    try:
        translator = GoogleTranslator(source=SOURCE_LANG_CODE, target=TARGET_LANG_CODE)
        print("âœ… ç¿»è­¯å¼•æ“å°±ç·’", file=sys.stderr, flush=True)
    except Exception as e:
        print(f"âŒ ç¿»è­¯å¼•æ“å¤±æ•—: {e}", file=sys.stderr, flush=True)
        sys.exit(1)

    def try_load_model(device, compute_type):
        try:
            print(f"ğŸ”„ è¼‰å…¥: {device}/{compute_type}...", file=sys.stderr, flush=True)
            model = WhisperModel(
                ASR_MODEL_NAME,
                device=device,
                compute_type=compute_type,
                download_root=MODEL_CACHE_DIR,
                cpu_threads=os.cpu_count() or 4,
                num_workers=2,
            )
            # é ç†±æ¸¬è©¦
            list(model.transcribe(np.zeros(16000, dtype=np.float32), language="ja"))
            return model
        except Exception as e:
            print(f"âš ï¸ {device}/{compute_type} å¤±æ•—: {e}", file=sys.stderr, flush=True)
            return None

    start = time.time()
    for device, ctype in [("cuda", "float16"), ("cuda", "int8_float16"), ("cpu", "int8")]:
        if device == "cuda" and not torch.cuda.is_available():
            continue
        asr_model = try_load_model(device, ctype)
        if asr_model:
            DEVICE, COMPUTE_TYPE = device, ctype
            break
    
    if not asr_model:
        print("âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—", file=sys.stderr, flush=True)
        sys.exit(1)
    
    status = "ğŸš€ GPU" if DEVICE == "cuda" else "âš ï¸ CPU"
    print(f"âœ… {status} æ¨¡å¼: {DEVICE}/{COMPUTE_TYPE}, {time.time()-start:.1f}s", file=sys.stderr, flush=True)

def check_voice_activity(audio_array: np.ndarray) -> bool:
    """ç°¡å–®çš„èªéŸ³æ´»å‹•åµæ¸¬ (VAD)ã€‚"""
    rms = np.sqrt(np.mean(audio_array ** 2))
    return rms > MIN_AUDIO_ENERGY

def get_context_prompt() -> str:
    """ç”Ÿæˆä¸Šä¸‹æ–‡æç¤º - é‡å°ç›´æ’­å„ªåŒ–"""
    # ğŸ¯ æ›´ç²¾ç¢ºçš„å ´æ™¯æè¿°ï¼Œå¹«åŠ© Whisper ç†è§£èªå¢ƒ
    base_prompt = "ã“ã‚Œã¯æ—¥æœ¬èªã®ãƒ©ã‚¤ãƒ–é…ä¿¡ã§ã™ã€‚é…ä¿¡è€…ãŒãƒªã‚¹ãƒŠãƒ¼ã¨ä¼šè©±ã—ã¦ã„ã¾ã™ã€‚"
    
    if not context_history:
        return base_prompt
    
    # å–æœ€è¿‘ 4 å¥ä½œç‚ºä¸Šä¸‹æ–‡ï¼ˆä¸è¦å¤ªé•·ä»¥å…èª¤å°ï¼‰
    recent = "ã€‚".join(list(context_history)[-4:])
    return f"{base_prompt} {recent}"

def whisper_asr(audio_array: np.ndarray) -> str:
    """ä½¿ç”¨ faster-whisper é€²è¡ŒèªéŸ³è¾¨è­˜ã€‚"""
    if asr_model is None or not check_voice_activity(audio_array):
        return ""

    try:
        segments, info = asr_model.transcribe(
            audio_array,
            language=SOURCE_LANG_CODE,
            
            # ğŸ¯ æº–ç¢ºåº¦å„ªåŒ– (ä¸å¢åŠ å»¶é²)
            beam_size=5,              # ç¶­æŒé€Ÿåº¦
            best_of=5,                # ğŸ¯ å¢åŠ å€™é¸æ•¸é‡ (3 -> 5)ï¼Œæå‡æº–ç¢ºåº¦
            patience=1.8,             # ğŸ¯ ç•¥å¾®å¢åŠ è€å¿ƒå€¼ (1.5 -> 1.8)
            
            temperature=[0.0, 0.15, 0.3],  # ğŸ¯ æ›´ç´°ç·»çš„æº«åº¦å›é€€
            compression_ratio_threshold=2.2,  # ğŸ¯ æ›´åš´æ ¼çš„å£“ç¸®æ¯” (éæ¿¾é‡è¤‡)
            
            condition_on_previous_text=True,  # ä¿æŒä¸Šä¸‹æ–‡
            no_speech_threshold=0.6,   # ğŸ¯ æé«˜éœéŸ³é–€æª» (0.5 -> 0.6)
            log_prob_threshold=-0.7,   # ğŸ¯ æ›´åš´æ ¼çš„ç½®ä¿¡åº¦ (-0.8 -> -0.7)
            
            initial_prompt=get_context_prompt(),
            
            # ğŸ¯ VAD å„ªåŒ–ï¼šå¹³è¡¡éŸ¿æ‡‰èˆ‡æº–ç¢ºåº¦
            vad_filter=True,
            vad_parameters=dict(
                threshold=0.4,            # ğŸ¯ ç¨å¾®æé«˜é–€æª» (æ¸›å°‘å™ªéŸ³)
                min_speech_duration_ms=180,  # ğŸ¯ ç•¥å¾®å¢åŠ æœ€å°èªéŸ³é•·åº¦
                min_silence_duration_ms=350, # ğŸ¯ é©åº¦å¢åŠ éœéŸ³åˆ¤å®š
                speech_pad_ms=220,
            ),
            
            word_timestamps=False,    # ç¶­æŒé—œé–‰ä»¥ä¿æŒé€Ÿåº¦
        )
        
        text_parts = []
        for seg in segments:
            # ğŸ¯ æ›´åš´æ ¼çš„ç½®ä¿¡åº¦éæ¿¾
            if seg.avg_logprob > -0.7 and seg.no_speech_prob < 0.5:
                text_parts.append(seg.text)
            elif seg.avg_logprob > -0.85 and seg.no_speech_prob < 0.3:
                # ğŸ¯ æ¬¡å„ªä½†é«˜ç¢ºå®šæ€§çš„ç‰‡æ®µä¹Ÿæ¥å—
                text_parts.append(seg.text)
        
        result = "".join(text_parts).strip()
        
        # ğŸ¯ æ›´æ–°ä¸Šä¸‹æ–‡ (åªä¿ç•™æœ‰æ„ç¾©çš„å…§å®¹)
        if result and len(result) >= 4:
            context_history.append(result)
        
        return result

    except Exception as e:
        print(f"ASR éŒ¯èª¤: {e}", file=sys.stderr, flush=True)
        return ""

def google_mt(text: str) -> str:
    """ä½¿ç”¨ Deep Translator é€²è¡Œç¿»è­¯ã€‚"""
    if not text or not translator:
        return ""
    try:
        return translator.translate(text)
    except Exception as e:
        print(f"ç¿»è­¯éŒ¯èª¤: {e}", file=sys.stderr, flush=True)
        return ""

def filter_text(text: str) -> str:
    """éæ¿¾ç„¡æ•ˆæ–‡å­—ã€‚"""
    if not text:
        return ""
    
    # æ—¥æ–‡å­—ç¬¦éæ¿¾
    pattern = re.compile(r'[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FFF\uFF00-\uFFEF\u0020-\u007E]+')
    cleaned = "".join(pattern.findall(text)).strip()
    
    # ğŸ¯ æ“´å±•å¹»è¦ºéæ¿¾åˆ—è¡¨ (é‡å°ç›´æ’­å ´æ™¯)
    unwanted = [
        # å¸¸è¦‹å¹»è¦º
        "[éŸ³å£°ãªã—]", "ã”è¦–è´ã‚ã‚ŠãŒã¨ã†", "æœ€å¾Œã¾ã§ã”è¦–è´",
        "(æ‹æ‰‹)", "(ç¬‘ã„)", "(ãŸã‚æ¯)", "å­—å¹•",
        "ãƒãƒ£ãƒ³ãƒãƒ«ç™»éŒ²", "é«˜è©•ä¾¡", "MBSãƒ‹ãƒ¥ãƒ¼ã‚¹",
        "æä¾›ã¯", "ã”è¦§ã„ãŸã ã", "ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸ",
        # ğŸ¯ æ–°å¢ï¼šæ›´å¤šå¹»è¦ºæ¨¡å¼
        "ãŠç–²ã‚Œæ§˜ã§ã—ãŸ", "ã¾ãŸä¼šã„ã¾ã—ã‚‡ã†", "ãƒã‚¤ãƒã‚¤",
        "æ¬¡å›ã‚‚", "ãƒãƒ£ãƒ³ãƒãƒ«", "ç™»éŒ²", "ãŠé¡˜ã„ã—ã¾ã™",
        "â™ª", "BGM", "éŸ³æ¥½", "ã‚¨ãƒ³ãƒ‡ã‚£ãƒ³ã‚°",
        "ãƒ†ãƒ­ãƒƒãƒ—", "ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³", "ã‚¢ãƒŠã‚¦ãƒ³ã‚¹",
    ]
    
    for phrase in unwanted:
        if phrase in cleaned:
            return ""
    
    # æª¢æŸ¥é‡è¤‡å­—ç¬¦ï¼ˆå¹»è¦ºç‰¹å¾µï¼‰
    if len(cleaned) > 4:
        char_count = max(cleaned.count(c) for c in set(cleaned))
        if char_count > len(cleaned) * 0.5:
            return ""
    
    return cleaned if len(cleaned) >= 2 else ""

def remove_duplicate(current: str, previous: str) -> str:
    """ç§»é™¤èˆ‡ä¸Šä¸€æ¬¡è½‰éŒ„é‡è¤‡çš„éƒ¨åˆ†ã€‚"""
    if not previous or not current:
        return current
    if current == previous or current in previous:
        return ""
    
    # ğŸ¯ æª¢æŸ¥æ˜¯å¦èˆ‡æœ€è¿‘çš„ä»»ä½•ä¸€æ¬¡è½‰éŒ„é‡è¤‡
    for old in last_transcriptions:
        if current == old or current in old:
            return ""
    
    # ğŸ¯ æ”¹é€²é‡ç–Šæª¢æ¸¬
    if previous in current:
        idx = current.find(previous)
        if idx == 0:
            return current[len(previous):].strip()
    
    # ğŸ¯ æ›´æ™ºèƒ½çš„å¾Œç¶´-å‰ç¶´é‡ç–Šæª¢æ¸¬
    max_overlap = min(len(previous), len(current), 20)  # é™åˆ¶æª¢æ¸¬é•·åº¦
    for i in range(max_overlap, 2, -1):  # è‡³å°‘ 3 å€‹å­—ç¬¦æ‰ç®—é‡ç–Š
        if previous[-i:] == current[:i]:
            return current[i:].strip()
    
    return current

# ----------------------------------------------------
# æ ¸å¿ƒè™•ç†å‡½æ•¸
# ----------------------------------------------------

def process_audio_chunk(audio_data_b64: str, r):
    """è™•ç†éŸ³è¨Šå¡Šï¼Œä½¿ç”¨æ»‘å‹•è¦–çª—æ©Ÿåˆ¶ã€‚"""
    global audio_buffer, overlap_buffer, last_transcription
    
    # è§£ç¢¼éŸ³è¨Š
    raw_bytes = base64.b64decode(audio_data_b64)
    audio_buffer = overlap_buffer + audio_buffer + raw_bytes
    
    # è¨ˆç®—ç›®æ¨™å¤§å°
    target_size = int(BUFFER_DURATION_S * SAMPLE_RATE * BYTES_PER_SAMPLE)
    overlap_size = int(OVERLAP_DURATION_S * SAMPLE_RATE * BYTES_PER_SAMPLE)
    
    if len(audio_buffer) < target_size:
        return
    
    # å–å‡ºè™•ç†çš„éŸ³è¨Š
    audio_to_process = audio_buffer[:target_size]
    # ğŸŒŸ ä¿ç•™é‡ç–Šéƒ¨åˆ†ä¾›ä¸‹æ¬¡ä½¿ç”¨
    overlap_buffer = audio_buffer[target_size - overlap_size:target_size]
    audio_buffer = audio_buffer[target_size:]
    
    # è½‰æ›ç‚º numpy array
    audio_array = np.frombuffer(audio_to_process, dtype=np.int16).astype(np.float32) / 32768.0
    
    # ASR è½‰éŒ„
    text = whisper_asr(audio_array)
    # éæ¿¾æ–‡å­—
    text = filter_text(text)
    if not text:
        return
    
    # ğŸ¯ å»é™¤é‡è¤‡ (ä½¿ç”¨æ­·å²è¨˜éŒ„)
    text = remove_duplicate(text, last_transcription)
    if not text:
        return
    
    # ğŸ¯ æ›´æ–°æ­·å²è¨˜éŒ„
    last_transcription = text
    last_transcriptions.append(text)
    
    # ğŸŒŸ ä¸¦è¡ŒåŸ·è¡Œç¿»è­¯
    future = executor.submit(google_mt, text)
    translation = future.result(timeout=5)
    
    # æ™‚é–“æˆ³
    tz = timezone(timedelta(hours=8))
    result = {
        "timestamp": datetime.now(tz).strftime("%H:%M:%S"),
        "source_lang": SOURCE_LANG_CODE,
        "target_lang": TARGET_LANG_CODE,
        "duration_s": f"{BUFFER_DURATION_S:.3f}",
        "transcription": text,
        "translation": translation
    }
    
    try:
        r.publish(TRANSLATION_CHANNEL, json.dumps(result, ensure_ascii=False))
    except Exception as e:
        print(f"ç™¼ä½ˆéŒ¯èª¤: {e}", file=sys.stderr, flush=True)

def main():
    """ä¸»å¾ªç’°ã€‚"""
    init_global_resources()

    try:
        r = redis.Redis(host=REDIS_HOST, port=REDIS_PORT, db=0)
        r.ping()
        print(f"âœ… Redis é€£ç·šæˆåŠŸ", file=sys.stderr, flush=True)
    except Exception as e:
        print(f"âŒ Redis é€£ç·šå¤±æ•—: {e}", file=sys.stderr, flush=True)
        sys.exit(1)

    p = r.pubsub()
    p.subscribe(AUDIO_CHANNEL)
    print(f"âœ… å·²è¨‚é–±: {AUDIO_CHANNEL}", file=sys.stderr, flush=True)

    for msg in p.listen():
        if msg['type'] == 'message':
            process_audio_chunk(msg['data'].decode('utf-8'), r)

if __name__ == "__main__":
    main()