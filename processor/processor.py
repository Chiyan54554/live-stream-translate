import sys
import json
import time
import asyncio
from datetime import datetime, timezone, timedelta
import numpy as np
import redis.asyncio as aioredis  # ğŸ¯ ç•°æ­¥ Redis
import os
import base64
import re
import aiohttp  # ğŸ¯ ç•°æ­¥ HTTP
from collections import deque

os.environ["HF_HUB_DISABLE_SYMLINKS_WARNING"] = "1"

# ğŸŒŸ ç¢ºä¿ cuDNN è·¯å¾‘æ­£ç¢ºï¼ˆåœ¨ import torch ä¹‹å‰ï¼‰
try:
    import nvidia.cudnn
    cudnn_lib = os.path.join(nvidia.cudnn.__path__[0], "lib")
    current_ld = os.environ.get("LD_LIBRARY_PATH", "")
    if cudnn_lib not in current_ld:
        os.environ["LD_LIBRARY_PATH"] = f"{cudnn_lib}:{current_ld}"
    print(f"âœ… cuDNN è·¯å¾‘å·²è¨­å®š: {cudnn_lib}", file=sys.stderr, flush=True)
except ImportError:
    print("âš ï¸ nvidia-cudnn æœªå®‰è£", file=sys.stderr, flush=True)

try:
    import torch
    print(f"PyTorch: {torch.__version__}", file=sys.stderr, flush=True)
    print(f"CUDA å¯ç”¨: {torch.cuda.is_available()}", file=sys.stderr, flush=True)
    if torch.cuda.is_available():
        print(f"CUDA ç‰ˆæœ¬: {torch.version.cuda}", file=sys.stderr, flush=True)
        print(f"GPU: {torch.cuda.get_device_name(0)}", file=sys.stderr, flush=True)
    
    # ğŸ¯ ä½¿ç”¨ stable-ts æ•´åˆ faster-whisper
    import stable_whisper
    import requests  # ğŸ¯ ç”¨æ–¼å‘¼å« Ollama LLM API
    
    print(f"âœ… stable-ts ç‰ˆæœ¬: {stable_whisper.__version__}", file=sys.stderr, flush=True)
    
    # ğŸ¯ å˜—è©¦è¼‰å…¥ Transformers pipeline (ç”¨æ–¼ kotoba-whisper-v2.1)
    TRANSFORMERS_AVAILABLE = False
    try:
        from transformers import pipeline as hf_pipeline
        TRANSFORMERS_AVAILABLE = True
        print("âœ… Transformers pipeline å¯ç”¨", file=sys.stderr, flush=True)
    except ImportError:
        print("âš ï¸ Transformers æœªå®‰è£ï¼Œå°‡ä½¿ç”¨ faster-whisper", file=sys.stderr, flush=True)
    
except ImportError as e:
    print(f"éŒ¯èª¤ï¼š{e}", file=sys.stderr, flush=True)
    sys.exit(1)

# --- é…ç½®åƒæ•¸ ---
SAMPLE_RATE = 16000
BYTES_PER_SAMPLE = 2
SOURCE_LANG_CODE = "ja"
TARGET_LANG_CODE = "zh-TW"

# ğŸ¯ æº–ç¢ºç‡å„ªåŒ–ï¼šå¹³è¡¡ç·©è¡èˆ‡å»¶é²
BUFFER_DURATION_S = 2.0       # ğŸ¯ 2 ç§’ç·©è¡ï¼Œç›®æ¨™ 3 ç§’å»¶é²
OVERLAP_DURATION_S = 0.3      # ğŸ¯ æœ€å°é‡ç–Š
MIN_AUDIO_ENERGY = 0.006      # ğŸ¯ é©ä¸­çš„èƒ½é‡é–€æª»

REDIS_HOST = os.getenv('REDIS_HOST', 'redis')
REDIS_PORT = int(os.getenv('REDIS_PORT', 6379))
AUDIO_CHANNEL = "audio_feed"
TRANSLATION_CHANNEL = "translation_feed"

# ğŸ¯ ASR æ¨¡å‹é¸æ“‡
# - large-v3: æ¨™æº– faster-whisper ç©©å®šç‰ˆ
# - kotoba-tech/kotoba-whisper-v2.1: æ—¥æ–‡å„ªåŒ– Transformers ç‰ˆ (å¹»è¦ºæ›´å°‘ï¼Œå¸¶æ¨™é»)
# - kotoba-tech/kotoba-whisper-v2.0-faster: æ—¥æ–‡å„ªåŒ– CTranslate2 ç‰ˆ (RTX 50 ç³»åˆ—å¯èƒ½ä¸ç›¸å®¹)
ASR_MODEL_NAME = os.getenv('ASR_MODEL_NAME', 'kotoba-tech/kotoba-whisper-v2.1')
MODEL_CACHE_DIR = os.getenv('MODEL_CACHE_DIR', '/root/.cache/huggingface/hub')

# ğŸ¯ è‡ªå‹•åˆ¤æ–·æ¨¡å‹é¡å‹
USE_KOTOBA_PIPELINE = 'kotoba-whisper-v2.1' in ASR_MODEL_NAME
USING_KOTOBA_PIPELINE = False  # ğŸ¯ å¯¦éš›ä½¿ç”¨çš„æ¨¡å¼ (ç”± init_global_resources è¨­å®š)

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
COMPUTE_TYPE = "float16" if DEVICE == "cuda" else "int8"

# ğŸ¯ LLM ç¿»è­¯é…ç½® (Ollama + Qwen2)
LLM_HOST = os.getenv('LLM_HOST', 'ollama')  # Docker æœå‹™åç¨±
LLM_PORT = os.getenv('LLM_PORT', '11434')
LLM_MODEL = os.getenv('LLM_MODEL', 'qwen2.5:7b-instruct')  # Qwen2.5 7B Instruct
LLM_API_URL = f"http://{LLM_HOST}:{LLM_PORT}/api/generate"
LLM_TIMEOUT = 8  # ğŸ¯ ç¿»è­¯è¶…æ™‚ç§’æ•¸ï¼ˆç¸®çŸ­åŠ å¿«éŸ¿æ‡‰ï¼‰

# ğŸ¯ stable-ts èˆ‡ VAD ç›¸é—œè¨­å®š
USE_STABLE_TS = True                    # å•Ÿç”¨ stable-ts
USE_VAD = True                          # å•Ÿç”¨ Silero VAD
VAD_THRESHOLD = 0.45                    # ğŸ¯ VAD èªéŸ³åµæ¸¬é–¾å€¼ï¼ˆæé«˜ä»¥æ¸›å°‘éåº¦åˆ‡å‰²ï¼‰
SUPPRESS_SILENCE = True                 # éœéŸ³æŠ‘åˆ¶
HALLUCINATION_SILENCE_TH = 1.5          # ğŸ¯ å¹»è¦ºéœéŸ³é–¾å€¼ï¼ˆç§’ï¼‰- é™ä½æ›´ç©æ¥µéæ¿¾
AVG_PROB_THRESHOLD = -0.7               # ğŸ¯ å¹³å‡ç½®ä¿¡åº¦é–¾å€¼ - ç¨å¾®æé«˜éæ¿¾ä½å“è³ª
MAX_INSTANT_WORDS = 0.30                # ğŸ¯ é™ä½é–¾å€¼ï¼Œæ›´ç©æ¥µéæ¿¾å¹»è¦º
ONLY_VOICE_FREQ = False                 # æ˜¯å¦åªä¿ç•™èªéŸ³é »ç‡ (200-5000 Hz)

asr_model = None
audio_buffer = b''
overlap_buffer = b''         # ğŸ¯ æ¢å¾©é‡ç–Šç·©è¡å€
last_transcription = ""
last_full_sentence = ""       # ğŸ¯ æ–°å¢ï¼šè¨˜éŒ„ä¸Šä¸€å€‹å®Œæ•´å¥å­
pending_text = ""             # ğŸ¯ æ–°å¢ï¼šå¾…è™•ç†çš„ä¸å®Œæ•´æ–‡å­—
last_publish_time = 0
recent_texts = deque(maxlen=10)
context_history = deque(maxlen=5)  # ğŸ¯ ç¸®çŸ­ä¸Šä¸‹æ–‡æ¸›å°‘é–‹éŠ·

# ğŸ¯ ç•°æ­¥ HTTP session (å…¨åŸŸ)
aio_session: aiohttp.ClientSession = None

MIN_PUBLISH_INTERVAL = 0.5    # ğŸ¯ ç¸®çŸ­æœ€å°é–“éš”åŠ å¿«è¼¸å‡º
SIMILARITY_THRESHOLD = 0.75   # ğŸ¯ æé«˜ç›¸ä¼¼åº¦é–¾å€¼æ¸›å°‘é‡è¤‡è™•ç†

# ğŸ¯ OpenCC ç°¡ç¹è½‰æ›å™¨ (s2twp = ç°¡é«”â†’ç¹é«”å°ç£ï¼ŒåŒ…å«è©å½™è½‰æ›)
try:
    import opencc
    OPENCC_CONVERTER = opencc.OpenCC('s2twp')  # ç°¡é«”â†’ç¹é«”(å°ç£æ­£é«”+å°ç£æ…£ç”¨è©)
    print(f"âœ… OpenCC ç°¡ç¹è½‰æ›å™¨å·²è¼‰å…¥ (s2twp)", file=sys.stderr, flush=True)
except ImportError:
    OPENCC_CONVERTER = None
    print(f"âš ï¸ OpenCC æœªå®‰è£ï¼Œå°‡ä½¿ç”¨å‚™ç”¨ txt å­—å…¸", file=sys.stderr, flush=True)

# ğŸ¯ è¼‰å…¥å‚™ç”¨ç°¡ç¹è½‰æ›è¡¨ (ç•¶ OpenCC ä¸å¯ç”¨æ™‚)
def load_simplified_to_traditional() -> dict:
    """å¾å¤–éƒ¨ txt æª”æ¡ˆè¼‰å…¥ç°¡ç¹è½‰æ›è¡¨ï¼ˆå‚™ç”¨ï¼‰"""
    mapping = {}
    txt_path = os.path.join(os.path.dirname(__file__), 'simplified_to_traditional.txt')
    
    try:
        with open(txt_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                # è·³éç©ºè¡Œå’Œè¨»è§£
                if not line or line.startswith('#'):
                    continue
                # è§£æ ç°¡é«”=ç¹é«” æ ¼å¼
                if '=' in line:
                    parts = line.split('=', 1)
                    if len(parts) == 2:
                        simp, trad = parts[0].strip(), parts[1].strip()
                        if simp and trad:
                            mapping[simp] = trad
        if not OPENCC_CONVERTER:
            print(f"âœ… è¼‰å…¥å‚™ç”¨ç°¡ç¹è½‰æ›è¡¨: {len(mapping)} çµ„", file=sys.stderr, flush=True)
    except FileNotFoundError:
        if not OPENCC_CONVERTER:
            print(f"âš ï¸ æ‰¾ä¸åˆ°ç°¡ç¹è½‰æ›è¡¨: {txt_path}", file=sys.stderr, flush=True)
    except Exception as e:
        if not OPENCC_CONVERTER:
            print(f"âš ï¸ è¼‰å…¥ç°¡ç¹è½‰æ›è¡¨å¤±æ•—: {e}", file=sys.stderr, flush=True)
    
    return mapping

def load_china_to_taiwan() -> dict:
    """å¾å¤–éƒ¨ txt æª”æ¡ˆè¼‰å…¥ä¸­åœ‹ç”¨èªè½‰å°ç£ç”¨èªè¡¨"""
    mapping = {}
    txt_path = os.path.join(os.path.dirname(__file__), 'china_to_taiwan.txt')
    
    try:
        with open(txt_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if not line or line.startswith('#'):
                    continue
                if '=' in line:
                    parts = line.split('=', 1)
                    if len(parts) == 2:
                        china, taiwan = parts[0].strip(), parts[1].strip()
                        if china and taiwan:
                            mapping[china] = taiwan
        print(f"âœ… è¼‰å…¥ä¸­å°ç”¨èªè¡¨: {len(mapping)} çµ„", file=sys.stderr, flush=True)
    except FileNotFoundError:
        print(f"âš ï¸ æ‰¾ä¸åˆ°ä¸­å°ç”¨èªè¡¨: {txt_path}", file=sys.stderr, flush=True)
    except Exception as e:
        print(f"âš ï¸ è¼‰å…¥ä¸­å°ç”¨èªè¡¨å¤±æ•—: {e}", file=sys.stderr, flush=True)
    
    return mapping

# å…¨åŸŸè½‰æ›è¡¨
SIMPLIFIED_TO_TRADITIONAL = load_simplified_to_traditional()
CHINA_TO_TAIWAN = load_china_to_taiwan()

def init_global_resources():
    global asr_model, DEVICE, COMPUTE_TYPE, USING_KOTOBA_PIPELINE
    
    print(f"="*50, file=sys.stderr, flush=True)
    print(f"ğŸ¯ è¨­å‚™: {DEVICE}, è¨ˆç®—é¡å‹: {COMPUTE_TYPE}", file=sys.stderr, flush=True)
    print(f"ğŸ¯ ASR æ¨¡å‹: {ASR_MODEL_NAME}", file=sys.stderr, flush=True)
    print(f"ğŸ¯ ä½¿ç”¨ Kotoba Pipeline: {USE_KOTOBA_PIPELINE}", file=sys.stderr, flush=True)
    print(f"ğŸ¯ LLM ç¿»è­¯: {LLM_MODEL} @ {LLM_HOST}:{LLM_PORT}", file=sys.stderr, flush=True)
    print(f"ğŸ¯ stable-ts: {USE_STABLE_TS}, VAD: {USE_VAD}", file=sys.stderr, flush=True)
    print(f"="*50, file=sys.stderr, flush=True)

    # ğŸ¯ æ¸¬è©¦ Ollama LLM é€£ç·š
    try:
        test_resp = requests.post(
            LLM_API_URL,
            json={"model": LLM_MODEL, "prompt": "test", "stream": False},
            timeout=30
        )
        if test_resp.status_code == 200:
            print(f"âœ… LLM ç¿»è­¯å¼•æ“å°±ç·’ ({LLM_MODEL})", file=sys.stderr, flush=True)
        else:
            print(f"âš ï¸ LLM å›æ‡‰ç•°å¸¸: {test_resp.status_code}", file=sys.stderr, flush=True)
    except requests.exceptions.ConnectionError:
        print(f"âš ï¸ ç„¡æ³•é€£æ¥ Ollama ({LLM_API_URL})ï¼Œç¿»è­¯åŠŸèƒ½å¯èƒ½ç„¡æ³•ä½¿ç”¨", file=sys.stderr, flush=True)
        print(f"   è«‹ç¢ºä¿ Ollama æ­£åœ¨é‹è¡Œ: ollama serve", file=sys.stderr, flush=True)
    except Exception as e:
        print(f"âš ï¸ LLM æ¸¬è©¦å¤±æ•—: {e}", file=sys.stderr, flush=True)

    start = time.time()
    
    # ğŸ¯ æ ¹æ“šæ¨¡å‹é¡å‹é¸æ“‡è¼‰å…¥æ–¹å¼
    if USE_KOTOBA_PIPELINE:
        if not TRANSFORMERS_AVAILABLE:
            print(f"âš ï¸ ä½¿ç”¨ Kotoba v2.1 éœ€è¦ Transformersï¼Œä½†æœªå®‰è£", file=sys.stderr, flush=True)
            print(f"ğŸ”„ è‡ªå‹•åˆ‡æ›åˆ° large-v3 (faster-whisper)...", file=sys.stderr, flush=True)
            # ç›´æ¥è·³åˆ° faster-whisper è¼‰å…¥
        else:
            # ===== Kotoba-Whisper v2.1 (Transformers Pipeline) =====
            try:
                print(f"ğŸ”„ ä½¿ç”¨ Transformers Pipeline è¼‰å…¥ Kotoba-Whisper...", file=sys.stderr, flush=True)
                
                torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32
                device = "cuda:0" if torch.cuda.is_available() else "cpu"
                model_kwargs = {"attn_implementation": "sdpa"} if torch.cuda.is_available() else {}
                
                asr_model = hf_pipeline(
                    "automatic-speech-recognition",
                    model=ASR_MODEL_NAME,
                    torch_dtype=torch_dtype,
                    device=device,
                    model_kwargs=model_kwargs,
                    batch_size=1,  # ç›´æ’­ç”¨å–®æ‰¹æ¬¡
                    trust_remote_code=True,
                )
                
                DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
                COMPUTE_TYPE = "float16" if DEVICE == "cuda" else "float32"
                USING_KOTOBA_PIPELINE = True  # ğŸ¯ æ¨™è¨˜å¯¦éš›ä½¿ç”¨ Kotoba Pipeline
                
                print(f"âœ… Kotoba-Whisper v2.1 å·²å°±ç·’ (Transformers)", file=sys.stderr, flush=True)
                print(f"âœ… ğŸš€ GPU æ¨¡å¼: {DEVICE}/{COMPUTE_TYPE}, {time.time()-start:.1f}s", file=sys.stderr, flush=True)
                return
                
            except Exception as e:
                print(f"âš ï¸ Kotoba Pipeline è¼‰å…¥å¤±æ•—: {e}", file=sys.stderr, flush=True)
                print(f"ğŸ”„ é€€å›ä½¿ç”¨ large-v3 (faster-whisper)...", file=sys.stderr, flush=True)
                import traceback
                traceback.print_exc()
    
    # ===== æ¨™æº– faster-whisper + stable-ts =====
    # å¦‚æœæ˜¯ Kotoba v2.1 ä½† Transformers å¤±æ•—ï¼Œæ”¹ç”¨ large-v3
    USING_KOTOBA_PIPELINE = False  # ğŸ¯ æ¨™è¨˜ä½¿ç”¨ faster-whisper
    fallback_model = "large-v3" if USE_KOTOBA_PIPELINE else ASR_MODEL_NAME
    
    def try_load_model(device, compute_type):
        try:
            print(f"ğŸ”„ ä½¿ç”¨ stable-ts è¼‰å…¥ {fallback_model}: {device}/{compute_type}...", file=sys.stderr, flush=True)
            
            # ğŸ¯ ä½¿ç”¨ stable-ts çš„ load_faster_whisper
            model = stable_whisper.load_faster_whisper(
                fallback_model,  # ğŸ¯ ä½¿ç”¨ fallback æ¨¡å‹åç¨±
                device=device,
                compute_type=compute_type,
                download_root=MODEL_CACHE_DIR,
                cpu_threads=os.cpu_count() or 4,
                num_workers=2,
            )
            
            # é ç†±æ¸¬è©¦
            warmup_audio = np.zeros(16000, dtype=np.float32)
            _ = model.transcribe(
                warmup_audio,
                language="ja",
                vad=False,  # é ç†±æ™‚é—œé–‰ VAD åŠ é€Ÿ
                suppress_silence=False,
            )
            
            return model
        except Exception as e:
            print(f"âš ï¸ {device}/{compute_type} å¤±æ•—: {e}", file=sys.stderr, flush=True)
            import traceback
            traceback.print_exc()
            return None

    for device, ctype in [("cuda", "float16"), ("cuda", "int8_float16"), ("cpu", "int8")]:
        if device == "cuda" and not torch.cuda.is_available():
            continue
        asr_model = try_load_model(device, ctype)
        if asr_model:
            DEVICE, COMPUTE_TYPE = device, ctype
            break
    
    if not asr_model:
        print("âŒ æ¨¡å‹è¼‰å…¥å¤±æ•—", file=sys.stderr, flush=True)
        sys.exit(1)
    
    status = "ğŸš€ GPU" if DEVICE == "cuda" else "âš ï¸ CPU"
    print(f"âœ… {status} æ¨¡å¼ ({fallback_model}): {DEVICE}/{COMPUTE_TYPE}, {time.time()-start:.1f}s", file=sys.stderr, flush=True)
    print(f"âœ… stable-ts æ¨¡å‹å·²å°±ç·’", file=sys.stderr, flush=True)

def check_voice_activity(audio_array: np.ndarray) -> bool:
    """ç°¡å–®çš„èªéŸ³æ´»å‹•åµæ¸¬ (VAD)ã€‚"""
    rms = np.sqrt(np.mean(audio_array ** 2))
    return rms > MIN_AUDIO_ENERGY

def get_context_prompt() -> str:
    """ç”Ÿæˆä¸Šä¸‹æ–‡æç¤º - é‡å°ç›´æ’­å„ªåŒ–"""
    # ğŸ¯ ç§»é™¤ initial_prompt - å®ƒå¯èƒ½è¢« Whisper ç•¶æˆè½‰éŒ„è¼¸å‡º
    # è¿”å›ç©ºå­—ä¸²ä»¥é¿å…å¹»è¦º
    return ""

def whisper_asr(audio_array: np.ndarray) -> str:
    """ä½¿ç”¨ ASR é€²è¡ŒèªéŸ³è¾¨è­˜ - æ”¯æ´ Kotoba Pipeline å’Œ faster-whisper"""
    if asr_model is None or not check_voice_activity(audio_array):
        return ""

    try:
        # ğŸ¯ Kotoba-Whisper v2.1 (Transformers Pipeline)
        if USING_KOTOBA_PIPELINE:
            # Transformers pipeline éœ€è¦çš„è¼¸å…¥æ ¼å¼
            audio_input = {
                "raw": audio_array,
                "sampling_rate": SAMPLE_RATE
            }
            
            result = asr_model(
                audio_input,
                chunk_length_s=10,            # ğŸ¯ ç¸®çŸ­ chunk åŠ å¿«è™•ç†
                return_timestamps=True,
                generate_kwargs={"language": "ja", "task": "transcribe"},
                ignore_warning=True,  # ğŸ¯ éš±è— chunk_length_s å¯¦é©—æ€§è­¦å‘Š
            )
            
            text = result.get("text", "").strip() if isinstance(result, dict) else str(result).strip()
            return text
        
        # ğŸ¯ æ¨™æº– faster-whisper + stable-ts
        # ä½¿ç”¨ stable-ts çš„ transcribe æ–¹æ³•
        # é€™æœƒè‡ªå‹•æ•´åˆ VADã€éœéŸ³æŠ‘åˆ¶ã€é‡è¤‡ç§»é™¤ç­‰åŠŸèƒ½
        result = asr_model.transcribe(
            audio_array,
            language=SOURCE_LANG_CODE,
            
            # === åŸºæœ¬ Whisper åƒæ•¸ ===
            beam_size=5,
            best_of=5,
            patience=1.2,
            temperature=[0.0, 0.2],
            compression_ratio_threshold=2.0,
            condition_on_previous_text=False,  # ğŸ¯ é—œé–‰é¿å…éŒ¯èª¤ç´¯ç©
            no_speech_threshold=0.5,
            log_prob_threshold=AVG_PROB_THRESHOLD,
            initial_prompt=get_context_prompt(),
            word_timestamps=True,  # ğŸ¯ å•Ÿç”¨è©ç´šæ™‚é–“æˆ³ä»¥æ”¯æ´å»é‡è¤‡
            
            # === stable-ts VAD èˆ‡éœéŸ³æŠ‘åˆ¶ ===
            vad=USE_VAD,                      # ğŸ¯ ä½¿ç”¨ Silero VAD
            vad_threshold=VAD_THRESHOLD,      # ğŸ¯ VAD é–¾å€¼
            suppress_silence=SUPPRESS_SILENCE, # ğŸ¯ éœéŸ³æŠ‘åˆ¶
            suppress_word_ts=True,            # ğŸ¯ æŠ‘åˆ¶éœéŸ³æ™‚çš„æ™‚é–“æˆ³
            
            # === é¡å¤–çš„ stable-ts åƒæ•¸ ===
            min_word_dur=0.1,                 # æœ€çŸ­è©æŒçºŒæ™‚é–“
            nonspeech_error=0.3,              # éèªéŸ³èª¤å·®å®¹å¿åº¦
            only_voice_freq=ONLY_VOICE_FREQ,  # åªä¿ç•™èªéŸ³é »ç‡ç¯„åœ
            
            regroup=True,  # ğŸ¯ è‡ªå‹•é‡æ–°åˆ†çµ„ç‰‡æ®µ
        )
        
        # ğŸ¯ stable-ts çš„æ ¸å¿ƒåŠŸèƒ½ï¼šç§»é™¤é‡è¤‡
        if hasattr(result, 'remove_repetition'):
            result.remove_repetition(max_words=1, verbose=False)
        
        # ğŸ¯ éæ¿¾ä½ç½®ä¿¡åº¦ç‰‡æ®µ
        text_parts = []
        if hasattr(result, 'segments'):
            for seg in result.segments:
                # å–å¾—ç‰‡æ®µå±¬æ€§
                seg_text = seg.text if hasattr(seg, 'text') else str(seg)
                avg_prob = getattr(seg, 'avg_logprob', -0.5)
                no_speech = getattr(seg, 'no_speech_prob', 0.5)
                
                # ğŸ¯ å¹»è¦ºåµæ¸¬ï¼šæª¢æŸ¥æ˜¯å¦æœ‰éå¤šç¬æ™‚è©
                if hasattr(seg, 'words') and seg.words:
                    instant_words = sum(1 for w in seg.words if hasattr(w, 'duration') and w.duration < 0.05)
                    instant_ratio = instant_words / len(seg.words) if seg.words else 0
                    if instant_ratio > MAX_INSTANT_WORDS:
                        print(f"âš ï¸ è·³éç¬æ™‚è©éå¤šç‰‡æ®µ: {seg_text[:30]}...", file=sys.stderr, flush=True)
                        continue
                
                # ğŸ¯ é¡å¤–å¹»è¦ºåµæ¸¬ï¼šæª¢æŸ¥å–®è©é‡è¤‡
                if hasattr(seg, 'words') and seg.words and len(seg.words) >= 4:
                    word_texts = [w.word.strip() for w in seg.words if hasattr(w, 'word')]
                    if word_texts:
                        from collections import Counter
                        word_counts = Counter(word_texts)
                        max_word_count = max(word_counts.values())
                        # å¦‚æœæŸå€‹è©å‡ºç¾è¶…é 40% çš„æ¬¡æ•¸ï¼Œè¦–ç‚ºå¹»è¦º
                        if max_word_count > len(word_texts) * 0.4:
                            print(f"âš ï¸ è·³éå–®è©é‡è¤‡ç‰‡æ®µ: {seg_text[:30]}...", file=sys.stderr, flush=True)
                            continue
                
                # ğŸ¯ åˆ†ç´šç½®ä¿¡åº¦éæ¿¾
                if avg_prob > -0.4 and no_speech < 0.3:
                    text_parts.append(seg_text)
                elif avg_prob > -0.7 and no_speech < 0.4 and len(seg_text.strip()) >= 3:
                    text_parts.append(seg_text)
                elif avg_prob > -1.0 and no_speech < 0.15 and len(seg_text.strip()) >= 5:
                    text_parts.append(seg_text)
        else:
            # fallback: ç›´æ¥å–å¾—æ–‡å­—
            text_parts = [result.text if hasattr(result, 'text') else str(result)]
        
        text = "".join(text_parts).strip()
        return text

    except Exception as e:
        print(f"ASR éŒ¯èª¤: {e}", file=sys.stderr, flush=True)
        import traceback
        traceback.print_exc()
        return ""

async def llm_translate(text: str) -> str:
    """ğŸ¯ ç•°æ­¥ç‰ˆï¼šä½¿ç”¨ Ollama Qwen2 LLM é€²è¡Œæ—¥æ–‡åˆ°ç¹é«”ä¸­æ–‡ç¿»è­¯"""
    global aio_session
    
    if not text:
        return ""
    
    # ğŸ¯ å„ªåŒ–çš„ç¿»è­¯ prompt - ä½¿ç”¨ ChatML æ ¼å¼
    prompt = f"""<|im_start|>system
ä½ æ˜¯å°ˆæ¥­çš„æ—¥æ–‡å³æ™‚ç›´æ’­ç¿»è­¯å“¡ã€‚å°‡æ—¥æ–‡éŠæˆ²ç›´æ’­å°è©±ç¿»è­¯æˆè‡ªç„¶æµæš¢çš„ç¹é«”ä¸­æ–‡ã€‚

é‡è¦è¦å‰‡ï¼š
- åªè¼¸å‡ºç¿»è­¯çµæœï¼Œä¸è¦è§£é‡‹æˆ–åŠ è¨»
- ä½¿ç”¨ç¹é«”ä¸­æ–‡å’Œå°ç£æ…£ç”¨èª
- ä¿æŒå£èªåŒ–ã€è‡ªç„¶çš„å°è©±èªæ°£
- å¦‚æœè¼¸å…¥æ˜¯ä¸å®Œæ•´ç‰‡æ®µæˆ–å–®å­—ï¼Œç¿»è­¯å…¶æœ€å¯èƒ½çš„æ„æ€
- å¦‚æœè¼¸å…¥ç„¡æ³•è¾¨è­˜æˆ–æ²’æœ‰æ„ç¾©ï¼Œå›è¦†ç©ºç™½
- äººåä¿ç•™æ—¥æ–‡ç™¼éŸ³çš„éŸ³è­¯ï¼ˆå¦‚ï¼šã‚†ã†ãâ†’å„ªå¸Œï¼‰
- éŠæˆ²è¡“èªä½¿ç”¨å°ç£ç©å®¶å¸¸ç”¨è­¯æ³•
- ä¸è¦é‡è¤‡ç¿»è­¯åŒæ¨£çš„å…§å®¹
- ä¸è¦è‡ªè¡Œæ·»åŠ åŸæ–‡æ²’æœ‰çš„å…§å®¹
<|im_end|>
<|im_start|>user
{text}
<|im_end|>
<|im_start|>assistant
"""
    
    try:
        async with aio_session.post(
            LLM_API_URL,
            json={
                "model": LLM_MODEL,
                "prompt": prompt,
                "stream": False,
                "raw": True,
                "options": {
                    "temperature": 0.1,       # ğŸ¯ é™ä½æº«åº¦åŠ å¿«ç”Ÿæˆ
                    "top_p": 0.85,            # ğŸ¯ ç¨å¾®æ”¶ç·Š
                    "num_predict": 150,       # ğŸ¯ ç¸®çŸ­æœ€å¤§è¼¸å‡º
                    "stop": ["<|im_end|>", "<|im_start|>", "\n\n", "æ—¥æ–‡åŸæ–‡"]
                }
            },
            timeout=aiohttp.ClientTimeout(total=LLM_TIMEOUT)
        ) as response:
            if response.status == 200:
                result = await response.json()
                translated = result.get('response', '').strip()
                
                # ğŸ¯ æ¸…ç† LLM è¼¸å‡º
                translated = clean_llm_output(translated)
                
                # ğŸ¯ éæ¿¾ç¿»è­¯å¾Œçš„é‡è¤‡å…§å®¹
                if translated:
                    translated = filter_translated_repetition(translated)
                
                return translated
            else:
                print(f"LLM ç¿»è­¯å¤±æ•—: HTTP {response.status}", file=sys.stderr, flush=True)
                return ""
                
    except asyncio.TimeoutError:
        print(f"LLM ç¿»è­¯è¶…æ™‚ ({LLM_TIMEOUT}s)", file=sys.stderr, flush=True)
        return ""
    except aiohttp.ClientError as e:
        print(f"ç„¡æ³•é€£æ¥ LLM æœå‹™: {e}", file=sys.stderr, flush=True)
        return ""
    except Exception as e:
        print(f"LLM ç¿»è­¯éŒ¯èª¤: {e}", file=sys.stderr, flush=True)
        return ""

def clean_llm_output(text: str) -> str:
    """æ¸…ç† LLM è¼¸å‡ºçš„å„ç¨®å•é¡Œ"""
    if not text:
        return ""
    
    # ğŸ¯ åµæ¸¬ä¸¦éæ¿¾ç•°å¸¸è¼¸å‡º
    # 1. ç¾…é¦¬æ‹¼éŸ³ï¼ˆæ—¥æ–‡æœªç¿»è­¯ï¼‰- é€£çºŒçš„å°å¯«å­—æ¯+ç©ºæ ¼
    romaji_pattern = re.compile(r'^[a-z\s\-\']+$', re.IGNORECASE)
    if romaji_pattern.match(text.strip()) and len(text) > 10:
        print(f"âš ï¸ éæ¿¾ç¾…é¦¬æ‹¼éŸ³: {text[:40]}", file=sys.stderr, flush=True)
        return ""
    
    # 2. æ··åˆèªè¨€åµæ¸¬ï¼ˆä¿„æ–‡ã€æ—¥æ–‡å‡ååœ¨ä¸­æ–‡å¥å­ä¸­ï¼‰
    # ä¿„æ–‡å­—æ¯
    if re.search(r'[Ğ°-ÑĞ-Ğ¯Ñ‘Ğ]', text):
        text = re.sub(r'[Ğ°-ÑĞ-Ğ¯Ñ‘Ğ]+', '', text)
        print(f"âš ï¸ ç§»é™¤ä¿„æ–‡å­—ç¬¦", file=sys.stderr, flush=True)
    
    # 3. å¦‚æœå¥å­å¤§éƒ¨åˆ†æ˜¯æ—¥æ–‡å‡åï¼ˆæœªç¿»è­¯ï¼‰ï¼Œç›´æ¥éæ¿¾
    hiragana_katakana = len(re.findall(r'[\u3040-\u309F\u30A0-\u30FF]', text))
    chinese_chars = len(re.findall(r'[\u4E00-\u9FFF]', text))
    if hiragana_katakana > chinese_chars and hiragana_katakana > 5:
        print(f"âš ï¸ éæ¿¾æœªç¿»è­¯æ—¥æ–‡: {text[:40]}", file=sys.stderr, flush=True)
        return ""
    
    # 4. éæ¿¾ç•°å¸¸çš„è‹±æ–‡/ç¬¦è™Ÿæ··åˆï¼ˆå¦‚ apol_gad, spleen neneskoï¼‰
    if re.match(r'^[a-zA-Z_\s]+$', text.strip()) and len(text) > 5:
        print(f"âš ï¸ éæ¿¾ç´”è‹±æ–‡: {text[:40]}", file=sys.stderr, flush=True)
        return ""
    
    # ç§»é™¤å¸¸è¦‹å‰ç¶´
    prefixes = ['ç¿»è­¯ï¼š', 'ç¿»è­¯:', 'ä¸­æ–‡ï¼š', 'ä¸­æ–‡:', 'ç­”ï¼š', 'ç­”:', 
                'ç¹é«”ä¸­æ–‡ï¼š', 'ç¹é«”ä¸­æ–‡:', 'è­¯æ–‡ï¼š', 'è­¯æ–‡:', 'å›ç­”ï¼š', 'å›ç­”:']
    for prefix in prefixes:
        if text.startswith(prefix):
            text = text[len(prefix):].strip()
    
    # ç§»é™¤å¼•è™ŸåŒ…è£¹
    if len(text) >= 2:
        if (text[0] == '"' and text[-1] == '"') or \
           (text[0] == 'ã€Œ' and text[-1] == 'ã€') or \
           (text[0] == 'ã€' and text[-1] == 'ã€') or \
           (text[0] == "'" and text[-1] == "'"):
            text = text[1:-1].strip()
    
    # ğŸ¯ ç§»é™¤å¥‡æ€ªçš„ç¬¦è™Ÿçµ„åˆ
    # ç§»é™¤ ,} )} :)> !"); ç­‰
    text = re.sub(r'[,\s]*[}\]]\s*', '', text)
    text = re.sub(r'[:\s]*[)\]>]+\s*[?\s]*$', '', text)
    text = re.sub(r'^[,\s]*[{\[]\s*', '', text)
    text = re.sub(r'[!?]*["\';)]+\s*$', '', text)  # ç§»é™¤çµå°¾çš„ !"); ç­‰
    text = re.sub(r'["\';(]+\s*[!?]*\s*$', '', text)  # ç§»é™¤çµå°¾å¼•è™Ÿæ‹¬è™Ÿ
    text = re.sub(r'\s*[!]{2,}["\');\s]*$', '', text)  # ç§»é™¤ !!"); ç­‰
    text = re.sub(r'çš„["\'\s.ã€‚ï¼Œ,]+$', 'çš„', text)  # ä¿®æ­£ã€Œçš„".ã€ç­‰çµå°¾
    text = re.sub(r'ä½ é€™[.\s]*$', 'ä½ é€™å‚¢ä¼™', text)  # è£œå®Œä¸å®Œæ•´å¥å­
    text = re.sub(r'[.\s]+$', '', text)  # ç§»é™¤çµå°¾å¤šé¤˜çš„é»å’Œç©ºæ ¼
    
    # ç§»é™¤é–‹é ­çµå°¾çš„ç‰¹æ®Šç¬¦è™Ÿ
    text = re.sub(r'^[-=_*#]+\s*', '', text)
    text = re.sub(r'\s*[-=_*#]+$', '', text)
    
    # ç§»é™¤ markdown æ ¼å¼
    text = re.sub(r'\*\*(.+?)\*\*', r'\1', text)
    text = re.sub(r'__(.+?)__', r'\1', text)
    text = re.sub(r'`(.+?)`', r'\1', text)
    
    # ğŸ¯ ç§»é™¤å¥ä¸­ç•°å¸¸çš„è‹±æ–‡ç‰‡æ®µï¼ˆä¿ç•™å¸¸è¦‹éŠæˆ²è¡“èªï¼‰
    # å…è¨±çš„è‹±æ–‡ï¼šK, KO, Combo, Gauge, Guard, Attack, Win ç­‰
    allowed_english = ['K', 'KO', 'OK', 'Combo', 'Gauge', 'Guard', 'Attack', 'Win', 
                       'Lose', 'HP', 'MP', 'SP', 'BGM', 'NG', 'GG', 'VS', 'DLC',
                       'Online', 'Offline', 'S', 'A', 'B', 'C', 'D']
    
    def clean_english(match):
        word = match.group(0)
        # ä¿ç•™å…è¨±çš„è‹±æ–‡å’ŒçŸ­è‹±æ–‡
        if word.upper() in [w.upper() for w in allowed_english] or len(word) <= 2:
            return word
        # ç§»é™¤é•·çš„ç•°å¸¸è‹±æ–‡
        return ''
    
    text = re.sub(r'\b[a-zA-Z_]{4,}\b', clean_english, text)
    
    # ğŸ¯ åœ¨è½‰æ›å‰å…ˆæ¸…ç†é€£çºŒé‡è¤‡ï¼ˆå¦‚ï¼šé€™ä»£ç¢¼ä¸éŒ¯é€™ä»£ç¢¼ä¸éŒ¯ï¼‰
    text = remove_inline_repetition(text)
    
    # ğŸ¯ ç°¡é«”è½‰ç¹é«” - å„ªå…ˆä½¿ç”¨ OpenCC
    if OPENCC_CONVERTER:
        try:
            text = OPENCC_CONVERTER.convert(text)
        except Exception as e:
            print(f"âš ï¸ OpenCC è½‰æ›å¤±æ•—: {e}", file=sys.stderr, flush=True)
            # fallback åˆ° txt å­—å…¸
            sorted_mappings = sorted(SIMPLIFIED_TO_TRADITIONAL.items(), key=lambda x: len(x[0]), reverse=True)
            for simp, trad in sorted_mappings:
                text = text.replace(simp, trad)
    else:
        # ä½¿ç”¨å‚™ç”¨ txt å­—å…¸
        sorted_mappings = sorted(SIMPLIFIED_TO_TRADITIONAL.items(), key=lambda x: len(x[0]), reverse=True)
        for simp, trad in sorted_mappings:
            text = text.replace(simp, trad)
    
    # ğŸ¯ ä¸­åœ‹ç”¨èª â†’ å°ç£ç”¨èª - é¡å¤–è£œå…… (OpenCC s2twp å·²åŒ…å«å¤§éƒ¨åˆ†)
    for china, taiwan in CHINA_TO_TAIWAN.items():
        text = text.replace(china, taiwan)
    
    # ç§»é™¤å¤šé¤˜ç©ºæ ¼
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text

def remove_inline_repetition(text: str) -> str:
    """ç§»é™¤å¥ä¸­é€£çºŒé‡è¤‡çš„ç‰‡æ®µï¼ˆå¦‚ï¼šé€™ä»£ç¢¼ä¸éŒ¯é€™ä»£ç¢¼ä¸éŒ¯ï¼‰"""
    if not text or len(text) < 8:
        return text
    
    original = text
    
    # ğŸ¯ æ–¹æ³• 1: åµæ¸¬å®Œå…¨ç›¸åŒçš„é€£çºŒé‡è¤‡
    # å¾é•·åˆ°çŸ­å˜—è©¦æ‰¾é‡è¤‡æ¨¡å¼
    for pattern_len in range(min(25, len(text) // 2), 3, -1):
        for start in range(len(text) - pattern_len * 2 + 1):
            pattern = text[start:start + pattern_len]
            
            # è·³éç´”æ¨™é»æˆ–ç©ºç™½
            if all(c in 'ï¼Œã€‚ï¼ï¼Ÿã€ ï½~' for c in pattern):
                continue
            
            # æª¢æŸ¥æ˜¯å¦é€£çºŒé‡è¤‡
            repeat_pos = start + pattern_len
            if text[repeat_pos:repeat_pos + pattern_len] == pattern:
                # æ‰¾åˆ°é‡è¤‡ï¼è¨ˆç®—é‡è¤‡æ¬¡æ•¸
                count = 2
                check_pos = repeat_pos + pattern_len
                while text[check_pos:check_pos + pattern_len] == pattern:
                    count += 1
                    check_pos += pattern_len
                
                # é‡å»ºæ–‡å­—ï¼šå‰ç¶´ + ä¸€æ¬¡ pattern + å¾Œç¶´
                prefix = text[:start]
                suffix = text[start + pattern_len * count:]
                result = (prefix + pattern + suffix).strip()
                
                if result != original:
                    print(f"ğŸ”§ ç§»é™¤è¡Œå…§é‡è¤‡: {original[:40]} -> {result[:40]}", file=sys.stderr, flush=True)
                    # éè¿´è™•ç†å¯èƒ½çš„å¤šé‡é‡è¤‡
                    return remove_inline_repetition(result)
    
    # ğŸ¯ æ–¹æ³• 2: åµæ¸¬ã€Œç‚ºä»€éº¼...ç‚ºä»€éº¼...ç‚ºä»€éº¼ã€é€™ç¨®éé€£çºŒé‡è¤‡
    # æ‰¾å‡ºé‡è¤‡å‡ºç¾ 3 æ¬¡ä»¥ä¸Šçš„çŸ­èª
    for phrase_len in range(3, min(15, len(text) // 3)):
        for start in range(len(text) - phrase_len):
            phrase = text[start:start + phrase_len]
            if all(c in 'ï¼Œã€‚ï¼ï¼Ÿã€ ï½~' for c in phrase):
                continue
            
            count = text.count(phrase)
            if count >= 3:
                # åªä¿ç•™ç¬¬ä¸€æ¬¡å‡ºç¾
                first_idx = text.find(phrase)
                # ç§»é™¤å¾ŒçºŒé‡è¤‡
                result = text[:first_idx + phrase_len]
                remaining = text[first_idx + phrase_len:]
                remaining = remaining.replace(phrase, '')
                result = (result + remaining).strip()
                
                # æ¸…ç†å¤šé¤˜æ¨™é»
                result = re.sub(r'[ï¼Œã€‚ï¼ï¼Ÿ]{2,}', 'ã€‚', result)
                
                if result != original and len(result) >= 4:
                    print(f"ğŸ”§ ç§»é™¤æ•£è½é‡è¤‡: {original[:40]} -> {result[:40]}", file=sys.stderr, flush=True)
                    return result
    
    return text

def filter_translated_repetition(text: str) -> str:
    """éæ¿¾ç¿»è­¯å¾Œçš„é‡è¤‡å…§å®¹ - åŠ å¼·ç‰ˆ"""
    if not text or len(text) < 4:
        return text
    
    original_text = text
    
    # ğŸ¯ å…ˆç”¨ remove_inline_repetition è™•ç†
    text = remove_inline_repetition(text)
    if text != original_text:
        original_text = text
    
    # ğŸ¯ æ–¹æ³• 0: åµæ¸¬ç©ºæ ¼åˆ†éš”çš„å®Œå…¨ç›¸åŒç‰‡æ®µ (å¦‚ï¼šä¸åœ¨ä¹çš„åŸºå¾·å…ˆç”Ÿ ä¸åœ¨ä¹çš„åŸºå¾·å…ˆç”Ÿ)
    if ' ' in text:
        space_parts = [p.strip() for p in text.split(' ') if p.strip()]
        if len(space_parts) >= 2:
            # æª¢æŸ¥é€£çºŒé‡è¤‡
            unique_space = []
            for p in space_parts:
                if not unique_space or p != unique_space[-1]:
                    # ä¹Ÿæª¢æŸ¥ç›¸ä¼¼åº¦
                    is_dup = False
                    for u in unique_space:
                        if p == u or calculate_similarity(p, u) > 0.7:
                            is_dup = True
                            break
                    if not is_dup:
                        unique_space.append(p)
            
            if len(unique_space) < len(space_parts):
                result = ' '.join(unique_space)
                print(f"ğŸ”§ å»é™¤ç©ºæ ¼é‡è¤‡: {original_text[:40]} -> {result[:40]}", file=sys.stderr, flush=True)
                text = result
                original_text = result
    
    # ğŸ¯ æ–¹æ³• 1: åµæ¸¬é€£çºŒé‡è¤‡çš„å­å­—ä¸² (å¦‚ï¼šæˆ‘å¯ä»¥èµ°äº†å—ï¼Ÿæˆ‘å¯ä»¥èµ°äº†å—ï¼Ÿ)
    cleaned = remove_repeated_substrings(text)
    if cleaned != text:
        print(f"ğŸ”§ å»é™¤é‡è¤‡å­å­—ä¸²: {original_text[:40]} -> {cleaned[:40]}", file=sys.stderr, flush=True)
        return cleaned
    
    # ğŸ¯ æ–¹æ³• 2: æŒ‰æ¨™é»åˆ†å‰²ä¸¦å»é‡
    separators = ['ï¼Œ', 'ã€‚', 'ï¼', 'ï¼Ÿ']
    for sep in separators:
        if sep in text and text.count(sep) >= 1:
            parts = [p.strip() for p in text.split(sep) if p.strip()]
            if len(parts) >= 2:
                # æª¢æŸ¥æ˜¯å¦æœ‰é‡è¤‡æˆ–é«˜åº¦ç›¸ä¼¼
                unique = []
                for p in parts:
                    is_dup = False
                    for u in unique:
                        # ğŸ¯ é™ä½ç›¸ä¼¼åº¦é–¾å€¼ï¼Œæ›´ç©æ¥µå»é‡
                        if p == u or calculate_similarity(p, u) > 0.6:
                            is_dup = True
                            break
                    if not is_dup:
                        unique.append(p)
                
                if len(unique) < len(parts):
                    result = sep.join(unique)
                    if sep in ['ã€‚', 'ï¼', 'ï¼Ÿ']:
                        result = result + sep if not result.endswith(sep) else result
                    print(f"ğŸ”§ å»é™¤ç¿»è­¯é‡è¤‡: {original_text[:40]} -> {result[:40]}", file=sys.stderr, flush=True)
                    return result
    
    return text

def remove_repeated_substrings(text: str) -> str:
    """ç§»é™¤é€£çºŒé‡è¤‡çš„å­å­—ä¸² - ä¿ç•™ä¸é‡è¤‡çš„å‰ç¶´"""
    if len(text) < 8:
        return text
    
    # ğŸ¯ æ–¹æ³• 1: æŒ‰å¥å°¾æ¨™é»åˆ†å‰²ï¼Œæ‰¾å®Œæ•´çš„é‡è¤‡å¥å­
    sentence_endings = ['ã€‚', 'ï¼', 'ï¼Ÿ', '!', '?']
    for ending in sentence_endings:
        if ending in text:
            # æŒ‰å¥å°¾åˆ†å‰²
            parts = []
            current = ""
            for char in text:
                current += char
                if char == ending:
                    if current.strip():
                        parts.append(current.strip())
                    current = ""
            if current.strip():
                parts.append(current.strip())
            
            if len(parts) >= 2:
                # ğŸ¯ ä¿ç•™æ‰€æœ‰ä¸é‡è¤‡çš„å¥å­
                unique = []
                seen = set()
                for p in parts:
                    if p not in seen:
                        unique.append(p)
                        seen.add(p)
                
                # åªæœ‰ç•¶ç¢ºå¯¦æœ‰é‡è¤‡è¢«ç§»é™¤æ™‚æ‰è¿”å›
                if len(unique) < len(parts):
                    return ''.join(unique)
    
    # ğŸ¯ æ–¹æ³• 2: åµæ¸¬é€£çºŒé‡è¤‡çš„å­å­—ä¸²æ¨¡å¼
    # å„ªå…ˆå˜—è©¦è¼ƒé•·çš„æ¨¡å¼ (å¾é•·åˆ°çŸ­)
    for pattern_len in range(min(30, len(text) // 2), 4, -1):
        for start in range(len(text) - pattern_len * 2 + 1):
            pattern = text[start:start + pattern_len]
            
            # è·³éç´”æ¨™é»æˆ–ç©ºç™½
            if all(c in 'ï¼Œã€‚ï¼ï¼Ÿ ã€,.!? ' for c in pattern):
                continue
            
            # ğŸ¯ ç¢ºä¿ pattern ä»¥æ¨™é»çµå°¾
            has_ending = any(pattern.endswith(e) for e in ['ã€‚', 'ï¼', 'ï¼Ÿ', 'ï¼Œ', '!', '?', ','])
            if not has_ending:
                continue
            
            # è¨ˆç®—é€£çºŒå‡ºç¾æ¬¡æ•¸
            count = 0
            pos = 0
            first_idx = -1
            while True:
                idx = text.find(pattern, pos)
                if idx == -1:
                    break
                if first_idx == -1:
                    first_idx = idx
                count += 1
                pos = idx + len(pattern)
            
            # å¦‚æœæ¨¡å¼é€£çºŒå‡ºç¾ 2 æ¬¡ä»¥ä¸Š
            if count >= 2 and len(pattern) * count > len(text) * 0.5:
                # ğŸ¯ ä¿ç•™é‡è¤‡å‰çš„å…§å®¹ + ä¸€æ¬¡é‡è¤‡æ¨¡å¼
                prefix = text[:first_idx].strip() if first_idx > 0 else ""
                result = pattern.strip()
                if prefix:
                    return prefix + result
                return result
    
    return text

def filter_text(text: str) -> str:
    """éæ¿¾ç„¡æ•ˆæ–‡å­—ï¼Œå»é™¤é‡è¤‡å¾Œä¿ç•™æœ‰æ•ˆå…§å®¹ç¹¼çºŒè™•ç†ã€‚"""
    if not text:
        return ""
    
    # æ—¥æ–‡å­—ç¬¦éæ¿¾
    pattern = re.compile(r'[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FFF\uFF00-\uFFEF\u0020-\u007E]+')
    cleaned = "".join(pattern.findall(text)).strip()
    
    if not cleaned:
        return ""
    
    # ğŸ¯ å¹»è¦ºéæ¿¾åˆ—è¡¨ (é€™äº›æ˜¯å®Œå…¨ç„¡æ„ç¾©çš„ï¼Œç›´æ¥éæ¿¾)
    unwanted = [
        # === Whisper å¸¸è¦‹å¹»è¦º ===
        "[éŸ³å£°ãªã—]", "ã”è¦–è´ã‚ã‚ŠãŒã¨ã†", "æœ€å¾Œã¾ã§ã”è¦–è´",
        "(æ‹æ‰‹)", "(ç¬‘ã„)", "(ãŸã‚æ¯)", "å­—å¹•",
        "ãƒãƒ£ãƒ³ãƒãƒ«ç™»éŒ²", "é«˜è©•ä¾¡", "MBSãƒ‹ãƒ¥ãƒ¼ã‚¹",
        "æä¾›ã¯", "ã”è¦§ã„ãŸã ã", "ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸ",
        "ãŠç–²ã‚Œæ§˜ã§ã—ãŸ", "ã¾ãŸä¼šã„ã¾ã—ã‚‡ã†", "ãƒã‚¤ãƒã‚¤",
        "æ¬¡å›ã‚‚", "ãƒãƒ£ãƒ³ãƒãƒ«", "ç™»éŒ²", "ãŠé¡˜ã„ã—ã¾ã™",
        "â™ª", "BGM", "éŸ³æ¥½", "ã‚¨ãƒ³ãƒ‡ã‚£ãƒ³ã‚°",
        "ãƒ†ãƒ­ãƒƒãƒ—", "ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³", "ã‚¢ãƒŠã‚¦ãƒ³ã‚¹",
        # === Initial Prompt è¢«èª¤è¼¸å‡ºçš„å…§å®¹ ===
        "è©±ã—è¨€è‘‰", "ã‚«ã‚¸ãƒ¥ã‚¢ãƒ«ãªè¡¨ç¾", "ãƒãƒƒãƒˆã‚¹ãƒ©ãƒ³ã‚°",
        "VTuberé…ä¿¡", "é…ä¿¡è€…ã¨ãƒªã‚¹ãƒŠãƒ¼", "æ—¥æœ¬èªã®",
        # === å…¶ä»–å¸¸è¦‹å¹»è¦ºæ¨¡å¼ ===
        "ç¿»è¨³", "å­—å¹•æä¾›", "è‡ªå‹•ç”Ÿæˆ", "æ©Ÿæ¢°ç¿»è¨³",
        "ç¶šãã¯", "è©³ã—ãã¯", "ãƒªãƒ³ã‚¯ã¯",
        "æ¦‚è¦æ¬„", "èª¬æ˜æ¬„", "ã‚³ãƒ¡ãƒ³ãƒˆæ¬„",
    ]
    
    for phrase in unwanted:
        if phrase in cleaned:
            return ""
    
    # ğŸ¯ å»é™¤é‡è¤‡å­—ç¬¦å¾Œä¿ç•™æœ‰æ•ˆå…§å®¹
    if detect_character_repetition(cleaned):
        deduped = remove_source_repetition(cleaned)
        if deduped and len(deduped) >= 2:
            print(f"ğŸ”„ å»é™¤æºæ–‡é‡è¤‡: {cleaned[:30]}... -> {deduped[:30]}", file=sys.stderr, flush=True)
            cleaned = deduped
        else:
            print(f"âš ï¸ éæ¿¾ç´”é‡è¤‡: {cleaned[:30]}...", file=sys.stderr, flush=True)
            return ""
    
    # ğŸ¯ å»é™¤é‡è¤‡è©çµ„å¾Œä¿ç•™æœ‰æ•ˆå…§å®¹
    if detect_phrase_repetition(cleaned):
        deduped = remove_source_repetition(cleaned)
        if deduped and len(deduped) >= 2:
            print(f"ğŸ”„ å»é™¤æºæ–‡é‡è¤‡: {cleaned[:30]}... -> {deduped[:30]}", file=sys.stderr, flush=True)
            cleaned = deduped
        else:
            print(f"âš ï¸ éæ¿¾ç´”é‡è¤‡: {cleaned[:30]}...", file=sys.stderr, flush=True)
            return ""
    
    return cleaned if len(cleaned) >= 2 else ""

def remove_source_repetition(text: str) -> str:
    """å¾æ—¥æ–‡æºæ–‡ä¸­å»é™¤é‡è¤‡ï¼Œä¿ç•™æœ‰æ„ç¾©çš„å…§å®¹"""
    if not text or len(text) < 4:
        return text
    
    original = text
    
    # ğŸ¯ æ–¹æ³• 1: æŒ‰ç©ºæ ¼åˆ†å‰²å»é‡
    if ' ' in text:
        parts = text.split(' ')
        unique = []
        seen = set()
        for p in parts:
            p = p.strip()
            if p and p not in seen:
                unique.append(p)
                seen.add(p)
        if len(unique) < len(parts):
            text = ' '.join(unique)
    
    # ğŸ¯ æ–¹æ³• 2: å°‹æ‰¾é‡è¤‡æ¨¡å¼ä¸¦åªä¿ç•™ä¸€æ¬¡
    for pattern_len in range(2, min(30, len(text) // 2 + 1)):
        for start in range(min(5, len(text) - pattern_len * 2)):
            pattern = text[start:start + pattern_len]
            
            # è·³éç´”æ¨™é»æˆ–ç©ºç™½
            if all(c in 'ã€ï¼Œã€‚ï¼ï¼Ÿã€€ ãƒ»ãƒ¼' for c in pattern):
                continue
            
            # è¨ˆç®—é€£çºŒå‡ºç¾æ¬¡æ•¸
            count = text.count(pattern)
            
            if count >= 3 and len(pattern) * count > len(text) * 0.4:
                # æ‰¾åˆ°é‡è¤‡æ¨¡å¼ï¼Œä¿ç•™ä¸€æ¬¡ + å‰å¾Œå…§å®¹
                first_idx = text.find(pattern)
                last_idx = text.rfind(pattern)
                
                prefix = text[:first_idx].strip() if first_idx > 0 else ""
                suffix = text[last_idx + len(pattern):].strip() if last_idx + len(pattern) < len(text) else ""
                
                result = prefix + pattern + suffix
                result = result.strip()
                
                if result and len(result) >= 2:
                    return result
    
    # ğŸ¯ æ–¹æ³• 3: å¦‚æœæ•´å€‹æ–‡å­—åªæ˜¯å–®ä¸€æ¨¡å¼é‡è¤‡
    for pattern_len in range(2, min(20, len(text) // 3 + 1)):
        pattern = text[:pattern_len]
        if all(c in 'ã€ï¼Œã€‚ï¼ï¼Ÿã€€ ãƒ»ãƒ¼' for c in pattern):
            continue
        
        # æª¢æŸ¥æ˜¯å¦æ•´å€‹æ–‡å­—éƒ½æ˜¯é€™å€‹æ¨¡å¼çš„é‡è¤‡
        repeated = pattern * (len(text) // len(pattern) + 1)
        if text in repeated or repeated.startswith(text):
            return pattern.strip()
    
    return text

def detect_character_repetition(text: str) -> bool:
    """åµæ¸¬ç•°å¸¸çš„å­—ç¬¦é‡è¤‡ (å¹»è¦ºç‰¹å¾µ) - å„ªåŒ–ç‰ˆ"""
    if len(text) < 6:
        return False
    
    # ğŸ¯ æ’é™¤å¸¸è¦‹çš„åˆæ³•é‡è¤‡
    valid_patterns = ['ww', 'ãƒ¼ãƒ¼', '...', 'ï¼ï¼', 'ï¼Ÿï¼Ÿ', 'ã€œã€œ']
    temp_text = text
    for vp in valid_patterns:
        temp_text = temp_text.replace(vp, '')
    
    if len(temp_text) < 4:
        return False
    
    # ğŸ¯ è¨ˆç®—æ¯å€‹å­—ç¬¦å‡ºç¾çš„æ¯”ä¾‹ï¼ˆæ’é™¤ç©ºæ ¼å’Œæ¨™é»ï¼‰
    content_chars = [c for c in temp_text if c not in ' ã€€ã€ã€‚ï¼ï¼Ÿï¼Œ']
    if len(content_chars) < 4:
        return False
    
    from collections import Counter
    char_counts = Counter(content_chars)
    max_count = max(char_counts.values())
    
    # å–®å­—ç¬¦ä½”æ¯”è¶…é 35%
    if max_count > len(content_chars) * 0.35:
        return True
    
    # ğŸ¯ åµæ¸¬é€£çºŒé‡è¤‡æ¨¡å¼ (å¦‚ ABCABCABC)
    for pattern_len in range(2, min(15, len(text) // 3 + 1)):
        for start in range(min(3, len(text) - pattern_len * 3)):
            pattern = text[start:start + pattern_len]
            # è·³éç´”æ¨™é»
            if all(c in 'ã€ï¼Œã€‚ï¼ï¼Ÿã€€ ãƒ»' for c in pattern):
                continue
            if pattern * 3 in text:
                return True
    
    return False

def detect_phrase_repetition(text: str) -> bool:
    """åµæ¸¬é‡è¤‡çš„è©çµ„ - åŠ å¼·ç‰ˆ"""
    # ğŸ¯ æ–¹æ³• 1: åµæ¸¬é€£çºŒé‡è¤‡çš„å­å­—ä¸²
    for pattern_len in range(2, min(20, len(text) // 2 + 1)):
        for start in range(len(text) - pattern_len * 2 + 1):
            pattern = text[start:start + pattern_len]
            
            # è·³éç´”æ¨™é»
            if all(c in 'ã€ï¼Œã€‚ï¼ï¼Ÿã€€ ' for c in pattern):
                continue
            
            # æª¢æŸ¥é€£çºŒé‡è¤‡
            if pattern * 3 in text:
                return True
    
    # ğŸ¯ æ–¹æ³• 2: æŒ‰æ¨™é»åˆ†å‰²æª¢æŸ¥
    separators = ['ã€', 'ï¼Œ', 'ã€‚', ' ']
    for sep in separators:
        if sep in text:
            parts = [p.strip() for p in text.split(sep) if p.strip() and len(p.strip()) >= 2]
            if len(parts) >= 3:
                # æª¢æŸ¥é€£çºŒç›¸åŒ
                consecutive = 1
                for i in range(1, len(parts)):
                    if parts[i] == parts[i-1]:
                        consecutive += 1
                        if consecutive >= 2:  # ğŸ¯ é™ä½åˆ° 2 æ¬¡å°±è¦–ç‚ºé‡è¤‡
                            return True
                    else:
                        consecutive = 1
                
                # æª¢æŸ¥ç¸½é‡è¤‡ç‡
                from collections import Counter
                counts = Counter(parts)
                for part, count in counts.items():
                    if count >= 2 and count >= len(parts) * 0.4:  # ğŸ¯ é™ä½é–€æª»
                        return True
    
    return False

def remove_duplicate(current: str, previous: str) -> str:
    """ç§»é™¤èˆ‡ä¸Šä¸€æ¬¡è½‰éŒ„é‡è¤‡çš„éƒ¨åˆ†ã€‚"""
    if not previous or not current:
        return current
    if current == previous or current in previous:
        return ""
    
    # ğŸ¯ æª¢æŸ¥æ˜¯å¦èˆ‡æœ€è¿‘çš„ä»»ä½•ä¸€æ¬¡è½‰éŒ„é‡è¤‡
    for old in recent_texts:
        if current == old or current in old:
            return ""
    
    # ğŸ¯ æ”¹é€²é‡ç–Šæª¢æ¸¬
    if previous in current:
        idx = current.find(previous)
        if idx == 0:
            return current[len(previous):].strip()
    
    # ğŸ¯ æ›´æ™ºèƒ½çš„å¾Œç¶´-å‰ç¶´é‡ç–Šæª¢æ¸¬
    max_overlap = min(len(previous), len(current), 20)  # é™åˆ¶æª¢æ¸¬é•·åº¦
    for i in range(max_overlap, 2, -1):  # è‡³å°‘ 3 å€‹å­—ç¬¦æ‰ç®—é‡ç–Š
        if previous[-i:] == current[:i]:
            return current[i:].strip()
    
    return current

def calculate_similarity(s1: str, s2: str) -> float:
    """è¨ˆç®—å…©å€‹å­—ä¸²çš„ç›¸ä¼¼åº¦ (0-1) - ä½¿ç”¨å¤šç¨®ç®—æ³•"""
    if not s1 or not s2:
        return 0.0
    if s1 == s2:
        return 1.0
    
    # ğŸ¯ æ–¹æ³• 1: å­å­—ä¸²æª¢æ¸¬
    if s1 in s2 or s2 in s1:
        shorter = min(len(s1), len(s2))
        longer = max(len(s1), len(s2))
        return shorter / longer
    
    # ğŸ¯ æ–¹æ³• 2: N-gram ç›¸ä¼¼åº¦ (æ›´æº–ç¢º)
    def get_ngrams(s, n=2):
        return set(s[i:i+n] for i in range(len(s)-n+1)) if len(s) >= n else {s}
    
    ngrams1 = get_ngrams(s1, 2)
    ngrams2 = get_ngrams(s2, 2)
    
    if not ngrams1 or not ngrams2:
        # fallback to character set
        set1 = set(s1)
        set2 = set(s2)
        intersection = len(set1 & set2)
        union = len(set1 | set2)
        return intersection / union if union > 0 else 0.0
    
    intersection = len(ngrams1 & ngrams2)
    union = len(ngrams1 | ngrams2)
    
    return intersection / union if union > 0 else 0.0

def is_duplicate_or_overlap(text: str) -> bool:
    """æª¢æŸ¥æ–‡å­—æ˜¯å¦èˆ‡æœ€è¿‘ç™¼å¸ƒçš„å…§å®¹é‡è¤‡æˆ–é«˜åº¦é‡ç–Š"""
    global recent_texts, last_transcription
    
    if not text:
        return True
    
    # æª¢æŸ¥æ˜¯å¦å®Œå…¨é‡è¤‡
    if text == last_transcription:
        return True
    
    # æª¢æŸ¥æ˜¯å¦ç‚ºå­å­—ä¸²
    if text in last_transcription or last_transcription in text:
        # å¦‚æœæ–°æ–‡å­—æ˜¯èˆŠæ–‡å­—çš„å­å­—ä¸²ï¼Œè·³é
        if text in last_transcription:
            return True
        # å¦‚æœèˆŠæ–‡å­—æ˜¯æ–°æ–‡å­—çš„å­å­—ä¸²ï¼Œè¨ˆç®—æ–°å¢éƒ¨åˆ†
        # ä¸è¦–ç‚ºé‡è¤‡ï¼Œç¨å¾Œæœƒè™•ç†
    
    # æª¢æŸ¥èˆ‡æœ€è¿‘æ–‡å­—çš„ç›¸ä¼¼åº¦
    for recent in recent_texts:
        similarity = calculate_similarity(text, recent)
        if similarity > SIMILARITY_THRESHOLD:
            return True
    
    return False

def extract_new_content(current: str, previous: str) -> str:
    """æå–æ–°å…§å®¹ï¼Œç§»é™¤èˆ‡å‰ä¸€æ¬¡é‡ç–Šçš„éƒ¨åˆ†"""
    if not previous or not current:
        return current
    
    if current == previous:
        return ""
    
    # å¦‚æœå‰ä¸€æ¬¡æ˜¯ç•¶å‰çš„å­å­—ä¸²ï¼Œæå–æ–°å¢éƒ¨åˆ†
    if previous in current:
        idx = current.find(previous)
        if idx == 0:
            # å‰ç¶´é‡è¤‡ï¼Œå–å¾Œé¢çš„æ–°å…§å®¹
            return current[len(previous):].strip()
        elif idx + len(previous) == len(current):
            # å¾Œç¶´é‡è¤‡ï¼Œå–å‰é¢çš„æ–°å…§å®¹
            return current[:idx].strip()
    
    # æª¢æŸ¥å‰ç¶´é‡ç–Š
    for i in range(min(len(previous), len(current)), 0, -1):
        if previous[-i:] == current[:i]:
            new_part = current[i:].strip()
            # åªæœ‰ç•¶æ–°éƒ¨åˆ†æœ‰æ„ç¾©æ™‚æ‰è¿”å›
            if len(new_part) >= 2:
                return new_part
            return ""
    
    # æª¢æŸ¥å¾Œç¶´é‡ç–Š
    for i in range(min(len(previous), len(current)), 0, -1):
        if previous[:i] == current[-i:]:
            new_part = current[:-i].strip()
            if len(new_part) >= 2:
                return new_part
            return ""
    
    return current

# ğŸ¯ æ–°å¢ï¼šå¥å°¾åµæ¸¬å‡½æ•¸
def is_sentence_complete(text: str) -> bool:
    """æª¢æŸ¥æ–‡å­—æ˜¯å¦ç‚ºå®Œæ•´å¥å­"""
    if not text:
        return False
    
    # æ—¥æ–‡å¥å°¾æ¨™è¨˜
    sentence_endings = [
        'ã€‚', 'ï¼', 'ï¼Ÿ', 'ã€',  # æ—¥æ–‡æ¨™é»
        'ã­', 'ã‚ˆ', 'ã‚ˆã­', 'ã‚', 'ã‹',  # èªæ°£è©
        'ã§ã™', 'ã¾ã™', 'ãŸ', 'ã ',  # å‹•è©çµå°¾
        'ã„', 'ã„ã‚ˆ', 'ã„ã­',  # å½¢å®¹è©çµå°¾
        '...', 'â€¦',  # çœç•¥è™Ÿ
    ]
    
    text = text.strip()
    for ending in sentence_endings:
        if text.endswith(ending):
            return True
    
    # å¦‚æœæ–‡å­—è¶…é 15 å€‹å­—ç¬¦ï¼Œå¯èƒ½æ˜¯å®Œæ•´å¥å­
    if len(text) >= 15:
        return True
    
    return False

# ğŸ¯ æ–°å¢ï¼šåˆä½µä¸å®Œæ•´çš„å¥å­
def merge_incomplete_sentence(pending: str, new_text: str) -> tuple:
    """åˆä½µä¸å®Œæ•´çš„å¥å­ï¼Œè¿”å› (å®Œæ•´å¥å­, å‰©é¤˜å¾…è™•ç†)"""
    if not pending:
        combined = new_text
    else:
        combined = pending + new_text
    
    if is_sentence_complete(combined):
        return combined, ""
    else:
        return "", combined

# ----------------------------------------------------
# æ ¸å¿ƒè™•ç†å‡½æ•¸
# ----------------------------------------------------

async def process_audio_chunk(audio_data_b64: str, r):
    """ğŸ¯ ç•°æ­¥ç‰ˆï¼šè™•ç†éŸ³è¨Šå¡Šï¼Œä½¿ç”¨æ»‘å‹•è¦–çª—æ©Ÿåˆ¶"""
    global audio_buffer, overlap_buffer, last_transcription, last_publish_time
    global recent_texts, pending_text, last_full_sentence
    
    # è§£ç¢¼éŸ³è¨Š
    raw_bytes = base64.b64decode(audio_data_b64)
    
    # ğŸ¯ æ¢å¾©é‡ç–Šæ©Ÿåˆ¶ï¼šå°‡é‡ç–Šç·©è¡ + æ–°æ•¸æ“šç´¯ç©
    audio_buffer = overlap_buffer + audio_buffer + raw_bytes
    
    # è¨ˆç®—ç›®æ¨™å¤§å°
    target_size = int(BUFFER_DURATION_S * SAMPLE_RATE * BYTES_PER_SAMPLE)
    overlap_size = int(OVERLAP_DURATION_S * SAMPLE_RATE * BYTES_PER_SAMPLE)
    
    if len(audio_buffer) < target_size:
        return
    
    # å–å‡ºè™•ç†çš„éŸ³è¨Š
    audio_to_process = audio_buffer[:target_size]
    
    # ğŸ¯ ä¿ç•™é‡ç–Šéƒ¨åˆ†ä¾›ä¸‹æ¬¡ä½¿ç”¨
    overlap_buffer = audio_buffer[target_size - overlap_size:target_size]
    audio_buffer = audio_buffer[target_size:]
    
    # è½‰æ›ç‚º numpy array
    audio_array = np.frombuffer(audio_to_process, dtype=np.int16).astype(np.float32) / 32768.0
    
    # ASR è½‰éŒ„ (åœ¨ç·šç¨‹æ± ä¸­åŸ·è¡Œï¼Œé¿å…é˜»å¡ event loop)
    loop = asyncio.get_event_loop()
    text = await loop.run_in_executor(None, whisper_asr, audio_array)
    text = filter_text(text)
    
    if not text:
        return
    
    # ğŸ¯ æª¢æŸ¥æ˜¯å¦èˆ‡æœ€è¿‘å…§å®¹é‡è¤‡
    if is_duplicate_or_overlap(text):
        return
    
    # ğŸ¯ æå–æ–°å…§å®¹
    text = extract_new_content(text, last_transcription)
    if not text or len(text) < 2:
        return
    
    # ğŸ¯ å¥å­å®Œæ•´æ€§è™•ç†
    complete_sentence, pending_text = merge_incomplete_sentence(pending_text, text)
    
    # å¦‚æœæ²’æœ‰å®Œæ•´å¥å­ï¼Œç­‰å¾…æ›´å¤šè³‡æ–™
    if not complete_sentence:
        # ä½†å¦‚æœå¾…è™•ç†æ–‡å­—å¤ªé•·ï¼Œå¼·åˆ¶ç™¼å¸ƒ
        if len(pending_text) >= 30:
            complete_sentence = pending_text
            pending_text = ""
        else:
            return
    
    # æª¢æŸ¥ç™¼å¸ƒé–“éš”
    current_time = time.time()
    if current_time - last_publish_time < MIN_PUBLISH_INTERVAL:
        # é–“éš”å¤ªçŸ­ï¼Œå°‡å…§å®¹åŠ å…¥å¾…è™•ç†
        pending_text = complete_sentence + pending_text
        return
    
    # æ›´æ–°ç‹€æ…‹
    last_transcription = complete_sentence
    last_full_sentence = complete_sentence
    last_publish_time = current_time
    recent_texts.append(complete_sentence)
    context_history.append(complete_sentence)
    
    # ğŸ¯ ç•°æ­¥ LLM ç¿»è­¯
    translation = await llm_translate(complete_sentence)
    
    # ç™¼å¸ƒçµæœ
    tz = timezone(timedelta(hours=8))
    result = {
        "timestamp": datetime.now(tz).strftime("%H:%M:%S"),
        "source_lang": SOURCE_LANG_CODE,
        "target_lang": TARGET_LANG_CODE,
        "duration_s": f"{BUFFER_DURATION_S:.3f}",
        "transcription": complete_sentence,
        "translation": translation
    }
    
    try:
        await r.publish(TRANSLATION_CHANNEL, json.dumps(result, ensure_ascii=False))
    except Exception as e:
        print(f"ç™¼ä½ˆéŒ¯èª¤: {e}", file=sys.stderr, flush=True)

async def main():
    """ğŸ¯ ç•°æ­¥ä¸»å¾ªç’°"""
    global aio_session
    
    init_global_resources()
    
    # ğŸ¯ å»ºç«‹ç•°æ­¥ HTTP session
    aio_session = aiohttp.ClientSession()
    
    try:
        # ğŸ¯ ä½¿ç”¨ç•°æ­¥ Redis
        r = aioredis.Redis(host=REDIS_HOST, port=REDIS_PORT, db=0)
        await r.ping()
        print(f"âœ… Redis é€£ç·šæˆåŠŸ (ç•°æ­¥æ¨¡å¼)", file=sys.stderr, flush=True)
    except Exception as e:
        print(f"âŒ Redis é€£ç·šå¤±æ•—: {e}", file=sys.stderr, flush=True)
        await aio_session.close()
        sys.exit(1)

    p = r.pubsub()
    await p.subscribe(AUDIO_CHANNEL)
    print(f"âœ… å·²è¨‚é–±: {AUDIO_CHANNEL}", file=sys.stderr, flush=True)
    print(f"ğŸ¯ stable-ts æ•´åˆæ¨¡å¼å·²å•Ÿç”¨ (ç•°æ­¥)", file=sys.stderr, flush=True)
    print(f"ğŸ¯ VAD: {USE_VAD}, éœéŸ³æŠ‘åˆ¶: {SUPPRESS_SILENCE}", file=sys.stderr, flush=True)

    try:
        # ğŸ¯ ç•°æ­¥è®€å–è¨Šæ¯
        async for msg in p.listen():
            if msg['type'] == 'message':
                data = msg['data']
                if isinstance(data, bytes):
                    data = data.decode('utf-8')
                await process_audio_chunk(data, r)
    except asyncio.CancelledError:
        print(f"ğŸ›‘ æ”¶åˆ°å–æ¶ˆä¿¡è™Ÿ", file=sys.stderr, flush=True)
    finally:
        # ğŸ¯ æ¸…ç†è³‡æº
        await p.unsubscribe(AUDIO_CHANNEL)
        await r.close()
        await aio_session.close()
        print(f"âœ… è³‡æºå·²æ¸…ç†", file=sys.stderr, flush=True)

if __name__ == "__main__":
    asyncio.run(main())