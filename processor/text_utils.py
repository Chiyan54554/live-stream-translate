"""
æ–‡å­—è™•ç†æ¨¡çµ„ - éæ¿¾ã€å»é‡ã€æ¸…ç†
ğŸš€ å„ªåŒ–ç‰ˆï¼šé ç·¨è­¯æ­£å‰‡ã€é«˜æ•ˆè³‡æ–™çµæ§‹ã€æ¸›å°‘é‡è¤‡é‹ç®—
"""
import re
import sys
from collections import Counter
from functools import lru_cache

# ============================================================
# ğŸš€ é ç·¨è­¯æ­£å‰‡è¡¨é”å¼ï¼ˆæ¨¡çµ„è¼‰å…¥æ™‚åªç·¨è­¯ä¸€æ¬¡ï¼‰
# ============================================================

RE_JAPANESE_CHARS = re.compile(r'[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FFF\uFF00-\uFFEF\u0020-\u007E]+')
RE_PUNCTUATION_CLEANUP = re.compile(r'[ï¼Œã€‚ï¼ï¼Ÿ]{2,}')
RE_GIBBERISH_PUNCT = re.compile(r'[ï¼Œã€‚ï¼ï¼Ÿã€ï½~\s]+')

# æ¨™é»ç¬¦è™Ÿé›†åˆï¼ˆO(1) æŸ¥æ‰¾ï¼‰
PUNCTUATION_SET = frozenset('ï¼Œã€‚ï¼ï¼Ÿã€ ï½~')
PUNCTUATION_ONLY_SET = frozenset('ã€ï¼Œã€‚ï¼ï¼Ÿã€€ ãƒ»')
SENTENCE_ENDINGS_SET = frozenset(('ã€‚', 'ï¼', 'ï¼Ÿ', '!', '?'))

# å¹»è¦ºéæ¿¾åˆ—è¡¨ï¼ˆfrozenset O(1) æŸ¥æ‰¾ï¼‰
UNWANTED_PHRASES = frozenset({
    "[éŸ³å£°ãªã—]", "ã”è¦–è´ã‚ã‚ŠãŒã¨ã†", "æœ€å¾Œã¾ã§ã”è¦–è´",
    "(æ‹æ‰‹)", "(ç¬‘ã„)", "(ãŸã‚æ¯)", "å­—å¹•",
    "ãƒãƒ£ãƒ³ãƒãƒ«ç™»éŒ²", "é«˜è©•ä¾¡", "MBSãƒ‹ãƒ¥ãƒ¼ã‚¹",
    "æä¾›ã¯", "ã”è¦§ã„ãŸã ã", "ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸ",
    "ãŠç–²ã‚Œæ§˜ã§ã—ãŸ", "ã¾ãŸä¼šã„ã¾ã—ã‚‡ã†", "ãƒã‚¤ãƒã‚¤",
    "æ¬¡å›ã‚‚", "ãƒãƒ£ãƒ³ãƒãƒ«", "ç™»éŒ²", "ãŠé¡˜ã„ã—ã¾ã™",
    "â™ª", "BGM", "éŸ³æ¥½", "ã‚¨ãƒ³ãƒ‡ã‚£ãƒ³ã‚°",
    "ãƒ†ãƒ­ãƒƒãƒ—", "ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³", "ã‚¢ãƒŠã‚¦ãƒ³ã‚¹",
    "è©±ã—è¨€è‘‰", "ã‚«ã‚¸ãƒ¥ã‚¢ãƒ«ãªè¡¨ç¾", "ãƒãƒƒãƒˆã‚¹ãƒ©ãƒ³ã‚°",
    "VTuberé…ä¿¡", "é…ä¿¡è€…ã¨ãƒªã‚¹ãƒŠãƒ¼", "æ—¥æœ¬èªã®",
    "ç¿»è¨³", "å­—å¹•æä¾›", "è‡ªå‹•ç”Ÿæˆ", "æ©Ÿæ¢°ç¿»è¨³",
    "ç¶šãã¯", "è©³ã—ãã¯", "ãƒªãƒ³ã‚¯ã¯",
    "æ¦‚è¦æ¬„", "èª¬æ˜æ¬„", "ã‚³ãƒ¡ãƒ³ãƒˆæ¬„",
})

# ğŸ¯ ASR å¹»è¦ºæ¨¡å¼ï¼ˆæ­£å‰‡åŒ¹é…ï¼‰
RE_ASR_HALLUCINATION = re.compile(
    r'^[JKLMNOPQRSTUVWXYZï¼ªï¼«ï¼¬ï¼­ï¼®ï¼¯ï¼°ï¼±ï¼²ï¼³ï¼´ï¼µï¼¶ï¼·ï¼¸ï¼¹ï¼º][\u4e00-\u9fff]+'
    r'[JKLMNOPQRSTUVWXYZï¼ªï¼«ï¼¬ï¼­ï¼®ï¼¯ï¼°ï¼±ï¼²ï¼³ï¼´ï¼µï¼¶ï¼·ï¼¸ï¼¹ï¼º]'  # Jæ„›å¿ƒJ æ¨¡å¼
)
RE_BROKEN_PATTERN = re.compile(r'([^\s]{1,3}[JKL][^\s]{1,3}){2,}')  # é‡è¤‡ J/K/L æ¨¡å¼

# ğŸ¯ éæ¿¾ã€ŒJ+ä¸­æ–‡ã€é–‹é ­çš„ç¿»è­¯ï¼ˆASR å¸¸è¦‹å¹»è¦ºï¼‰
RE_J_PREFIX_HALLUCINATION = re.compile(r'^[JKLMN]\s*[\u4e00-\u9fff]')

# ğŸ¯ ç¿»è­¯å“è³ªæª¢æ¸¬ï¼ˆé‡è¤‡è©æ¨¡å¼ï¼‰
RE_REPEATED_WORDS = re.compile(r'(è¦ä¸è¦|æƒ³è¦|æ˜¯ä¸æ˜¯|æœ‰æ²’æœ‰|å¯ä¸å¯ä»¥){3,}')
RE_STUTTERING = re.compile(r'(.{2,4})\1{2,}')  # é€£çºŒé‡è¤‡ 2-4 å­— 3æ¬¡ä»¥ä¸Š

# å¥å°¾çµæŸè©ï¼ˆtuple è¼ƒå¿«è¿­ä»£ï¼‰
SENTENCE_ENDINGS = (
    'ã€‚', 'ï¼', 'ï¼Ÿ', 'ã€',
    'ã­', 'ã‚ˆ', 'ã‚ˆã­', 'ã‚', 'ã‹',
    'ã§ã™', 'ã¾ã™', 'ãŸ', 'ã ',
    'ã„', 'ã„ã‚ˆ', 'ã„ã­',
    '...', 'â€¦',
)

# åŒç¾©è©ç¾¤çµ„ï¼ˆé ç·¨è­¯æ­£å‰‡ï¼‰
SIMILAR_GROUPS_PATTERNS = tuple(
    (re.compile('(' + '|'.join(re.escape(w) for w in group) + ')'), group[0])
    for group in (
        ('å³é‚Š', 'å³å´', 'å³é¢'),
        ('å·¦é‚Š', 'å·¦å´', 'å·¦é¢'),
        ('ä¸Šé¢', 'ä¸Šé‚Š', 'ä¸Šæ–¹'),
        ('ä¸‹é¢', 'ä¸‹é‚Š', 'ä¸‹æ–¹'),
        ('å‰é¢', 'å‰é‚Š', 'å‰æ–¹'),
        ('å¾Œé¢', 'å¾Œé‚Š', 'å¾Œæ–¹'),
        ('é€™é‚Š', 'é€™è£¡', 'é€™å…’'),
        ('é‚£é‚Š', 'é‚£è£¡', 'é‚£å…’'),
    )
)

# æœ‰æ•ˆé‡è¤‡æ¨¡å¼ï¼ˆç”¨æ–¼ detect_character_repetitionï¼‰
VALID_REPEAT_PATTERNS = frozenset({'ww', 'ãƒ¼ãƒ¼', '...', 'ï¼ï¼', 'ï¼Ÿï¼Ÿ', 'ã€œã€œ'})

# å¸¸è¦‹æœ‰æ„ç¾©çš„é‡è¤‡ï¼ˆä¸æ‡‰è¢«éæ¿¾ï¼‰
COMMON_VALID_REPEATS = frozenset({'å“ˆ', 'å‘µ', 'å˜¿', 'å—¯', 'å•Š', 'æ¬¸', 'å–”', 'å™¢', 'è€¶', 'å”‰', 'å˜»', 'ç¬‘'})

# éŸ³è­¯å¸¸ç”¨å­—ï¼ˆfrozenset O(1) æŸ¥æ‰¾ï¼‰
TRANSLITERATION_CHARS = frozenset(
    'å·´æ‰˜æ–¯æ‹‰é”é¦¬å¡å¸•å¡”ç“¦è–©ç´æ‹‰è«è«¾æ´›ç¾…æ³¢ç´¢ä½å¤šç§‘æˆˆä¼Šå°¼é‡ŒåŸºç±³å¸Œ'
    'å…‹å¾·ç‰¹çˆ¾å¸ƒæ ¼æ©å§†å¸«å¤«å‰æ–¯å®‰åˆ—æ–‡èŒ²è¨±å‹’è’‚å¨œé›…è©²èµ«é˜¿å¡'
    'æ¢…æ³°å…‹å°¼å¾·å‰é›·æˆˆè²ç±³æ£®å°¼å…‹ç»´å…‹å¡å·'
)

# æ’é™¤çš„å¸¸ç”¨å­—ï¼ˆfrozenset O(1) æŸ¥æ‰¾ï¼‰
COMMON_CHARS_EXCLUDE = frozenset('çš„æ˜¯äº†åœ¨æœ‰æˆ‘ä½ ä»–å¥¹å®ƒå€‘é€™é‚£ä¸ä¹Ÿå°±éƒ½è€Œä¸”ä½†åªè¦å¦‚æœå› ç‚ºæ‰€ä»¥é‚„å¯ä»¥å¾ˆå¤ªçœŸå¥½å£')


@lru_cache(maxsize=256)
def _get_bigrams(s: str) -> frozenset:
    """å–å¾—å­—ä¸²çš„ bigram é›†åˆï¼ˆå¿«å–çµæœï¼‰"""
    if len(s) < 2:
        return frozenset({s})
    return frozenset(s[i:i+2] for i in range(len(s)-1))


def calculate_similarity(s1: str, s2: str) -> float:
    """è¨ˆç®—å…©å€‹å­—ä¸²çš„ç›¸ä¼¼åº¦ (0-1) - å„ªåŒ–ç‰ˆ"""
    if not s1 or not s2:
        return 0.0
    if s1 == s2:
        return 1.0
    
    len1, len2 = len(s1), len(s2)
    
    # æ–¹æ³• 1: å­å­—ä¸²æª¢æ¸¬ï¼ˆå…ˆæª¢æŸ¥è¼ƒçŸ­çš„ï¼‰
    if len1 <= len2:
        if s1 in s2:
            return len1 / len2
    else:
        if s2 in s1:
            return len2 / len1
    
    # æ–¹æ³• 2: Bigram Jaccard ç›¸ä¼¼åº¦ï¼ˆä½¿ç”¨å¿«å–ï¼‰
    bigrams1 = _get_bigrams(s1)
    bigrams2 = _get_bigrams(s2)
    
    intersection = len(bigrams1 & bigrams2)
    union = len(bigrams1 | bigrams2)
    
    return intersection / union if union > 0 else 0.0


def remove_inline_repetition(text: str) -> str:
    """ç§»é™¤å¥ä¸­é€£çºŒé‡è¤‡çš„ç‰‡æ®µ - å„ªåŒ–ç‰ˆï¼ˆé ç·¨è­¯æ­£å‰‡ï¼‰"""
    if not text or len(text) < 8:
        return text
    
    original = text
    
    # æ–¹æ³• 0: åµæ¸¬é€£çºŒç›¸ä¼¼è©ï¼ˆä½¿ç”¨é ç·¨è­¯æ­£å‰‡ï¼‰
    for pattern, replacement in SIMILAR_GROUPS_PATTERNS:
        matches = pattern.findall(text)
        if len(matches) >= 3:
            # ç§»é™¤é€£çºŒå‡ºç¾çš„åŒç¾©è©ï¼ˆä¿ç•™ç¬¬ä¸€å€‹ï¼‰
            result = pattern.sub(lambda m, c=[0]: (c.__setitem__(0, c[0]+1), m.group(0) if c[0] == 1 else '')[1], text)
            if result != text:
                print(f"ğŸ”§ ç§»é™¤åŒç¾©è©é‡è¤‡: {text[:40]} -> {result[:40]}", file=sys.stderr, flush=True)
                text = result
                original = result
    
    # æ–¹æ³• 1: åµæ¸¬å®Œå…¨ç›¸åŒçš„é€£çºŒé‡è¤‡ï¼ˆå„ªåŒ–ï¼šå¾å¤§åˆ°å°ï¼Œæ‰¾åˆ°å³è¿”å›ï¼‰
    text_len = len(text)
    max_pattern_len = min(25, text_len // 2)
    
    for pattern_len in range(max_pattern_len, 3, -1):
        search_range = text_len - pattern_len * 2 + 1
        for start in range(search_range):
            pattern = text[start:start + pattern_len]
            
            # ä½¿ç”¨ frozenset å¿«é€Ÿæª¢æŸ¥ï¼ˆO(1)ï¼‰
            if all(c in PUNCTUATION_SET for c in pattern):
                continue
            
            repeat_pos = start + pattern_len
            if text[repeat_pos:repeat_pos + pattern_len] == pattern:
                # è¨ˆç®—é‡è¤‡æ¬¡æ•¸
                count = 2
                check_pos = repeat_pos + pattern_len
                while check_pos + pattern_len <= text_len and text[check_pos:check_pos + pattern_len] == pattern:
                    count += 1
                    check_pos += pattern_len
                
                # ç›´æ¥æ‹¼æ¥çµæœ
                result = (text[:start] + pattern + text[start + pattern_len * count:]).strip()
                
                if result != original:
                    print(f"ğŸ”§ ç§»é™¤è¡Œå…§é‡è¤‡: {original[:40]} -> {result[:40]}", file=sys.stderr, flush=True)
                    return remove_inline_repetition(result)
    
    # æ–¹æ³• 2: åµæ¸¬éé€£çºŒé‡è¤‡ï¼ˆå„ªåŒ–ï¼šæå‰çµ‚æ­¢ï¼‰
    max_phrase_len = min(15, text_len // 3)
    for phrase_len in range(3, max_phrase_len):
        search_limit = text_len - phrase_len
        for start in range(search_limit):
            phrase = text[start:start + phrase_len]
            
            # ä½¿ç”¨ frozenset O(1) æª¢æŸ¥
            if all(c in PUNCTUATION_SET for c in phrase):
                continue
            
            count = text.count(phrase)
            if count >= 3:
                first_idx = text.find(phrase)
                result = text[:first_idx + phrase_len] + text[first_idx + phrase_len:].replace(phrase, '')
                result = RE_PUNCTUATION_CLEANUP.sub('ã€‚', result).strip()
                
                if result != original and len(result) >= 4:
                    print(f"ğŸ”§ ç§»é™¤æ•£è½é‡è¤‡: {original[:40]} -> {result[:40]}", file=sys.stderr, flush=True)
                    return result
    
    return text


def remove_repeated_substrings(text: str) -> str:
    """ç§»é™¤é€£çºŒé‡è¤‡çš„å­å­—ä¸² - ä¿ç•™ä¸é‡è¤‡çš„å‰ç¶´"""
    if len(text) < 8:
        return text
    
    # æŒ‰å¥å°¾æ¨™é»åˆ†å‰²
    sentence_endings = ['ã€‚', 'ï¼', 'ï¼Ÿ', '!', '?']
    for ending in sentence_endings:
        if ending in text:
            parts = []
            current = ""
            for char in text:
                current += char
                if char == ending:
                    if current.strip():
                        parts.append(current.strip())
                    current = ""
            if current.strip():
                parts.append(current.strip())
            
            if len(parts) >= 2:
                unique = []
                seen = set()
                for p in parts:
                    if p not in seen:
                        unique.append(p)
                        seen.add(p)
                
                if len(unique) < len(parts):
                    return ''.join(unique)
    
    # åµæ¸¬é€£çºŒé‡è¤‡çš„å­å­—ä¸²æ¨¡å¼
    for pattern_len in range(min(30, len(text) // 2), 4, -1):
        for start in range(len(text) - pattern_len * 2 + 1):
            pattern = text[start:start + pattern_len]
            
            if all(c in 'ï¼Œã€‚ï¼ï¼Ÿ ã€,.!? ' for c in pattern):
                continue
            
            has_ending = any(pattern.endswith(e) for e in ['ã€‚', 'ï¼', 'ï¼Ÿ', 'ï¼Œ', '!', '?', ','])
            if not has_ending:
                continue
            
            count = 0
            pos = 0
            first_idx = -1
            while True:
                idx = text.find(pattern, pos)
                if idx == -1:
                    break
                if first_idx == -1:
                    first_idx = idx
                count += 1
                pos = idx + len(pattern)
            
            if count >= 2 and len(pattern) * count > len(text) * 0.5:
                prefix = text[:first_idx].strip() if first_idx > 0 else ""
                result = pattern.strip()
                if prefix:
                    return prefix + result
                return result
    
    return text


def filter_translated_repetition(text: str) -> str:
    """éæ¿¾ç¿»è­¯å¾Œçš„é‡è¤‡å…§å®¹ - åŠ å¼·ç‰ˆ"""
    if not text or len(text) < 4:
        return text
    
    original_text = text
    
    # ğŸ¯ éæ¿¾æ˜é¡¯çš„ç¿»è­¯å“è³ªå•é¡Œï¼ˆè¦ä¸è¦è¦ä¸è¦ã€æƒ³è¦æƒ³è¦æƒ³è¦ï¼‰
    if RE_REPEATED_WORDS.search(text):
        print(f"âš ï¸ éæ¿¾é‡è¤‡è©ç¿»è­¯: {text[:40]}", file=sys.stderr, flush=True)
        return ""
    
    # ğŸ¯ éæ¿¾é€£çºŒçµå·´æ¨¡å¼ï¼ˆXX XX XXï¼‰
    stuttering_match = RE_STUTTERING.search(text)
    if stuttering_match:
        # å˜—è©¦ä¿®å¾©ï¼šåªä¿ç•™ä¸€æ¬¡é‡è¤‡
        pattern = stuttering_match.group(1)
        fixed = RE_STUTTERING.sub(pattern, text)
        if fixed != text:
            print(f"ğŸ”§ ä¿®å¾©çµå·´ç¿»è­¯: {text[:40]} -> {fixed[:40]}", file=sys.stderr, flush=True)
            text = fixed
            original_text = fixed
    
    # ğŸ¯ éæ¿¾ã€ŒJ+ä¸­æ–‡ã€é–‹é ­çš„å¹»è¦ºç¿»è­¯
    if RE_J_PREFIX_HALLUCINATION.match(text):
        print(f"âš ï¸ éæ¿¾ J å‰ç¶´å¹»è¦º: {text[:40]}", file=sys.stderr, flush=True)
        return ""
    
    # å…ˆç”¨ remove_inline_repetition è™•ç†
    text = remove_inline_repetition(text)
    if text != original_text:
        original_text = text
    
    # åµæ¸¬ç©ºæ ¼åˆ†éš”çš„å®Œå…¨ç›¸åŒç‰‡æ®µ
    if ' ' in text:
        space_parts = [p.strip() for p in text.split(' ') if p.strip()]
        if len(space_parts) >= 2:
            unique_space = []
            for p in space_parts:
                if not unique_space or p != unique_space[-1]:
                    is_dup = False
                    for u in unique_space:
                        if p == u or calculate_similarity(p, u) > 0.7:
                            is_dup = True
                            break
                    if not is_dup:
                        unique_space.append(p)
            
            if len(unique_space) < len(space_parts):
                result = ' '.join(unique_space)
                print(f"ğŸ”§ å»é™¤ç©ºæ ¼é‡è¤‡: {original_text[:40]} -> {result[:40]}", file=sys.stderr, flush=True)
                text = result
                original_text = result
    
    # åµæ¸¬é€£çºŒé‡è¤‡çš„å­å­—ä¸²
    cleaned = remove_repeated_substrings(text)
    if cleaned != text:
        print(f"ğŸ”§ å»é™¤é‡è¤‡å­å­—ä¸²: {original_text[:40]} -> {cleaned[:40]}", file=sys.stderr, flush=True)
        return cleaned
    
    # æŒ‰æ¨™é»åˆ†å‰²ä¸¦å»é‡
    separators = ['ï¼Œ', 'ã€‚', 'ï¼', 'ï¼Ÿ']
    for sep in separators:
        if sep in text and text.count(sep) >= 1:
            parts = [p.strip() for p in text.split(sep) if p.strip()]
            if len(parts) >= 2:
                unique = []
                for p in parts:
                    is_dup = False
                    for u in unique:
                        if p == u or calculate_similarity(p, u) > 0.6:
                            is_dup = True
                            break
                    if not is_dup:
                        unique.append(p)
                
                if len(unique) < len(parts):
                    result = sep.join(unique)
                    if sep in ['ã€‚', 'ï¼', 'ï¼Ÿ']:
                        result = result + sep if not result.endswith(sep) else result
                    print(f"ğŸ”§ å»é™¤ç¿»è­¯é‡è¤‡: {original_text[:40]} -> {result[:40]}", file=sys.stderr, flush=True)
                    return result
    
    return text


def detect_character_repetition(text: str) -> bool:
    """åµæ¸¬ç•°å¸¸çš„å­—ç¬¦é‡è¤‡ - å„ªåŒ–ç‰ˆï¼ˆfrozenset + æå‰çµ‚æ­¢ï¼‰"""
    text_len = len(text)
    if text_len < 6:
        return False
    
    # ç§»é™¤æœ‰æ•ˆæ¨¡å¼ï¼ˆä½¿ç”¨ frozenset æª¢æŸ¥ï¼‰
    temp_text = text
    for vp in VALID_REPEAT_PATTERNS:
        if vp in temp_text:
            temp_text = temp_text.replace(vp, '')
    
    if len(temp_text) < 4:
        return False
    
    # ä½¿ç”¨ frozenset O(1) éæ¿¾
    content_chars = [c for c in temp_text if c not in PUNCTUATION_ONLY_SET]
    content_len = len(content_chars)
    if content_len < 4:
        return False
    
    # Counter çµ±è¨ˆ
    char_counts = Counter(content_chars)
    max_count = max(char_counts.values())
    threshold = content_len * 0.35
    
    if max_count > threshold:
        return True
    
    # æ¨¡å¼é‡è¤‡æª¢æ¸¬ï¼ˆå„ªåŒ–è¿´åœˆç¯„åœï¼‰
    max_pattern = min(15, text_len // 3 + 1)
    for pattern_len in range(2, max_pattern):
        max_start = min(3, text_len - pattern_len * 3)
        for start in range(max_start):
            pattern = text[start:start + pattern_len]
            # ä½¿ç”¨ frozenset O(1) æª¢æŸ¥
            if all(c in PUNCTUATION_ONLY_SET for c in pattern):
                continue
            if pattern * 3 in text:
                return True
    
    return False


def detect_phrase_repetition(text: str) -> bool:
    """åµæ¸¬é‡è¤‡çš„è©çµ„"""
    for pattern_len in range(2, min(20, len(text) // 2 + 1)):
        for start in range(len(text) - pattern_len * 2 + 1):
            pattern = text[start:start + pattern_len]
            if all(c in 'ã€ï¼Œã€‚ï¼ï¼Ÿã€€ ' for c in pattern):
                continue
            if pattern * 3 in text:
                return True
    
    separators = ['ã€', 'ï¼Œ', 'ã€‚', ' ']
    for sep in separators:
        if sep in text:
            parts = [p.strip() for p in text.split(sep) if p.strip() and len(p.strip()) >= 2]
            if len(parts) >= 3:
                consecutive = 1
                for i in range(1, len(parts)):
                    if parts[i] == parts[i-1]:
                        consecutive += 1
                        if consecutive >= 2:
                            return True
                    else:
                        consecutive = 1
                
                counts = Counter(parts)
                for part, count in counts.items():
                    if count >= 2 and count >= len(parts) * 0.4:
                        return True
    
    return False


def remove_source_repetition(text: str) -> str:
    """å¾æ—¥æ–‡æºæ–‡ä¸­å»é™¤é‡è¤‡ï¼Œä¿ç•™æœ‰æ„ç¾©çš„å…§å®¹"""
    if not text or len(text) < 4:
        return text
    
    # æŒ‰ç©ºæ ¼åˆ†å‰²å»é‡
    if ' ' in text:
        parts = text.split(' ')
        unique = []
        seen = set()
        for p in parts:
            p = p.strip()
            if p and p not in seen:
                unique.append(p)
                seen.add(p)
        if len(unique) < len(parts):
            text = ' '.join(unique)
    
    # å°‹æ‰¾é‡è¤‡æ¨¡å¼ä¸¦åªä¿ç•™ä¸€æ¬¡
    for pattern_len in range(2, min(30, len(text) // 2 + 1)):
        for start in range(min(5, len(text) - pattern_len * 2)):
            pattern = text[start:start + pattern_len]
            
            if all(c in 'ã€ï¼Œã€‚ï¼ï¼Ÿã€€ ãƒ»ãƒ¼' for c in pattern):
                continue
            
            count = text.count(pattern)
            
            if count >= 3 and len(pattern) * count > len(text) * 0.4:
                first_idx = text.find(pattern)
                last_idx = text.rfind(pattern)
                
                prefix = text[:first_idx].strip() if first_idx > 0 else ""
                suffix = text[last_idx + len(pattern):].strip() if last_idx + len(pattern) < len(text) else ""
                
                result = prefix + pattern + suffix
                result = result.strip()
                
                if result and len(result) >= 2:
                    return result
    
    # å¦‚æœæ•´å€‹æ–‡å­—åªæ˜¯å–®ä¸€æ¨¡å¼é‡è¤‡
    for pattern_len in range(2, min(20, len(text) // 3 + 1)):
        pattern = text[:pattern_len]
        if all(c in 'ã€ï¼Œã€‚ï¼ï¼Ÿã€€ ãƒ»ãƒ¼' for c in pattern):
            continue
        
        repeated = pattern * (len(text) // len(pattern) + 1)
        if text in repeated or repeated.startswith(text):
            return pattern.strip()
    
    return text


def filter_text(text: str) -> str:
    """éæ¿¾ç„¡æ•ˆæ–‡å­— - å„ªåŒ–ç‰ˆï¼ˆé ç·¨è­¯æ­£å‰‡ + frozensetï¼‰"""
    if not text:
        return ""
    
    # æ—¥æ–‡å­—ç¬¦éæ¿¾ï¼ˆä½¿ç”¨é ç·¨è­¯æ­£å‰‡ï¼‰
    cleaned = "".join(RE_JAPANESE_CHARS.findall(text)).strip()
    
    if not cleaned:
        return ""
    
    # ğŸ¯ ASR å¹»è¦ºéæ¿¾ï¼ˆJæ„›å¿ƒJ ç­‰æ¨¡å¼ï¼‰
    if RE_ASR_HALLUCINATION.search(cleaned) or RE_BROKEN_PATTERN.search(cleaned):
        print(f"âš ï¸ éæ¿¾ ASR å¹»è¦º: {cleaned[:40]}", file=sys.stderr, flush=True)
        return ""
    
    # å¹»è¦ºéæ¿¾ï¼ˆä½¿ç”¨ frozenset O(1) æŸ¥æ‰¾ï¼‰
    for phrase in UNWANTED_PHRASES:
        if phrase in cleaned:
            return ""
    
    # å»é™¤é‡è¤‡å­—ç¬¦
    if detect_character_repetition(cleaned):
        deduped = remove_source_repetition(cleaned)
        if deduped and len(deduped) >= 2:
            print(f"ğŸ”„ å»é™¤æºæ–‡é‡è¤‡: {cleaned[:30]}... -> {deduped[:30]}", file=sys.stderr, flush=True)
            cleaned = deduped
        else:
            print(f"âš ï¸ éæ¿¾ç´”é‡è¤‡: {cleaned[:30]}...", file=sys.stderr, flush=True)
            return ""
    
    # å»é™¤é‡è¤‡è©çµ„
    if detect_phrase_repetition(cleaned):
        deduped = remove_source_repetition(cleaned)
        if deduped and len(deduped) >= 2:
            print(f"ğŸ”„ å»é™¤æºæ–‡é‡è¤‡: {cleaned[:30]}... -> {deduped[:30]}", file=sys.stderr, flush=True)
            cleaned = deduped
        else:
            print(f"âš ï¸ éæ¿¾ç´”é‡è¤‡: {cleaned[:30]}...", file=sys.stderr, flush=True)
            return ""
    
    return cleaned if len(cleaned) >= 2 else ""


def is_sentence_complete(text: str) -> bool:
    """æª¢æŸ¥æ–‡å­—æ˜¯å¦ç‚ºå®Œæ•´å¥å­ - å„ªåŒ–ç‰ˆï¼ˆtuple è¿­ä»£ï¼‰"""
    if not text:
        return False
    
    text = text.strip()
    text_len = len(text)
    
    # é•·åº¦è¶³å¤ å³è¦–ç‚ºå®Œæ•´
    if text_len >= 15:
        return True
    
    # ä½¿ç”¨é å®šç¾© tupleï¼ˆæ¯” list å¿«ï¼‰
    for ending in SENTENCE_ENDINGS:
        if text.endswith(ending):
            return True
    
    return False


def merge_incomplete_sentence(pending: str, new_text: str) -> tuple:
    """åˆä½µä¸å®Œæ•´çš„å¥å­ï¼Œè¿”å› (å®Œæ•´å¥å­, å‰©é¤˜å¾…è™•ç†)"""
    if not pending:
        combined = new_text
    else:
        combined = pending + new_text
    
    if is_sentence_complete(combined):
        return combined, ""
    else:
        return "", combined


def extract_new_content(current: str, previous: str) -> str:
    """æå–æ–°å…§å®¹ï¼Œç§»é™¤èˆ‡å‰ä¸€æ¬¡é‡ç–Šçš„éƒ¨åˆ†"""
    if not previous or not current:
        return current
    
    if current == previous:
        return ""
    
    if previous in current:
        idx = current.find(previous)
        if idx == 0:
            return current[len(previous):].strip()
        elif idx + len(previous) == len(current):
            return current[:idx].strip()
    
    for i in range(min(len(previous), len(current)), 0, -1):
        if previous[-i:] == current[:i]:
            new_part = current[i:].strip()
            if len(new_part) >= 2:
                return new_part
            return ""
    
    for i in range(min(len(previous), len(current)), 0, -1):
        if previous[:i] == current[-i:]:
            new_part = current[:-i].strip()
            if len(new_part) >= 2:
                return new_part
            return ""
    
    return current


def detect_gibberish_transliteration(text: str) -> bool:
    """åµæ¸¬ç„¡æ„ç¾©çš„éŸ³è­¯ä¸² - å„ªåŒ–ç‰ˆï¼ˆé ç·¨è­¯æ­£å‰‡ + frozensetï¼‰"""
    if not text or len(text) < 8:
        return False
    
    # ç§»é™¤æ¨™é»ç¬¦è™Ÿï¼ˆä½¿ç”¨é ç·¨è­¯æ­£å‰‡ï¼‰
    clean_text = RE_GIBBERISH_PUNCT.sub('', text)
    clean_len = len(clean_text)
    if clean_len < 6:
        return False
    
    # 1. ç›¸åŒéŸ³ç¯€é‡è¤‡æª¢æ¸¬
    for syllable_len in range(2, 5):
        max_start = clean_len - syllable_len * 3
        for start in range(max_start):
            syllable = clean_text[start:start + syllable_len]
            repeated = syllable * 3
            if repeated in clean_text:
                # ä½¿ç”¨ frozenset O(1) æª¢æŸ¥
                if not any(syllable.startswith(c) for c in COMMON_VALID_REPEATS):
                    return True
    
    # 2. éŸ³è­¯æ¯”ä¾‹æª¢æ¸¬ï¼ˆä½¿ç”¨ frozenset O(1) æŸ¥æ‰¾ï¼‰
    text_chars = [c for c in clean_text if c not in COMMON_CHARS_EXCLUDE]
    if text_chars:
        # ä½¿ç”¨ç”Ÿæˆå™¨é¿å…å»ºç«‹ä¸­é–“åˆ—è¡¨
        transliteration_count = sum(1 for c in text_chars if c in TRANSLITERATION_CHARS)
        transliteration_ratio = transliteration_count / len(text_chars)
        if transliteration_ratio > 0.5 and clean_len > 12:
            return True
    
    return False


def clean_gibberish_from_translation(text: str) -> str:
    """æ¸…ç†ç¿»è­¯çµæœä¸­çš„ç„¡æ„ç¾©éŸ³è­¯ä¸²"""
    if not text:
        return text
    
    # å…ˆæª¢æŸ¥æ•´å¥æ˜¯å¦ç‚ºç„¡æ„ç¾©éŸ³è­¯
    if detect_gibberish_transliteration(text):
        print(f"âš ï¸ éæ¿¾ç„¡æ„ç¾©éŸ³è­¯: {text[:40]}", file=sys.stderr, flush=True)
        return ""
    
    # åˆ†å¥è™•ç†ï¼Œç§»é™¤ç„¡æ„ç¾©çš„éƒ¨åˆ†
    separators = ['ï¼Œ', 'ã€‚', 'ï¼', 'ï¼Ÿ']
    for sep in separators:
        if sep in text:
            parts = text.split(sep)
            cleaned_parts = []
            for part in parts:
                part = part.strip()
                if part and not detect_gibberish_transliteration(part):
                    cleaned_parts.append(part)
            
            if len(cleaned_parts) < len([p for p in parts if p.strip()]):
                result = sep.join(cleaned_parts)
                if sep in ['ã€‚', 'ï¼', 'ï¼Ÿ'] and result and not result.endswith(sep):
                    result += sep
                if result != text:
                    print(f"ğŸ”§ ç§»é™¤éƒ¨åˆ†ç„¡æ„ç¾©éŸ³è­¯: {text[:40]} -> {result[:40]}", file=sys.stderr, flush=True)
                return result
    
    return text
